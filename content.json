{"meta":{"title":"不圆的石头","subtitle":null,"description":null,"author":"不圆的石头","url":"http://qioinglong.top"},"pages":[],"posts":[{"title":"","slug":"计划-2019年06月计划","date":"2019-06-10T10:27:29.101Z","updated":"2019-06-10T11:04:57.629Z","comments":true,"path":"2019/06/10/计划-2019年06月计划/","link":"","permalink":"http://qioinglong.top/2019/06/10/计划-2019年06月计划/","excerpt":"","text":"title: 2019年06月计划date: 2019/06/01tags: 计划清单toc: truecategories: 读书​—- 1.上月计划完成情况总结05月没有计划，一团乱 2.本月计划2.1.专业学习 完成线性代数课本学习 法询金融-小白金融学课程 可可英语背单词-完成到第15关 2.1.阅读 外婆的道歉信 嫌疑人X的献身 2.2.锻炼 3次步行回家 每天步行5公里 40公里跑步(平均每周2次跑步) 2.3.孩子 每周读三次绘本 好好跟孩子沟通 2.4.其它 不跟太太吵架 每天早上一杯温水 每晚泡脚","categories":[],"tags":[],"keywords":[]},{"title":"金融资本的流动过程","slug":"金融资本流动的过程","date":"2019-05-30T09:51:00.000Z","updated":"2019-05-30T11:13:40.639Z","comments":true,"path":"2019/05/30/金融资本流动的过程/","link":"","permalink":"http://qioinglong.top/2019/05/30/金融资本流动的过程/","excerpt":"","text":"1.金融中介在金融活动中，主要有三类金融中介，信用中介，资本中介，服务中介 1.1.信用中介 graph LR 出借人 -->|闲余资金/债权1| 商业银行贷款人 商业银行贷款人 -->|贷款/债权2| 借款人 1. 资本流通：商业银行调剂货币资本的存款和借款 2. 利息：资金的出借人（食利者）获得固定的利息 3. 信用：流通中的商业银行使用了自身的信用，存款本息的兑付是刚性的 4. 利息差：商业银行使用信用(担保本息)得到风险补偿。 5. 商业银行集中管理存款（资金流入）和贷款（资金流出） 6. 在信用中介中，债权1与债权2不能等价传递 ### 1.2.资本中介 ### 资本中介主要是二级市场金融工具的流通。 graph LR 投资人 --> 券商 券商 --> 二级市场 投资人通过券商进入二级市场，买卖股票，债权 券商主要有经纪业务（证券经纪服务、融资融券）和投行业务（证券承销和保荐） 1.3.服务中介 graph LR 资金 --> 资产管理 资产管理 --> 资产 资产管理帮资产找资金，帮资金找资产 资管范围：银行理财，信托计划，券商资管，保险资管，期货资管，基金子公司，私募资金 2.资金的成本与收益资金的成本是需要资金的借款人愿意支付的“使用成本”，叫做利息（也叫做占用一笔资金所要支付的代价），利息又分为单利和复利两种类型。 2.1.单利单利是指一笔资金无论存期多长，只有本金计取利息，而以前各期利息在下一个利息周期内不计算利息的计息方法。单利利息的计算公式为： 利息（I）=本金（P）×利率（i）×计息期数（n）单利终值的计算 F=P+P×i×n =P×(1+i×n)单利现值的计算公式为： P=F-I=F-P×i×t=F/(1+i×n)2.2.复利复利是指一笔资金除本金产生利息外，在下一个计息周期内，以前各计息周期内产生的利息也计算利息的计息方法。复利的计算是对本金及其产生的利息一并计算，也就是利上有利。复利计算的特点是：把上期末的本利和作为下一期的本金，在计算时每一期本金的数额是不同的。复利的计算公式是： S = p(1+i)^n2.3.资金的收益资金的收益就是利息差，如果存款利息大于贷款利息，那么银行需要自己补贴利息，这样就会发生亏损，在我国，除了一些政策性银行为了支持某些行业和产业的发展，一般来说银行都以盈利为目的，一般不会发生亏损。 3.金融活动中的风险金融活动中常见的风险有信用风险，市场风险，流动性风险，操作风险等。 3.1. 信用风险信用风险又称为交易对方风险或履约风险，指交易对方不履行到期债务的风险。由于结算方式的不同，场内衍生交易和场外衍生交易各自所涉的信用风险也有所不同。 3.2. 市场风险市场风险是指由于基础资产市场价格的不利变动或者急剧波动而导致衍生工具价格或者价值变动的风险。基础资产的市场价格变动包括市场利率、汇率、股票、债券行情的变动。 3.3. 流动性风险流动性风险指商业银行虽然有清偿能力，但无法及时获得充足资金或无法以合理成本及时获得充足资金以应对资产增长或支付到期债务的风险。 3.4. 操作风险操作风险是指由于信息系统或内部控制缺陷导致意外损失的风险。引起操作风险的原因包括: 人为错误、电脑系统故障、工作程序和内部控制不当。","categories":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}],"tags":[{"name":"金融","slug":"金融","permalink":"http://qioinglong.top/tags/金融/"}],"keywords":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}]},{"title":"农村信用社","slug":"银行-农村信用社","date":"2019-05-26T16:00:00.000Z","updated":"2019-05-29T10:30:11.792Z","comments":true,"path":"2019/05/27/银行-农村信用社/","link":"","permalink":"http://qioinglong.top/2019/05/27/银行-农村信用社/","excerpt":"","text":"1.农村信用社概念农村信用合作社(规模较大的可改制为农商行)是指经中国人民银行批准设立、由社员入股组成、实行民主管理、主要为社员提供金融服务的农村合作金融机构，其前身是央行在农村的网点。农信社原来是隶属于农业银行管辖。1996年8月，国务院颁发《关于农村金融体制改革的决定》，明确农村信用社与中国农业银行脱离行政隶属关系，对其业务管理和金融监管分别由县联社和中国人民银行承担，然后按合作制原则加以规范。农信社虽然也是银行类金融机构，但是它是基于合作制的基础上，吸收股份制运作机制组成的股份合作制的社区性地方金融机构，其业务经营是在民主选举的基础上由社员指定人员管理经营，并对社员负责。早期的农信系统只有省联社—县级联社这一个模式，既没有地市级这一层级，也没有农商行这一概念。后来随着改革的深入，不仅有的县级联社改制成了农商行，也出现了多家县级联社跨区域合并成立的地市级农商。但不管是信用社还是农商行，不管是县级还是地市级，除非演变成双头模式，否则仍属于省联社管辖的。 2.农村信用社管理模式 农商行结构：在此模式下，大一统的农商行作为一家独立法人，有完善的公司治理结构，自主经营，自负盈亏。这种模式下的农商行，与其他商业银行没有本质的差别，省联社做为一个实体已经消失。 省联社、县级联社二元体制：这是目前绝大多数地方农信系统所采用的模式。在此模式下，各县级联社作为独立法人，共同出资成立省联社，省联社不作为经营实体，仅承担管理、指导、协调、服务职能。这种模式下，县级联社在法律上独立，但在经营、管理上均受到省联社诸多掣肘。 双头管理模型：，深圳农商行不归广东联社管辖，自行其是；天津滨海农商银行也不归天津农商银行管辖；成都农商行也不归四川联社管辖。出现双头模型的深圳、滨海、成都等，不仅在当地，甚至在全国都有其独特的政治经济地位，是其他地区难以模仿的。 省联社整体转型农商行：这种农商行既是一个独立法人实体，又通过控股的方式对下面县级法人社承担管理职能。简单来说，这种农商行既当运动员，又当裁判员，是农信系统改革不完善的典型，例如黄河农商行。 3.农村信用社机构设置 4.常用机构数据模型 T99_COUN_ORG_CD 县域机构代码表 Inn_Org_ID 机构编号 CITY_CD 地市代码 COUN_CD 县域代码 CORR_CD 大社代码 T04_SCRCU_INN_ORG 农信内部机构信息 Inn_Org_ID 内部机构编号 Inn_Org_Src_Sys_Ordr_Num 内部机构源系统序号 Org_Full_Cd 机构全码 Inn_Org_Nm 内部机构名称 Inn_Org_Sht_Nm 内部机构简称 Inn_Org_Lvl_Cd 内部机构级别代码 Up_Org_ID 上级机构编号 Org_Lvl_Cd 机构层次编码 Org_Data_Sum_Lvl_Cd 机构数据汇总层次编码 Dumy_Sum_Org_Ind 虚拟汇总机构标志 Org_Set_Up_Dt 机构成立日期 Org_Recall_Dt 机构撤销日期 Valid_Ind 有效标志","categories":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}],"tags":[{"name":"银行业务","slug":"银行业务","permalink":"http://qioinglong.top/tags/银行业务/"}],"keywords":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}]},{"title":"智慧银行网点","slug":"银行-智慧银行网点","date":"2019-05-26T16:00:00.000Z","updated":"2019-05-28T05:17:22.225Z","comments":true,"path":"2019/05/27/银行-智慧银行网点/","link":"","permalink":"http://qioinglong.top/2019/05/27/银行-智慧银行网点/","excerpt":"1.什么是智慧银行网点智慧银行是一场理念、策略、流程的变革和实践，依托创新科技(智能设备数字媒体、人机交互等技术)推进金融新服务、新产品、新运营和业务模式创新。主要包含以下几个方面：","text":"1.什么是智慧银行网点智慧银行是一场理念、策略、流程的变革和实践，依托创新科技(智能设备数字媒体、人机交互等技术)推进金融新服务、新产品、新运营和业务模式创新。主要包含以下几个方面： 1.1.身份识别智能化 通过人脸识别、人证比对作为落实客户身份实名制的重要手段，解决柜面难于核实、疏于核实的问题； 将人脸识别作为柜员身份核实方式，解决柜员身份识别问题； 在满足监管要求的前提下，探索将人脸识别作为账户支取方式，即可实现刷脸取款。 1.2.业务办理场景化 整合柜面系统，统一系统入口，一次身份认证，实现单点登录。 重构业务流程，以交易场景为导向，以固化流程为目标，实现业务办理一次身份认证、一次签名、一张综合业务凭证。 引入智能柜员机，通过业务模式向“自助+协助”转型，提升业务处理效率和网点服务能力。 1.3.业务凭证无纸化 通过柜外清设备进行电子签名或指纹签名，实现签字凭证无纸化，即解决部分老年客户不会写字的问题，又杜绝了柜员办理业务一手清。 通过用电子印章替代柜面实物印章，在解决印章违规使用、管理不善的同时，实现盖章凭证无纸化。 辅以电子签章功能，通过电子签章验证码，实现业务场景可追溯性，并增强凭证的合法性效力。 1.4.数据联动协同化 线上线下渠道协同，实现线下网点对线上业务的落地承接。 支持对现有行内行外、有人无人设备的统一接入，实现机具设备间数据联动共享，解决系统、机具孤岛的问题。 统一信息发布，实现各类营销信息、产品功能、业务通知等在各渠道、各终端的统一发布。 支持设备接入的多样性，通过建立准入标准，更能灵活兼顾统一性和行特色化需求。 1.5.营销风控数据化 通过大数据运用，全方位向大堂经理展示客户视图、客户画像，通过重要信息的及时推送，为厅堂精准营销创造条件。 通过对各个服务终端交易数据、客户行为数据以及柜员操作数据进行采集。根据交易行为特点，结合监管要求和内外部案件风险特征，分别建立指标模型进行大数据分析，实现对外部客户行为风险的有效识别预警、内部柜员操作风险的监测阻断。 1.6.运营管理工具化 按照案例特征化、特征指标化、指标模型化的思路，建立运营监测指标模型，包括业务量指标、员工履职指标、现金重空指标、差错指标、反洗钱监测指标等进行集中运营监测。 支持日常运营线上管理，实现省级、行（社）级对不同纬度的信息管理，如柜员基础信息、违规积分、轮岗、现场和非现场检查、重要账户核对等。 通过构建管理驾驶舱，为分析决策提供依据。 1.7.为什么需要智慧银行网点面对同业竞争白热化、金融科技飞速发展、客户快速迁徙等的内外夹击，顺应网点轻型化、智能化、小型化发展趋势，加快推进智慧银行网点转型，以提升网点效能、优化资源配置、增强竞争力迫在眉睫。而目前网点普通存在以下的问题： 运营能将低，员工，网点分布不合理，经济效益差。 体验差，客户等待时间长，业务办理流程复杂，渠道不协同。 案件频发，客户与员工身份识别不到位，还在利用靠人、靠制度来防范风险。 实施智能银行网点后，将会从以下几个方面改善目前存在的问题。 风险控制将从弱工具、弱流程、弱制度方式向强工具、强流程、弱制度的转变。将有效减少冒用他人身份证开户、冒名贷款、私下客户存款等案件的发生。 运营成本将从高成本、高能耗、低盗用向低成本、低能耗、高效转变，大幅降低网点的业务服务成本。 整合业务场景、零散的系统孤岛、零散的交易方式等，提升服务效率。 人力资源的流动，机具的灵活调配，现金的管理等运营模式将会得到显著的提高。 2.智慧网点方案智慧银行网点要根据客户群体的不同以及网点位置进行差异化部署，主要部署方案有以下几种。 网点分类 客户定位 功能布局 服务模式 旗舰网点 城区、商业中心的客户 高柜区、低柜区、产品体验区、理财区、自助服务区 人工+自助、智能+协助、代客+自营、洽谈+体验、线上+线下 中型网点 城郊、居民社区的客户 高柜区、自助服务区 人工+自助、智能+协助、洽谈+体验、线上+线下 轻型网点 乡镇、村民聚集点的客户 高柜区、自助服务区 人工+自助、智能+协助、线上+线下 智能设备类型 智能柜员机：主要用于办理非现金业务，比如开户、转账汇款、账户激活、挂失解挂、账户签约等，还可以直接申领各类银行卡/u-key等产品的机具 智能pad：主要用于办理非现金业务以及不涉及介质领取的业务。 厅堂手持pad：用于现场管理、业务审核、客户识别以及营销支持。 柜外清设备：集密码输入、电子签名、指纹采集、语音提示、场景抓拍以及信息交互等功能于一体的柜外服务终端。","categories":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}],"tags":[{"name":"银行业务","slug":"银行业务","permalink":"http://qioinglong.top/tags/银行业务/"}],"keywords":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}]},{"title":"高等数学_微分","slug":"20190213_高等数学_微分2019022220190222","date":"2019-02-20T22:52:31.000Z","updated":"2019-02-22T07:14:56.092Z","comments":true,"path":"2019/02/21/20190213_高等数学_微分2019022220190222/","link":"","permalink":"http://qioinglong.top/2019/02/21/20190213_高等数学_微分2019022220190222/","excerpt":"","text":"1. 微分的定义设函数 y=f(x) 在某区间内有定义，$x_0$ 及 $x_0 + \\Delta x$ 在这区间内，如果函数的增量 \\Delta y = f(x_0 + \\Delta x) - f(x_0)可表示为 \\Delta y = A\\Delta x + O(\\Delta x)其中 A是不依赖于 $\\Delta x$的常数,那么称函数y=f(x)在点 x0 是可微的，而 $A\\Delta x$ 叫做函数 y=f(x) 在点 x0 相应于自变量增量 $\\Delta x$ 的微分，记作 dy ，即 dy = A \\Delta x函数 f(x)在点 x0 可微 $\\Leftarrow\\Rightarrow$ 函数f(x)在点 x0 可导，且当函数f(x)在点 x0 可微时，其微分一定是 dy = f'(x_0) \\Delta x y=f(x) 在任意点 x 的微分称为函数的微分，记作 dy 或 df(x) ,即 $dy=f’(x)\\Delta x$ 把自变量的增量 $\\Delta x$称为自变量的微分，记作 dx ，即 $dx=\\Delta x$dy=f'(x)dx \\frac{dy}{dx}=f'(x) 导数又称为“微商” 由 $dy=f’(x)dx$ ,求函数的微分dy:求出导数f’(x),再乘以dx 2. 复合函数的微分法则设y=f(u)及u=g(x)都可导，则y=f[g(x)]的微分为 dy=f'(u)g'(x)dx另外 g’(x)dx=du所以 dy=f'(u)du无论u 是自变量还是中间变量，微分形式 $dy=f’(u)du$ 保持不变，这一性质称为微分形式的不变性","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_导数","slug":"20190213_高等数学_导数2019022120190221","date":"2019-02-19T22:52:31.000Z","updated":"2019-02-21T08:47:28.132Z","comments":true,"path":"2019/02/20/20190213_高等数学_导数2019022120190221/","link":"","permalink":"http://qioinglong.top/2019/02/20/20190213_高等数学_导数2019022120190221/","excerpt":"","text":"1. 导数的定义设函数y=f(x)在点$x_0$的某个邻域内有定义，当自变量 x在$x_0$处取得增量 $\\Delta x$ ($x_0$ + $\\Delta x$ 仍在该邻域内)时，因变量 y 相应地取得增量 \\Delta y=f(x_0 + \\Delta x) - f(x_0)若 $\\displaystyle \\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x}$ 存在，则称函数 y =f(x) 在点 $x_0$处可导并称该极限为y=f(x)在点 $x_0$ 处的导数,记为 $f’(x_0)$, f'(x_0)=\\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} =\\lim_{\\Delta x \\to 0} \\frac{f(x_0+\\Delta x)-f(x_0)}{\\Delta x}y'|_{x=x_0},\\frac{dy}{dx}|_{x=x_0}或\\frac{df(x)}{dx}|_{x=x_0}函数在点$x_0$处可导(具有导数和导数存在) 左导数与右导数统称为单侧导数. 函数 f ( x ) 在点 x0 处可导的充分必要条件是左导数 $f’_-(x_0)$ 和右导数 $f’_+(x_0)$ 都存在且相等 如果函数 f (x ) 在开区间(a,b)内可导, $f’_+(a)$ $f’_-(b)$ 都存在，那么就称 f (x )在闭区间[a,b]内可导 过切点 A 且与切线垂直的直线叫做曲线y=f(x)在点 A 处的法线 切线方程y-y_0=f'(x_0)(x-x_0) 法线方程y-y_0=-\\frac{1}{f'(x_0)}(x-x_0) (且f'(x_0) \\neq 0) 如果y=f(x)在点 x0 处的导数为无穷大,这时y=f(x)的割线以垂直于 x 轴的直线 x =x0 为极限位置,即y=f(x)在点 A 处具有垂直于 x 轴的切线 x=x0,其法线方程为y=y0 若y=f(x)在点x0处可导，则f(x)在点x0处连续 若y=f(x)在点x0处连续，但f(x)在点x0处不一定可导 2. 函数的和、差、积、商的求导法测如果函数u=u(x)和v=v(x)都在点x可导，那么他们的和、差、积、商（除分母为0的点外）都在店x可导，且： $(u \\pm v)’ = u’ \\pm v’$ $(uv)’=u’v+uv’$ $(\\frac{u}{v})’=\\frac{u’v-uv’}{v^2}(v(x) \\neq 0)$ 3.反函数的求导法则设函数 $x=f(y)$ 在区间 $I_y$ 内单调、 可导,且 $f’(y) \\neq 0$ ,则其反函数 $y=f^{-1}(x)$ 在区间 $I_x={x|x=f(y), y \\in I_y}$ 内也可导，且： [f^{-1}(x)]'=\\frac{1}{f'(y)} 或 \\frac{dy}{dx}=\\frac{1}{\\frac{dx}{dy}}反函数的导数是直接函数导数的倒数 4. 复合函数的求导法则设 $u=g(x)$ 在点x可导，而 $y=f(u)$ 在点 $u=g(x)$ 可导，那么复合函数 $y=f[g(x)]$ 在点x可导，且: \\frac{dy}{dx}=f'(u).g'(x)或\\frac{dy}{dx}=\\frac{dy}{du}.\\frac{du}{dx}5. 导数实例 $(C)’=0$ $(x^u)’=ux^{u-1}$ $(a^x)’=a^xlna$ $(e^x)’=e^x$ $(log_ax)’=\\frac{1}{xlna}$ $(lnx)’=\\frac{1}{x}$ $(sinx)’=cosx$ $(cosx)’=-sinx$ $[cu(x)]’=cu’(x)$ $(tanx)’=sec^2x$ $(cotx)’=-\\frac{1}{sin^2x}=-csc^2x$ $(secx)’=secxtanx$ $(cscx)’=-cscxcotx$ $(arcsinx)’=\\frac{1}{\\sqrt{1-x^2}}$ $(arccosx)’=-\\frac{1}{\\sqrt{1-x^2}}$ $(arctanx)’=\\frac{1}{1+x^2}$ $(arccotx)’=-\\frac{1}{1+x^2}$ 6. 高阶导数一般地,函数 $y=f(x)$ 的导数 $y’=f’(x)$ 仍然是x的函数，我们把 $y’=f’(x)$ 的导数叫做函数 $y=f(x)$ 的二阶导数，记作 $y’’或\\frac{d^2y}{dx^2}$ y''=(y')' 或 \\frac{d^2y}{dx^2}=\\frac{d}{dx}(\\frac{dy}{dx})把y=f(x)的导数f’(x)叫做函数y=f(x)的一阶导数，y’’二阶导数…二阶及二阶以上的导数均称为高阶导数，函数y=f(x)具有n阶导数，也称为函数f(x)为n阶可导。 $(e^x)^{(n)}=e^x$ $(cosx)^{(n)}=cos(x+\\frac{n\\pi}{2})$ $(sinx)^{(n)}=sin(x+\\frac{n\\pi}{2})$ $(x^u)^{(n)}=u(u-1)(u-2)…(u-n+1)x^{u-n}$ $(x^n)^{(n)}=n!$ $(x^n)^{(n+1)}=0$ $(\\frac{1}{x+a})^{n}=\\frac{(-1)^nn!}{(x+a)^{n+1}}$ $[ln(x+b)]^{(n)}=(\\frac{1}{x+b})^{(n-1)}$ 如果函数u=u(x)和v=v(x)都在点 x具有 n阶导数，那么它们的和、差也在点 x具有 n阶导数，且 (u\\pm v)^{(n)} = u^{(n)} \\pm v^{(n)}7. 莱布尼兹公式若f(x)、g(x)具有n阶导数，则 [f(x).g(x)]^{(n)} = f^{(n)}g+C_n^1f^{(n-1)}g'+...C_n^kf^{(n-k)}g^{(k)}+...fg^{(n)}其中 $C_n^k=\\frac{n(n-1)…(n-k+1)}{k!}$ 8. 隐函数的导数如果变量x和y满足一个方程F(x,y)=0,在一定条件下，当 x 取某区间的任一值时，相应地总有满足此方程的唯一的 y 值存在，那么就说方程F(x,y)=0在该区间内确定了一个隐函数,把一个隐函数化成显函数，叫做隐函数的显化。隐函数导数的方法：一般地，方程F(x,y)=0确定y=f(x),取得F[x,y(x)]=0;在F[x,y(x)]=0两边对x求导即得隐函数的导数.对数求导法：对数求导法就是先在 y = f(x) 的两边取对数，然后求出 y 的导数. 9. 由参数方程所确定的函数的导数一般地，若参数方程 \\begin{cases} x=\\phi(t) \\\\ y=\\psi(t) \\end{cases}确定 y 与 x 的函数关系，则称此函数关系所表达的函数为由参数方程所确定的函数。求导方法： \\frac{dy}{dx}=\\frac{dy}{dt}.\\frac{dt}{dx}=\\frac{dy}{dt}\\frac{1}{\\frac{dx}{dt}}=\\frac{\\psi'(t)}{\\phi'(t)}","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_函数的连续性与间断点","slug":"20190213_高等数学_函数的连续性与间断点2019022120190221","date":"2019-02-13T02:52:31.000Z","updated":"2019-02-21T05:17:48.362Z","comments":true,"path":"2019/02/13/20190213_高等数学_函数的连续性与间断点2019022120190221/","link":"","permalink":"http://qioinglong.top/2019/02/13/20190213_高等数学_函数的连续性与间断点2019022120190221/","excerpt":"","text":"1. 函数的连续性 定义：设函数 y =f (x)在点 $x_0$的某个邻域内有定义，如果 \\lim_{\\Delta x \\to 0} \\Delta y = \\lim_{\\Delta x \\to 0}[f(x_0+\\Delta x)-f(x_0)] =0则称函数y=f(x)在点$x_0$连续 定义：设函数 y =f (x)在点 $x_0$的某个邻域内有定义，如果 \\lim_{x \\to x_0} f(x) = f(x_0)则称函数y=f(x)在点$x_0$连续 在区间上每一点都连续的函数，叫做在该区间上的连续函数，或者说函数在该区间上连续.如果区间包含端点，那么函数在右端点连续是指左连续，在左端点连续是指右连续。 2.连续函数的四则运算的连续性 若函数f(x)、g(x)在点 $x_0$ 处连续,则$f(x) \\pm g(x)$ , $f(x).g(x)$ , $\\frac {f(x)}{g(x)}(g(x)\\neq0)$在点 $x_0$ 处连续 若函数y=f(x)在区间$I_x$上单调增加(或单调减少）且连续,则它的反函数$x=\\phi (x)$ 也在对应的区间I_y ={y|y=f(x),x \\in I_x}上单调增加（或单调减少）且连续 一切初等函数在其定义区间内都是连续的。 3. 函数的间断点假设函数 f (x) 在点 $x_0$ 的某去心邻域内有定义,如果函数 f (x) 有下列三种情形之一：（1）在 x= $x_0$ 处没有定义；（2）虽然在 x= $x_0$ 处有定义，但 $\\displaystyle\\lim_{x \\to x_0}f(x)$ 不存在；（3）虽然在 x= $x_0$ 处有定义， $\\displaystyle\\lim_{x \\to x_0}f(x)$ 存在； \\displaystyle\\lim_{x \\to x_0}f(x) \\neq f(x_0) 则称函数 f (x) 在点 $x_0$ 为不连续； 点 $x_0$ 称为 f (x) 的不连续点，或间断点. 若$f(x_0^+)$ 与 $f(x_0^-)$$存在，称为第一类间断点，否则称为第二类间断点。 第一类间断点中 $f(x_0^+) = f(x_0^-)$,则称$x=x_0$为函数f(x)的可去间断点，如果不等则为跳跃间断点 4. 闭区间上连续函数的性质 如果函数f(x)在开区间 (a,b)内连续，且在左端点 a 右连续,在右端点b左连续,则称函数f(x)在闭区间[a,b]上连续 4.1 最大值最小值定理与有界性定理 设函数f(x)在区间 I 上有定义，如果存在$x_0 \\in I$使得对任何$x \\in I$,均有 f(x) \\leq f(x_0) (f(x) \\geq f(x_0)) 则称f($x_0$)是函数f(x)在区间 I 上的最大值（最小值）,如果一个函数的最大值与最小值相等， 则该函数一定是常数函数. 4.2 零点定理与介质定理 方程f(x)=0的根称作函数f(x)的零点 零点定理：设函数f(x)在闭区间[a,b]上连续，且f(a).f(b)&lt;0,则至少存在一点$$\\xi \\in (a,b)$,使得$f(\\xi)=0$ 介质定理：设函数f(x)在闭区间[a,b]上连续，则对于 f(a) 与 f(b) 之间的任何一个数C,在开区间(a,b)内至少存在一点\\xi,使得$f(\\xi)=C$","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_无穷小的比较","slug":"20190127_高等数学_无穷小的比较2019021520190215","date":"2019-01-27T06:52:31.000Z","updated":"2019-02-15T01:48:00.368Z","comments":true,"path":"2019/01/27/20190127_高等数学_无穷小的比较2019021520190215/","link":"","permalink":"http://qioinglong.top/2019/01/27/20190127_高等数学_无穷小的比较2019021520190215/","excerpt":"","text":"1.无穷小的定义 设$\\alpha , \\beta$ 是同一自变量变化过程中的无穷小，且 $\\alpha \\neq 0 , \\lim \\frac{\\beta}{\\alpha}$ 表示这一变换过程中的极限。 如果 $\\lim \\frac{\\beta}{\\alpha} = 0$,则称 $\\beta$ 是比$\\alpha$高阶的无穷小，记作$\\beta = \\omicron(\\alpha)$ 如果 $\\lim \\frac{\\beta}{\\alpha} = \\infty$,则称 $\\beta$ 是比$\\alpha$低阶的无穷小 如果 $\\lim \\frac{\\beta}{\\alpha} = c \\neq 0$,则称 $\\beta$与$\\alpha$同阶无穷小 如果 $\\lim \\frac{\\beta}{\\alpha} = 1 $,则称 $\\beta$与$\\alpha$等价无穷小,记作$\\alpha $~$ \\beta$ 如果 $\\lim \\frac{\\beta}{\\alpha^k} = c \\neq 0$ k&gt;0,则称 $\\beta$是关于$\\alpha$的k阶无穷小 重要等价无穷小$sinx$ ~ x$tanx$ ~ x$arcsinx$ ~ x$arctanx$ ~ x$1-cosx$ ~ $\\frac{x^2}{2}$$\\sqrt[n]{1+x} - 1$ ~ $\\frac{1}{n}x$当$x \\rightarrow 0$ln(1+x) ~ x$e^x -1$ ~ x$(1+x)^a -1$ ~ ax对于形如$u(x)^{v(x)}(u(x)&gt;0,u(x)\\neq1)$的幂指函数，如果limu(x)=a&gt;0,limv(x)=b&gt;0,那么limu(x)^{v(x)}=a^b 2. 等价无穷小的性质 $\\alpha$ ~ $\\beta$的充分必要条件是$\\beta = \\alpha + \\omicron(\\alpha)$ $\\alpha$ ~ $\\alpha^{‘}$,$\\beta$ ~ $\\beta^{‘}$,且$\\lim \\frac{\\beta^{‘}}{\\alpha^{‘}}$存在，则$\\lim \\frac{\\beta}{\\alpha}$ = $\\lim \\frac{\\beta^{‘}}{\\alpha^{‘}}$ 在利用等价无穷小代换求极限时，只有对所求极限中相乘或相除的因式才能用等价无穷小来代替，而对极限中相加或相减的部分则不能随意来代替。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_极限的运算法则","slug":"20190127_高等数学_极限的运算法则2019012720190127","date":"2019-01-27T02:52:31.000Z","updated":"2019-01-27T07:18:16.495Z","comments":true,"path":"2019/01/27/20190127_高等数学_极限的运算法则2019012720190127/","link":"","permalink":"http://qioinglong.top/2019/01/27/20190127_高等数学_极限的运算法则2019012720190127/","excerpt":"","text":"1.极限运算法则 有限个无穷小的和是无穷小。 有界函数与无穷小的乘积是无穷小。(这里函数的有界性只要求在无穷小的自变量变化范围内成立) 常数与无穷小的乘积是无穷小. 有限个无穷小的乘积是无穷小. 无穷小的和、差积仍是无穷小. 如果 $ \\lim f(x) = A $ , $ \\lim g(x) = B $ 那么$ \\lim[f(x) \\pm g(x) ] = lim f(x) \\pm lim g(x) = A \\pm B $$ \\lim[f(x) \\cdot g(x) ] = lim f(x) \\cdot lim g(x) = A \\cdot B $$ \\lim \\frac{f(x)}{g(x)} = \\frac{lim f(x)}{ lim g(x)}= \\frac {A} {B} (B\\neq0) $$ \\lim[cf(x)] = c \\lim f(x) =cA (c为常数)$$ \\lim[f(x)]^n = [\\lim f(x)]^n = A^n(n为正整数)$ $f(x)=\\frac{P(x)}{Q(x)}$,其中P(X)，Q(x)为多项式，则当$Q(x_0) \\neq 0$,有\\lim_{x \\to x_0} f(x) = \\frac { \\lim_{x \\to x_0 } P(x)}{\\lim_{x \\to x_0} Q(x)}= \\frac{P(x_0)}{Q(x_0)}=f(x_0)当$Q(x_0)=0$时，则商的极限的运算法则不能应用。 当$a_0 \\neq 0,b_0 \\neq 0,m和n为非负整数时$ \\lim_{x \\rightarrow \\infty}\\frac{a_0x^m+a_1x^{m-1}+...+a_m}{b_0x^n+b_1x^{n-1}+...+b_n} =\\begin{cases} \\frac{a_0}{b_0},& n=m \\\\ 0,& n>m \\\\ \\infty ,& n","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_无穷小与无穷大","slug":"20190127_高等数学_无穷小与无穷大2019012720190127","date":"2019-01-26T22:52:31.000Z","updated":"2019-01-27T02:56:48.386Z","comments":true,"path":"2019/01/27/20190127_高等数学_无穷小与无穷大2019012720190127/","link":"","permalink":"http://qioinglong.top/2019/01/27/20190127_高等数学_无穷小与无穷大2019012720190127/","excerpt":"","text":"1. 无穷小 定义 如果 f (x) 当 $ x \\rightarrow 0 或 (x \\rightarrow \\infty )$时的极限为零，则称 f (x) 为当$ x \\rightarrow 0（或 x \\rightarrow \\infty ）$时的无穷小特别地，以零为极限的{$x_n$}也称为$n \\rightarrow \\infty $时的无穷小 除了常数 0 是无穷小， 其他任何常数， 即便是这个数的绝对值很小很小，都不是无穷小 无穷小是以零为极限的函数 在自变量 x 的同一变化过程中， 函数f (x) 有极限 A 的充分必要条件是f(x) = A + \\alpha(x) 其中$\\alpha(x)$是无穷小2 无穷大 定义 如果函数f(x) 当 $ x \\rightarrow x_0 或 (x \\rightarrow \\infty )$时,其绝对值 |f(x)|无限增大,则称函数f(x)为$ x \\rightarrow x_0 或 (x \\rightarrow \\infty )$时的无穷大。 如果 \\lim_{x \\rightarrow x_0^-} f(x) = \\infty 或 \\lim_{x \\rightarrow x_0^+} f(x) = \\infty则直线x=$x_0$是函数y=f(x)的图形的铅直渐近线 在自变量 x 的同一变化过程中，如果 f(x) 是无穷大,则$\\frac{1}{f(x)}$是无穷小；如果 f(x) 是无穷小，且 $ f(x) \\neq 0$,则$\\frac{1}{f(x)}$是无穷大","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_初等函数","slug":"20190126_高等数学_初等函数2019012620190126","date":"2019-01-26T02:52:31.000Z","updated":"2019-01-26T09:52:29.901Z","comments":true,"path":"2019/01/26/20190126_高等数学_初等函数2019012620190126/","link":"","permalink":"http://qioinglong.top/2019/01/26/20190126_高等数学_初等函数2019012620190126/","excerpt":"","text":"1. 基本初等函数1.1 幂函数y=x^a (a \\in R)1.2 指数函数y=a^x (a>0,a \\neq 1)1.3 对数函数y=log_a x (a>0,a \\neq 1)当 a =e 时， 记作 $y=lnx$ 1.4 三角函数y=sinxy=cosxy=tanxy=cotxy=sex=$ \\frac{1}{cosx} $y=cscx=$ \\frac{1}{sinx} $ 1.5 反三角函数y=arcsinxy=arccosxy=arctanxy=arccotx 2. 初等函数由常数和基本初等函数(幂函数,指数函数,对数函数,三角函数,反三角函数)经过有限次的四则运算和有限次的函数复合步骤所构成的并可用一个式子表示的函数，称为初等函数。$y=sin^2x$$y=arcsinx^2 + ln2x$","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_函数与极限笔记","slug":"20190126_高等数学_函数与极限笔记2019012720190127","date":"2019-01-25T22:52:31.000Z","updated":"2019-01-27T07:18:16.490Z","comments":true,"path":"2019/01/26/20190126_高等数学_函数与极限笔记2019012720190127/","link":"","permalink":"http://qioinglong.top/2019/01/26/20190126_高等数学_函数与极限笔记2019012720190127/","excerpt":"","text":"1. 函数概念设数集 $D \\subset R$ , 如果对 D 中每个数 x ，变量y 按照一定法则在 R 中有唯一确定的数值与之对应，则称 y 是 x 的函数,记为 y = f(x), $x \\subset D$ 其中 x 称为自变量, y 称为因变量, D 称为定义域，记作$D_f$ ,即 $D_f$ = D,函数值 f(x) 的全体所构成的集合称为函数 f 的值域,$R_f$或f(D),即 $R_f$ = f(D) = {y|y=f(x),$x \\subset D$} 2.函数的特性 函数的有界性 设函数 f(x) 的定义域为 D ，数集 $X \\subset D$,如果存在数 K1，使得对任一 $x \\subset X$ ，都有 $f (x) \\leq K_1$,则称函数 f (x) 在 X 上有上界，而 K1 称为函数 f (x)在 X 上的一个上界.如果存在数 K2 ，使得对任一$X \\subset D$ ，都有 $f (x) \\geq K_2$, ，则称函数 f (x) 在 X 上有下界，而 K2 称为函数f (x)在 X 上的一个下界.如果存在正数 M ，使得对任一 $X \\subset D$ ，都有 $|f (x)| \\leq M $ ，则称函数 f (x) 在 X 上有界如果这样的 M 不存在，则称函数 f (x)在 X 上无界．y=sinx在($ -\\infty , +\\infty $)有界y=tanx在{$- \\frac{\\pi}{2} , \\frac{\\pi}{2}$}无界 函数的单调性 设函数 f(x)的定义域为 D ，区间 $I \\subset D $ ，如果对于区间 I 内的任意两点 x1及 x2，当 $x_1 \\leq x_2 $时，恒有 $f(x_1) &lt; f(x_2)$ ，则称函数 f(x)在区间 I 内是单调增加的；如果对于区间 I 内的任意两点 $x_1及 x_2 $，当 $x_1 &lt; x_2 $ 时，恒有 $ f(x_1) &gt;f(x_2) $,则称函数 f x   在区间 I 内是单调减少的.单调增加或单调减少的函数统称为单调函数 函数的奇偶性 设函数 f(x) 的定义域 D 关于原点对称，如果对于任一 $x \\subset D$ ，f( -x ) = -f( x )恒成立，则称 f(x) 为奇函数；如 果 对 于 任 一 $x \\subset D$ ，f( x ) = f( -x )恒成立，则称 f( x ) 为偶函数. 注：奇函数关于原点对称，偶函数关于Y轴对称 函数的周期性 设函数 f( x )的定义域为 D ，如果存在一个正数T ，使得对于任一 $x \\subset D $ 有 $x + T \\subset D $ ，且f (x + T) = f ( x ) 恒成立，则称 f ( x )为周期函数， T 称为函数f ( x ) 的周期． &emsp;1. 通常我们说的周期函数的周期是指最小正周期. 比如，函数 sin x ， cos x 都是以 2π 为周期的周期函数， 函数 tan x 是以 π 为周期的周期函数. &emsp; 2. 并非所有函数都有最小正周期． 3. 数列的极限 数列的定义 如果按照某一法则，对每个正整数 n ，对应着一个确定的实数 $x_n$ ，这些实数 $x_n$按照下标 n从小到大排列得到的一个序列 x_1,x_2,x_3,... x_n ...就叫做数列，简记为${x_n}$．数列中的每一个数叫做数列的项，第 n 项 ${x_n}$称之为数列的一般项或通项. 极限的定义 设{$x_n$} 为一数列，如果存在常数 a ，对于任意给定的正数 $\\epsilon$ ，总存在着一个正整数 N ，使得对于n &gt; N 时的一切 n ，不等式 |$ x_n -a $| &lt; $\\epsilon$ 均成立，则称常数 a 是数列 {$x_n$}的极限，或者称数列{$x_n$} 收敛于 a记作 \\displaystyle \\lim_{n \\to \\infty} x_n = a 或者 x_n \\rightarrow a (n \\rightarrow \\infty)如果上述 a 不存在，则称数列{$x_n$}没有极限，或者说数列{$x_n$}是发散的，也说$\\displaystyle \\lim_{n \\to \\infty} x_n $不存在. 收敛数列的性质 极限的唯一性，如果数列是收敛的，那么它的极限值唯一 收敛数列的有界性,如果数列收敛，那么数列一定有界（如果数列无界，那么数列发散，数列有界未必收敛） 收敛数列的保号性,极限值与函数值具有一致保号性 如果数列{$x_n$} 收敛于 a ，则它的任一子数列也收敛，且其极限也为 a .若数列{$x_n$} 的某个子列发散或某两个子列收敛于两个不同的极限，则该数列{$x_n$}必发散. 4. 函数的极限 函数极限的定义 设函数 f(x)在点 $x_0$ 的某一去心邻域内,有定义，如果存在常数 A ，$\\forall \\varepsilon &gt;0 , \\exist \\delta&gt;0 $,当 0&lt;|$x-x_0$| &lt; $ \\delta $时，有$|f(x)-A| &lt; \\varepsilon $,那么常数 A 称为函数 f(x) 当 $x \\rightarrow x_0 $时的极限，记作: \\lim_{n \\to x_0} f(x) =A 或者 f(x) \\rightarrow A (当x \\rightarrow x_0) $x \\rightarrow x_0 $时函数 f(x) 有没有极限与f(x)在$x=x_0$处是否有定义无关 右极限和左极限统称为单侧极限 如果$f(x_0^+) 、f(x_0^-) $中有一个不存在，或两个虽存在但不相等， 则\\lim_{x \\to x_0} f(x)不存在。 函数极限的性质 函数极限的唯一性 函数极限的局部有界性 函数极限的局部保号性","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"高等数学_函数与极限笔记","slug":"assets20190520/20190126_高等数学_函数与极限笔记-3b6c607f","date":"2019-01-25T22:52:31.000Z","updated":"2019-01-26T13:30:28.908Z","comments":true,"path":"2019/01/26/assets20190520/20190126_高等数学_函数与极限笔记-3b6c607f/","link":"","permalink":"http://qioinglong.top/2019/01/26/assets20190520/20190126_高等数学_函数与极限笔记-3b6c607f/","excerpt":"","text":"1. 函数概念设数集 $D \\subset R$ , 如果对 D 中每个数 x ，变量y 按照一定法则在 R 中有唯一确定的数值与之对应，则称 y 是 x 的函数,记为 y = f(x), $x \\subset D$ 其中 x 称为自变量, y 称为因变量, D 称为定义域，记作$D_f$ ,即 $D_f$ = D,函数值 f(x) 的全体所构成的集合称为函数 f 的值域,$R_f$或f(D),即 $R_f$ = f(D) = {y|y=f(x),$x \\subset D$} 2.函数的特性 函数的有界性 设函数 f(x) 的定义域为 D ，数集 $X \\subset D$,如果存在数 K1，使得对任一 $x \\subset X$ ，都有 $f (x) \\leq K_1$,则称函数 f (x) 在 X 上有上界，而 K1 称为函数 f (x)在 X 上的一个上界.如果存在数 K2 ，使得对任一$X \\subset D$ ，都有 $f (x) \\geq K_2$, ，则称函数 f (x) 在 X 上有下界，而 K2 称为函数f (x)在 X 上的一个下界.如果存在正数 M ，使得对任一 $X \\subset D$ ，都有 $|f (x)| \\leq M $ ，则称函数 f (x) 在 X 上有界如果这样的 M 不存在，则称函数 f (x)在 X 上无界．y=sinx在($ -\\infty , +\\infty $)有界y=tanx在{$- \\frac{\\pi}{2} , \\frac{\\pi}{2}$}无界 函数的单调性 设函数 f(x)的定义域为 D ，区间 $I \\subset D $ ，如果对于区间 I 内的任意两点 x1及 x2，当 $x_1 \\leq x_2 $时，恒有 $f(x_1) &lt; f(x_2)$ ，则称函数 f(x)在区间 I 内是单调增加的；如果对于区间 I 内的任意两点 $x_1及 x_2 $，当 $x_1 &lt; x_2 $ 时，恒有 $ f(x_1) &gt;f(x_2) $,则称函数 f x   在区间 I 内是单调减少的.单调增加或单调减少的函数统称为单调函数 函数的奇偶性 设函数 f(x) 的定义域 D 关于原点对称，如果对于任一 $x \\subset D$ ，f( -x ) = -f( x )恒成立，则称 f(x) 为奇函数；如 果 对 于 任 一 $x \\subset D$ ，f( x ) = f( -x )恒成立，则称 f( x ) 为偶函数. 注：奇函数关于原点对称，偶函数关于Y轴对称 函数的周期性 设函数 f( x )的定义域为 D ，如果存在一个正数T ，使得对于任一 $x \\subset D $ 有 $x + T \\subset D $ ，且f (x + T) = f ( x ) 恒成立，则称 f ( x )为周期函数， T 称为函数f ( x ) 的周期． &emsp;1. 通常我们说的周期函数的周期是指最小正周期. 比如，函数 sin x ， cos x 都是以 2π 为周期的周期函数， 函数 tan x 是以 π 为周期的周期函数. &emsp; 2. 并非所有函数都有最小正周期．","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"http://qioinglong.top/tags/高等数学/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"秘密读书笔记","slug":"20190125_秘密读书笔记20190524","date":"2019-01-25T04:52:31.000Z","updated":"2019-05-24T04:39:13.338Z","comments":true,"path":"2019/01/25/20190125_秘密读书笔记20190524/","link":"","permalink":"http://qioinglong.top/2019/01/25/20190125_秘密读书笔记20190524/","excerpt":"","text":"如果人生可以重来，你会选择怎样的方式过完自己的一生。 我们都渴望成为别人的样子，却最不想成为自己，30岁的时候，我给自己许下成为最好自己的愿望，认真度过每一天，认真做好每件事，无关权利，无关荣誉，只是对自己微小的回应。 秘密是看过的最不像推理的推理小说，全书以一种最平淡，最真实的文字描述了一个不可思议的故事。 如果今天我打破了这个规矩，那以后我还会打破第二、第三个规矩，这样下去人生将一步步走向失败。我之前的人生就是这种活法的典型。结果呢，虽然从小学到大专，我在可以称之为学校的地方待了14年，到头来却没有掌握一项能够赖以生存的技能。我再也不想重走老路了，打死我也不想再产生一次同样的懊悔了。 世界上真的有许多精彩的事物。有很多东西，比如能让你感到幸福的东西、能改变你世界观的东西等等，都不需要花很多钱就能得到。你说，我以前怎么就没注意到呢？ 爱一个人，就应该让她幸福。 可我呢，到现在还一直把自己当成你的丈夫，心里一直想着不能背叛你。我没有花过心，没有考虑过再婚的事。你上小学时那个桥本老师不错吧？我也很喜欢她，甚至想过和她交往。但是最终怎样呢？还不是连电话都没给她打过？你知道这是为什么吗？因为我不想背叛你！因为我想到我是你的丈夫！ 谁都想象个小孩子般，偶尔做做傻事，旁人却不会允许你这样做。比如有人会说，你已经快当爸爸了，要好好振作啊!或是你已经做爷爷了，应该要稳重一点。要是你对他们说:不，我只是一个平凡的男人!他们绝不会认同你你。有了孩子，你就是父亲;有了孙子，你就是爷爷了。你是逃脱不了现实的。所以，这时候你只能思考，该扮演哪种父亲、爷爷。 总觉得直子很自私，可是谁又能理解她心中的痛，心中的苦呢！选择中充满了无奈，幸福快乐的活下去才是最好的选择吧！ 让心爱的幸福就好，可是面对夜深人静时候孤寂，吃饭一个人，快乐一个人，悲伤一个人时，又有几个人能真正做到让心爱的人幸福呢？","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://qioinglong.top/tags/读书笔记/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"1984读书笔记","slug":"20190125_1984读书笔记20190524","date":"2019-01-24T22:52:31.000Z","updated":"2019-05-24T04:39:13.308Z","comments":true,"path":"2019/01/25/20190125_1984读书笔记20190524/","link":"","permalink":"http://qioinglong.top/2019/01/25/20190125_1984读书笔记20190524/","excerpt":"","text":"很压抑的一本书，断断续续读了很久，终于把它读完。2019年的今天，你依然会震撼它丰富的寓意和深刻的政治远见，可惜奥威尔1950年就去世了…… 这是一个最好的时代，也是一个最坏的时代，而你只能跟随时代的潮流，不断向前。你总觉得书中的一切那么遥远，但又那么近在眼前，仿佛一切一切的场景都能与生活一一映射，这才是最可怕的地方。 战争即和平，自由即奴役，无知即力量。 栗树荫下，我出卖你，你出卖我。 历史不是一面镜子，而是黑板上的记号，可以随时擦去，随时填补。更为可怕的是，一旦涂改了，你找不到证据去证明这是篡改历史的行为。 一切都消失在迷雾中了。过去给抹掉了，而抹掉本身又被遗忘了，谎言便变成了真话。 最后101房间更是用每个人心底最恐惧的事物来施虐从而丧失一切美好，包括忠诚，信任和爱意，剩下的只有麻木，丑陋不堪。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://qioinglong.top/tags/读书笔记/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"markdown语法","slug":"20190121_markdown语法20190524","date":"2019-01-21T12:00:59.000Z","updated":"2019-05-24T04:59:12.111Z","comments":true,"path":"2019/01/21/20190121_markdown语法20190524/","link":"","permalink":"http://qioinglong.top/2019/01/21/20190121_markdown语法20190524/","excerpt":"markdown语法大全，记录，以备查询","text":"markdown语法大全，记录，以备查询 1. 基本语法1.1. 标题123456# 一级标题 ### 二级标题 ##### 三级标题 ####### 四级标题 ######### 五级标题 ########### 六级标题 ###### 1.2. 字体1234**加粗的文字***倾斜的文字****斜体加粗的文字***~~加删除线的文~~ 1.3. 引用在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt;123&gt;引用的内容&gt;&gt;引用的内容&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;引用的内容 1.4. 分割线三个或者三个以上的 - 或者 * 都可以。1234-------******** 1.5. 图片1234![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)图片alt就是显示在图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加 1.6. 超链接12[超链接名](超链接地址 &quot;超链接title&quot;)title可加可不加 1.7. 列表无序列表,无序列表用 - + * 任何一种都可以1234- 列表内容+ 列表内容* 列表内容注意：- + * 跟内容之间都要有一个空格 有序列表12341.列表内容2.列表内容3.列表内容注意：序号跟内容之间要有空格 列表嵌套,上一级和下一级之间敲三个空格即可123- 一级 - 二级 - 三级 1.8. 表格1234567891011表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左-两边加：表示文字居中-右边加：表示文字居右注：原生的语法两边都要用 | 包起来。 1.9. 代码单行代码：代码之间分别用一个反引号包起来1`` 代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行123456(```) 代码... 代码... 代码...(```)为了防止转译，前后三个反引号处加了小括号 2. 数学2.1 行内与独行 行内公式：将公式插入到本行内，符号：$公式内容$，例：$xyz$$xyz$ 独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，例：$$xyz$$xyz2.2 上标、下标与组合 上标符号，符号：^，例：$x^2$$x^2$ 下标符号，符号：_，例：$x_2$$x_2$ 组合符号，符号：{}，例：${16}_{8}O{2+}_{2}$${20}_{8}O{2+}_{2}$2.3 汉字、字体与格式 汉字形式，符号：\\mbox{}，例：$V_{\\mbox{开始}}$$V_{\\mbox{开始}}$ 字体控制，符号：\\displaystyle，例：$\\displaystyle \\frac{x+y}{y+z}$$\\displaystyle \\frac{x+y}{y+z}$ 字体控制，符号：\\displaystyle，例：$\\displaystyle \\frac{x+y}{y+z}$$\\displaystyle \\frac{x+y}{y+z}$ 下划线符号，符号：\\underline，例：$\\underline{x+y}$$\\underline{x+y}$ 标签，符号\\tag{数字}，例：$\\tag{11}$$\\tag{11}$ 上大括号，符号：\\overbrace{算式}，例：$\\overbrace{a+b+c+d}^{2.0}$$\\overbrace{a+b+c+d}^{2.0}$ 下大括号，符号：\\underbrace{算式}，例 ：$a+\\underbrace{b+c}_{1.0}+d$$a+\\underbrace{b+c}_{1.0}+d$ 上位符号，符号：\\stacrel{上位符号}{基位符号}，例：$\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}$$\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}$2.4 占位符 两个quad空格，符号：\\qquad，例：$x \\qquad y$$x \\qquad y$ quad空格，符号：\\quad，例：$x \\quad y$$x \\quad y$ 大空格，符号\\，例：$x \\ y$$x \\ y$ 中空格，符号\\:，例：$x \\: y$$x \\: y$ 小空格，符号\\,，例：$x \\, y$$x \\, y$ 没有空格，符号，例：$xy$$xy$ 紧贴，符号\\!，例：$x \\! y$$x ! y$ 2.5 定界符与组合 括号，符号：（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)，例：$（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)$$（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)$ 中括号，符号：[]，例：$[x+y]$$[x+y]$ 大括号，符号：\\{ \\}，例：${x+y}$${x+y}$ 自适应括号，符号：\\left \\right，例：$\\left(x\\right)$，$\\left(x{yz}\\right)$$\\left(x\\right)$，$\\left(x{yz}\\right)$ 组合公式，符号：`{上位公式 \\choose 下位公式}，例：${n+1 \\choose k}={n \\choose k}+{n \\choose k-1}$’${n+1 \\choose k}={n \\choose k}+{n \\choose k-1}$ 组合公式，符号：{上位公式 \\atop 下位公式}，例：$\\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots$$\\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots$ 2.6 四则运算 加法运算，符号：+，例：$x+y=z$$x+y=z$ 减法运算，符号：-，例：$x-y=z$$x-y=z$ 加减运算，符号：\\pm，例：$x \\pm y=z$$x \\pm y=z$ 减甲运算，符号：\\mp，例：$x \\mp y=z$$x \\mp y=z$ 乘法运算，符号：\\times，例：$x \\times y=z$$x \\times y=z$ 点乘运算，符号：\\cdot，例：$x \\cdot y=z$$x \\cdot y=z$ 星乘运算，符号：\\ast，例：$x \\ast y=z$$x \\ast y=z$ 除法运算，符号：\\div，例：$x \\div y=z$$x \\div y=z$ 斜法运算，符号：/，例：$x/y=z$$x/y=z$ 分式表示，符号：\\frac{分子}{分母}，例：$\\frac{x+y}{y+z}$$\\frac{x+y}{y+z}$ 分式表示，符号：{分子} \\voer {分母}，例：${x+y} \\over {y+z}$${x+y} \\over {y+z}$ 绝对值表示，符号：||，例：$|x+y|$$|x+y|$ 2.7 高级运算 平均数运算，符号：\\overline{算式}，例：$\\overline{xyz}$$\\overline{xyz}$ 开二次方运算，符号：\\sqrt，例：$\\sqrt x$$\\sqrt x$ 开方运算，符号：\\sqrt[开方数]{被开方数}，例：$\\sqrt[3]{x+y}$$\\sqrt[3]{x+y}$ 对数运算，符号：\\log，例：$\\log(x)$$\\log(x)$ 极限运算，符号：\\lim，例：$\\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$$\\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 极限运算，符号：\\displaystyle \\lim，例：$\\displaystyle \\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$$\\displaystyle \\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 求和运算，符号：\\sum，例：$\\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$$\\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 求和运算，符号：\\displaystyle \\sum，例：$\\displaystyle \\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$$\\displaystyle \\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 积分运算，符号：\\int，例：$\\int^{\\infty}_{0}{xdx}$$\\int^{\\infty}_{0}{xdx}$ 积分运算，符号：\\displaystyle \\int，例：$\\displaystyle \\int^{\\infty}_{0}{xdx}$$\\displaystyle \\int^{\\infty}_{0}{xdx}$ 微分运算，符号：\\partial，例：$\\frac{\\partial x}{\\partial y}$$\\frac{\\partial x}{\\partial y}$ 矩阵表示，符号：\\begin{matrix} \\end{matrix}，例：$\\left[ \\begin{matrix} 1 &amp;2 &amp;\\cdots &amp;4\\5 &amp;6 &amp;\\cdots &amp;8\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\13 &amp;14 &amp;\\cdots &amp;16\\end{matrix} \\right]$$\\left[ \\begin{matrix} 1 &amp;2 &amp;\\cdots &amp;4\\5 &amp;6 &amp;\\cdots &amp;8\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\13 &amp;14 &amp;\\cdots &amp;16\\end{matrix} \\right]$ 2.8 逻辑运算 等于运算，符号：=，例：$x+y=z$$x+y=z$ 大于运算，符号：&gt;，例：$x+y&gt;z$$x+y&gt;z$ 小于运算，符号：&lt;，例：$x+y&lt;z$$x+y&lt;z$ 大于等于运算，符号：\\geq，例：$x+y \\geq z$$x+y \\geq z$ 小于等于运算，符号：\\leq，例：$x+y \\leq z$$x+y \\leq z$ 不等于运算，符号：\\neq，例：$x+y \\neq z$$x+y \\neq z$ 不大于等于运算，符号：\\ngeq，例：$x+y \\ngeq z$$x+y \\ngeq z$ 不大于等于运算，符号：\\not\\geq，例：$x+y \\not\\geq z$$x+y \\not\\geq z$ 不小于等于运算，符号：\\nleq，例：$x+y \\nleq z$$x+y \\nleq z$ 不小于等于运算，符号：\\not\\leq，例：$x+y \\not\\leq z$$x+y \\not\\leq z$ 约等于运算，符号：\\approx，例：$x+y \\approx z$$x+y \\approx z$ 恒定等于运算，符号：\\equiv，例：$x+y \\equiv z$$x+y \\equiv z$ 2.9 集合运算 属于运算，符号：\\in，例：$x \\in y$$x \\in y$ 不属于运算，符号：\\notin，例：$x \\notin y$$x \\notin y$ 不属于运算，符号：\\not\\in，例：$x \\not\\in y$$x \\not\\in y$ 子集运算，符号：\\subset，例：$x \\subset y$$x \\subset y$ 子集运算，符号：\\supset，例：$x \\supset y$$x \\supset y$ 真子集运算，符号：\\subseteq，例：$x \\subseteq y$$x \\subseteq y$ 非真子集运算，符号：\\subsetneq，例：$x \\subsetneq y$$x \\subsetneq y$ 真子集运算，符号：\\supseteq，例：$x \\supseteq y$$x \\supseteq y$ 非真子集运算，符号：\\supsetneq，例：$x \\supsetneq y$$x \\supsetneq y$ 非子集运算，符号：\\not\\subset，例：$x \\not\\subset y$$x \\not\\subset y$ 非子集运算，符号：\\not\\supset，例：$x \\not\\supset y$$x \\not\\supset y$ 并集运算，符号：\\cup，例：$x \\cup y$$x \\cup y$ 交集运算，符号：\\cap，例：$x \\cap y$$x \\cap y$ 差集运算，符号：\\setminus，例：$x \\setminus y$$x \\setminus y$ 同或运算，符号：\\bigodot，例：$x \\bigodot y$$x \\bigodot y$ 同与运算，符号：\\bigotimes，例：$x \\bigotimes y$$x \\bigotimes y$ 实数集合，符号：\\mathbb{R}，例：$\\mathbb{R}$$\\mathbb{R}$ 自然数集合，符号：\\mathbb{Z}，例：$\\mathbb{Z}$$\\mathbb{Z}$ 空集，符号：\\emptyset，例：$\\emptyset$$\\emptyset$ 2.10 数学符号 无穷，符号：\\infty，如：$\\infty$$\\infty$ 虚数，符号：\\imath，如：$\\imath$$\\imath$ 虚数，符号：\\jmath，如：$\\jmath$$\\jmath$ 数学符号，符号\\hat{a}，如：$\\hat{a}$`$\\hat{a}$ 数学符号，符号\\check{a}，如：$\\check{a}$`$\\check{a}$ 数学符号，符号\\breve{a}，如：$\\breve{a}$`$\\breve{a}$ 数学符号，符号\\tilde{a}，如：$\\tilde{a}$`$\\tilde{a}$ 数学符号，符号\\bar{a}，如：$\\bar{a}$`$\\bar{a}$ 矢量符号，符号\\vec{a}，如：$\\vec{a}$`$\\vec{a}$ 数学符号，符号\\acute{a}，如：$\\acute{a}$`$\\acute{a}$ 数学符号，符号\\grave{a}，如：$\\grave{a}$`$\\grave{a}$ 数学符号，符号\\mathring{a}，如：$\\mathring{a}$`$\\mathring{a}$ 一阶导数符号，符号\\dot{a}，如：$\\dot{a}$`$\\dot{a}$ 二阶导数符号，符号\\ddot{a}，如：$\\ddot{a}$`$\\ddot{a}$ 上箭头，符号：\\uparrow，如：$\\uparrow$$\\uparrow$ 上箭头，符号：\\Uparrow，如：$\\Uparrow$$\\Uparrow$ 下箭头，符号：\\downarrow，如：$\\downarrow$$\\downarrow$ 下箭头，符号：\\Downarrow，如：$\\Downarrow$$\\Downarrow$ 左箭头，符号：\\leftarrow，如：$\\leftarrow$$\\leftarrow$ 左箭头，符号：\\Leftarrow，如：$\\Leftarrow$$\\Leftarrow$ 右箭头，符号：\\rightarrow，如：$\\rightarrow$$\\rightarrow$ 右箭头，符号：\\Rightarrow，如：$\\Rightarrow$$\\Rightarrow$ 底端对齐的省略号，符号：\\ldots，如：$1,2,\\ldots,n$$1,2,\\ldots,n$ 中线对齐的省略号，符号：\\cdots，如：$x_1^2 + x_2^2 + \\cdots + x_n^2$$x_1^2 + x_2^2 + \\cdots + x_n^2$ 竖直对齐的省略号，符号：\\vdots，如：$\\vdots$$\\vdots$ 斜对齐的省略号，符号：\\ddots，如：$\\ddots$$\\ddots$ 2.11 希腊字母 字母 实现 字母 实现 A A α \\alpha B B β \\beta Γ \\Gamma γ \\gamma Δ \\Delta δ \\delta E E ϵ \\epsilon Z Z ε \\varepsilon H H ζ \\zeta Θ \\Theta η \\eta I I θ \\theta K K ι \\iota Λ \\Lambda κ \\kappa M M λ \\lambda N N μ \\mu Ξ \\Xi ν \\nu O O ξ \\xi Π \\Pi ο \\omicron P P π \\pi Σ \\Sigma ρ \\rho T T σ \\sigma Υ \\Upsilon τ \\tau Φ \\Phi υ \\upsilon X X ϕ \\phi Ψ \\Psi ϖ \\varpi Ω \\Omega χ \\chi Ω \\varOmega ψ \\psi 斜体在输入前加var Υ \\varUpsilon ω \\omega ∀ \\forall ∃ \\exists 2.12 其它2.12.1 首行缩进方法 一个空格大小的表示,半角空格(En Space)：&amp;ensp;或&amp;#8194;，此时只要在相应需要缩进的段落前加上 4个 如上的标记即可，注意要带上分号。 两个空格的大小表示,全角空格( Em Space)&amp;emsp;或&amp;#8195;，同理，使用2个即可缩进2个汉字，推荐使用该方式。 不换行空格(No-Break Space)：&amp;nbsp;或&amp;#160;，使用4个&#160;即可。 2.12.2 颜色，字体123456&lt;font face=&quot;黑体&quot;&gt;黑体字&lt;/font&gt;&lt;font face=&quot;微软雅黑&quot;&gt;微软雅黑&lt;/font&gt;&lt;font face=&quot;STCAIYUN&quot;&gt;华文彩云&lt;/font&gt;&lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt;color=#0099ff size=72 face=&quot;黑体&quot;&lt;/font&gt;&lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt;&lt;font color=gray size=72&gt;color=gray&lt;/font&gt; 2.12.3 大括号右多行赋值 12345&amp;用来对齐$$P(x|Pa_x)=\\begin&#123;cases&#125; 1, &amp; x=f(Pa_&#123;x&#125;)\\\\ 0, &amp; other\\ values\\end&#123;cases&#125;$$ P(x|Pa_x)=\\begin{cases} 1, & x=f(Pa_{x})\\\\ 0, & other\\ values \\end{cases}","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://qioinglong.top/tags/markdown/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"曾国藩家书读书笔记-1","slug":"20190119_曾国藩家书读书笔记-120190524","date":"2019-01-19T12:00:59.000Z","updated":"2019-05-24T04:39:13.277Z","comments":true,"path":"2019/01/19/20190119_曾国藩家书读书笔记-120190524/","link":"","permalink":"http://qioinglong.top/2019/01/19/20190119_曾国藩家书读书笔记-120190524/","excerpt":"","text":"开始看曾国藩家书，很久没有读文言文了，读起来还是有点吃力，很多字不认识，不认识…….原计划一个月读完上下两本，现在改变注意了，慢慢读，慢慢体会，今年能读完就行。家书嘛，有很多琐碎之事，读起来有时候确实有点乏味，不过字里行间是对修身养性的见解，对家庭父母的敬意，对兄弟的教导爱护，有很多值得学习和借鉴的地方。 用功譬如掘井，与其多掘数井而皆不及泉，何若老守一井，力求及泉而用之不竭乎。求业之精，别无他法，曰专而已。万不可以兼营并鹜，兼营则必无一所能矣。不必人人皆能记也，但记一人，则恍如其人;不必事事皆能记也，但记一事，则恍如亲其事。 这些对我触动挺大，以前以为多学点总没坏事，现今才觉得，学得多不如学得精，所以今年给自己定下的目标就是学习数据分析，其它的暂时不折腾。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://qioinglong.top/tags/读书笔记/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"银行业务","slug":"银行业务20190524","date":"2019-01-17T22:52:31.000Z","updated":"2019-05-24T04:46:48.577Z","comments":true,"path":"2019/01/18/银行业务20190524/","link":"","permalink":"http://qioinglong.top/2019/01/18/银行业务20190524/","excerpt":"","text":"银行业务","categories":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}],"tags":[{"name":"银行业务","slug":"银行业务","permalink":"http://qioinglong.top/tags/银行业务/"}],"keywords":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}]},{"title":"2019年需要完成的事情","slug":"20190119_2019年需要完成的事情20190524","date":"2019-01-09T12:00:59.000Z","updated":"2019-05-24T04:39:13.231Z","comments":true,"path":"2019/01/09/20190119_2019年需要完成的事情20190524/","link":"","permalink":"http://qioinglong.top/2019/01/09/20190119_2019年需要完成的事情20190524/","excerpt":"","text":"2019年要完成的事情： 学习数据分析入门 重新学习高等数学、线性代数、概率与统计 完成跑步1000公里 瘦身到65KG 学会弹简单的吉它 去华山或者峨眉旅游一次 读三本儿童教育的书籍 完成25本关于传记、小说、历史、哲学阅读","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"计划与实现","slug":"计划与实现","permalink":"http://qioinglong.top/tags/计划与实现/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"2018年07月-月结月策","slug":"2018年07月-月结月策20190524","date":"2018-07-28T22:52:31.000Z","updated":"2019-05-24T04:39:13.206Z","comments":true,"path":"2018/07/29/2018年07月-月结月策20190524/","link":"","permalink":"http://qioinglong.top/2018/07/29/2018年07月-月结月策20190524/","excerpt":"","text":"太久没有静下心来想想自己需要什么。本月参与了Teradata数据仓库迁移，从原来的旧机器迁移到新机器上。看起来是一件很简单的事情，真正做起来涉及到很多的细节，主要有以下几点： 需求的讨论（软件、硬件、网络、程序以及数据验证） Teradata数据库一体机安装 数据库网络配置，与原有旧机器采用万兆网直连 ETL服务器安装，批量时序验证 数据库初始化 数据迁移（冷数据迁移，增量数据迁移以及数据合并） 数据验证（count统计验证，抽样验证，关键指标验证） 数据迁移影响性分析 在项目执行过程中，出现最多的问题是，项目组成员部分操作没有文档落地，当引以为戒。要做到凡事有据可依，有记录可查。数据迁移并不复杂，主要是一个比较精细的活，需要我们把每一步都想清楚。 本月停止了CPA的学习，认真分析了自己目前的现状，考CPA并不是最紧急的事情。本月完成阅读： 原则 可怕的两岁 下月开始，系统化学习数据分析的课程。预计完成以下几本书的阅读： 儿童心理学 大数据之路（阿里巴巴） 策略的思维 深入浅出数据分析 谁说菜鸟不会数据分析","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"月结月策_201710","slug":"月结月策_20171020190524","date":"2017-10-31T00:54:59.000Z","updated":"2019-05-24T04:39:13.451Z","comments":true,"path":"2017/10/31/月结月策_20171020190524/","link":"","permalink":"http://qioinglong.top/2017/10/31/月结月策_20171020190524/","excerpt":"","text":"10月份荒废了太多，无论是读书、学习、工作还是运动，都没有很好的坚持下去。国庆长假后，就开始变得什么东西都不想学习的状态，陪孩子玩、看电视、睡觉。更让自己吃惊的时候，8月到9月两个月得时间里体重增长了6斤，完全失去了控制。控诉一下自己这个月得不满： 读书完全没有章法，没有计划，想起来就读一下，想不起来就看电视，睡觉。勉强完成了，却没有进行认真总结，思考，跟没有读没什么区别。 运动完全搁置，说到底就是懒懒懒。 工作还是得过且过的样子，之前还想着学习一些新东西，看看有没有好的机会。现在好像变得越来越不想动了。 又在纠结那些让人烦心的事情，明明已经过去了，还是不肯放过自己。 总结： 放纵自己是要付出代价的。 认真对待生活生活才能也好的状态反馈你。 与其花费金钱让孩子进好的学校，不如认真充实自己，陪孩子一起成长。 行合一，将知识转化为行动，将行动转化为习惯。 计划不执行，就永远只是计划。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"月结月策","slug":"月结月策","permalink":"http://qioinglong.top/tags/月结月策/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"活出生命的意义-弗兰克尔","slug":"活出生命的意义-弗兰克尔20190524","date":"2017-10-30T06:52:31.000Z","updated":"2019-05-24T04:39:13.394Z","comments":true,"path":"2017/10/30/活出生命的意义-弗兰克尔20190524/","link":"","permalink":"http://qioinglong.top/2017/10/30/活出生命的意义-弗兰克尔20190524/","excerpt":"","text":"一个故事，一个理论。前半部分故事很精彩，后面的理论对于非心理专业的我来说看得不是很懂。总体来说，活着，就要寻找自己存在的价值、目标。有了目标才能活下去，好好的活下去。 曾经很着迷，活着到底有什么意义，苦苦寻求而不得，17岁之前，只为了努力读书，走出大山。十七岁后，就突然没有了确切的目标。大学时为了好面子，跨专业学习化学、考研化学、学习日语，浪费了很多的时间，到最后啥也没干成。如果当时自己能多想一下，多想想自己人生的意义，活着的意义，那也许今天也就有不一样的自己。弗兰克尔说，人生不同的阶段，赖以寄托精神的意义不同，诚然信也。 近一年，对生活失去了信心，就算孩子已经出生，待在自己身边，我还是未能觉得生活有了多大的变化，还是不能找到我究竟要做什么，要成为什么样的人，活着的意义。所以看到这本书时，心底感触挺深，给予我的不仅仅是精神上的震撼，也是实际行动中指导。从此为了坚守自己活着的意义，努力过好每一天，让每时每刻都有意义。 “人对待一切事情，不论是生死攸关还是日常琐事-积极的态度有助于我们成就生命的意义”。从记事到现在，我一直是这样一个人：消极、自卑、不愿意麻烦别人、能不做的事情就不做、别人能替自己完成的事情自己绝不插手。导致现在的自己非常敏感，别人的一句话，生后中的一件小事，也会像影子一样跟着自己很久，直到另外一件事情替代之前的事情。说得真好啊，生死也好，日常吃饭睡觉也好，不管大大小小的事情，我都希望自己可以乐观的对待。相信自己会把乐观的心态传递给周围的人。 “不经历深刻的痛苦就无法体会生命的意义”任何事，只有自己经历过，才会深有体会。但无论经历何种苦难，何种不容易，请不要丢失善良和对生活的热爱。 我们真正需要的，是生活态度上的根本转换。我们需要了解自身，而且需要说服那些绝望的人：我们期望生活给予什么并不重要，重要的是生活对我们有什么期望。我们不应该再问生活的意义是什么，而因该像那些每时每刻都被生活质问的人那样去思考自身。我们回答不是说与想，而是采取正确的行动，生命意味着承担和接受所有的挑战，完成自己应该完成的任务这一绝大责任 对自己未来丧失信心的犯人，注定要走向毁灭。由于他对未来失去了信念，他也就丧失了对精神的把握。他自甘堕落，成为行尸走肉、 不要只想着成功——你越想成功，就越容易失败。成功就像幸福一样，可遇而不可求。它是一种自然而然的产物，是一个人无意识地投身于某一伟大的事业时产生的衍生品，或者是为他人奉献时的副产品。幸福总会降临的，成功也同样：常常是无心插柳柳成荫。我希望你们的一切行为服从良心，并用知识去实现它。总有一天你会发现，当然是相当长的时间之后——注意，我说的是很长一段时间后！——正是由于这种不关注，成功将降临于你 尼采说过：“知道为什么而活的人，便能生存。 一本非常不错的书。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"数据仓库数据集成算法","slug":"数据仓库数据集成算法20190524","date":"2017-09-07T00:54:59.000Z","updated":"2019-05-24T04:59:16.775Z","comments":true,"path":"2017/09/07/数据仓库数据集成算法20190524/","link":"","permalink":"http://qioinglong.top/2017/09/07/数据仓库数据集成算法20190524/","excerpt":"","text":"1.数据仓库数据集成数据仓库将源系统数据抽取到ODS或者ODS提供数据给数据仓库后，需要将不同来源的数据根据业务需求集成在同一模型中。总体来说，集成算法与ODS算法基本相同，区别在于集成时当日数据来源并不是唯一的。从模型上来说，大体分为历史表模型，当前表模型，流水表模型，特殊数据模型将根据业务需求特殊加载。 2.历史表数据加载算法历史表指当源系统数据被删除（物理删除或逻辑删除）或者失效后，数据仓库不会将数据物理删除，只将数据的结束日期闭链，从而保证将源系统数据变化的过程保留下来。基本加载算法如下： 先根据映射将源数据加载到临时表ND(可能多个源) 1234INSERT INTO $&#123;ND&#125;( $COLS )SELECT $COLSFROM $&#123;SRC_DB&#125;.$&#123;SRC_TBNAME&#125;; 得到目标表上日有效数据OD 全量：获取上日全部有效数据 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 增量：获取上日主键在ND中的有效数据 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE ($PRI_KEY_NAME) IN (SELECT $PRI_KEY_NAME FROM $&#123;ND&#125;)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125; 产生需要插入的数据wt_i（当日数据不在昨日数据中） 123456789101112131415INSERT INTO WT_I( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT ) SELECT $COLS ,$TX_DATE ,$MAX_DT ,$TX_DATE FROM $&#123;ND&#125; WHERE ($COLS_NOT_NULL) NOT IN (SELECT $COLS_NOT_NULL FROM OD) ; 产生关链数据(昨日数据不在当日数据中,源系统已经失效数据)，并将数据加在到临时表WT_U中 123456789101112131415INSERT INTO WT_U( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;TX_DATE&#125; ,DATA_DTFROM ODWHERE ( $COLS_NOT_NULL ) NOT IN (SELECT $COLS_NOT_NULL FROM $&#123;ND&#125;); 根据wt_u表将目标表的数据关链 1234567UPDATE $&#123;DB&#125;.$&#123;TBNAME&#125;SET $&#123;HT_E_DT&#125; =$&#123;TX_DATE&#125;WHERE ($COLS_NOT_NULL) IN (SELECT $COLS_NOT_NULL FROM WT_U)AND ($PRI_KEY_NAME) IN (SELECT $PRI_KEY_NAME FROM $&#123;ND&#125;)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 根据wt_i表将新增的数据和变化的数据插入到目标表包含两部分数据：上日数据存在但属性值变化的、新增数据 1234567891011INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT)SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DTFROM WT_I; 3.流水表数据加载算法通常流水表只保留在ODS层，但某些流水数据可能来源于多个源系统，为了后续数据使用方便，也将多个来源的流水表集成到一起。 删除数据日期为当日的数据，目的是支持重跑 123 DELETE FROM $&#123;DB&#125;.$&#123;TBNAME&#125; WHERE Data_Dt = $DATA_DT; 将源表数据按照映射插入到目标表 可以先将数据加载到临时表ND，再从ND插入目标表 也可以直接从源表将数据插入到目标表 1234567INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,DATA_DT)SELECT $COLS ,DATA_DTFROM $&#123;SRC_DB&#125;.$&#123;SRC_TABLENAME&#125;; 4.当前表数据加载算法当前表是指只保留最新的数据，源系统失效或者删除（物理删除或逻辑删除）的数据，在数据仓库集成的时候，不再保留，直接物理删除。当前表只保留有效的数据。算法与历史表基本相同(只是wt_u是直接删除，而不是关链) 先根据映射将源数据加载到临时表ND 1234INSERT INTO $&#123;ND&#125;( $COLS )SELECT $COLSFROM $&#123;SRC_DB&#125;.$&#123;SRC_TBNAME&#125;; 得到目标表上日有效数据OD 全量：获取上日全部有效数据 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 增量：获取上日主键在ND中的有效数据 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE ($PRI_KEY_NAME) IN (SELECT $PRI_KEY_NAME FROM $&#123;ND&#125;)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125; 产生需要插入的数据wt_i（当日数据不在昨日数据中） 123456789101112131415INSERT INTO WT_I( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT ) SELECT $COLS ,$TX_DATE ,$MAX_DT ,$TX_DATE FROM $&#123;ND&#125; WHERE ($COLS_NOT_NULL) NOT IN (SELECT $COLS_NOT_NULL FROM OD) ; 产生关链数据(昨日数据不在当日数据中,源系统已经失效数据)，并将数据加在到临时表WT_U中 123456789101112131415INSERT INTO WT_U( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;TX_DATE&#125; ,DATA_DTFROM ODWHERE ( $COLS_NOT_NULL ) NOT IN (SELECT $COLS_NOT_NULL FROM $&#123;ND&#125;); 根据wt_u表将目标表的数据删除 12345 DELETE T1 FROM $&#123;DB&#125;.$&#123;TABLENAME&#125; T1 ,wt_u T2 WHERE T1.PK1 = T2.PK2; 根据wt_i表将新增的数据和变化的数据插入到目标表包含两部分数据：上日数据存在但属性值变化的、新增数据 1234567891011INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT)SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DTFROM WT_I;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://qioinglong.top/tags/数据仓库/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"ODS数据加载算法","slug":"ODS数据加载算法20190524","date":"2017-09-06T00:54:59.000Z","updated":"2019-05-24T04:59:13.731Z","comments":true,"path":"2017/09/06/ODS数据加载算法20190524/","link":"","permalink":"http://qioinglong.top/2017/09/06/ODS数据加载算法20190524/","excerpt":"","text":"1.ODSODS（数据操作存储），是企业数据的抽取与交换平台。通常ODS的数据不做任何转化，只保留源系统数据。并将数据分发给数据仓库，数据集市等下游系统，并在ODS数据基础上开发各类报表。目前接触到的项目中，将数据仓库与ODS集成到一个项目中，共用同一个数据库。减少了数据传输，提高了资源的利用率。在ODS层数据中，我们将源系统数据变化的过程保留下来，由于源系统和ODS不在同一服务器上，源系统通常通过文件交换的方式传输数据，为了减少文件传输，数据文件可分为增量文件和全量文件。根据保留数据类型的不同，又分为流水数据与历史数据。通常我们在开发ODS时，涉及到以下几种算法，全量历史加载算法，增量历史加载算法，流水全量加载算法，流水增量加载算法，全量覆盖加载算法。 2.相关变量说明 变量名 变量描述 ${DB} 数据库实例名 ${TBNAME} 表名 $COLS 表字段，以逗号分隔，col1,col2.. $DATA_DT 数据加载日期 ${SRC_DB} 源数据库实例名 ${SRC_TBNAME} 源表名 ND 临时表，用于加载当日数据 OD 临时表，用于加载上日数据 ${HT_S_DT} 拉链开始日期 ${HT_E_DT} 拉链结束日期 ${TX_DATE} 交易日期，作业运行日期 WT_U 临时表，用于保存需要关链数据 WT_I 临时表，用于保存需要插入的数据 3.全量覆盖加载算法全量覆盖加载是所有加载中最简单的一种数据加载方式。它是指直接将目标表中数据删除并将源系统提供的数据文件直接加载到目标表中的过程。伪代码如下：12345678910//清除表数据TRUNCATE TABLE $&#123;DB&#125;.$&#123;TBNAME&#125; ;//将源表数据插入到目标表中INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,DATA_DT)SELECT $COLS ,TO_DATE(&apos;$DATA_DT&apos;, &apos;YYYYMMDD&apos;)FROM $&#123;SRC_DB&#125;.$&#123;SRC_TBNAME&#125;; 4.全量历史加载算法 将当日数据加载到ND表中 1234INSERT INTO $&#123;ND&#125;( $COLS )SELECT $COLSFROM $&#123;SRC_DB&#125;.$&#123;SRC_TBNAME&#125;; 将上日正常数据加载到临时表OD中 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 产生插入结果数据（新增数据，当日数据不在昨日数据中），并将数据加载到临时表WT_I中 123456789101112131415INSERT INTO WT_I( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$TX_DATE ,$MAX_DT ,$TX_DATEFROM $&#123;ND&#125;WHERE ($COLS_NOT_NULL) NOT IN (SELECT $COLS_NOT_NULL FROM OD); 产生关链数据(昨日数据不在当日数据中,已经被源系统删除的数据)，并将数据加在到临时表WT_U中 123456789101112131415INSERT INTO WT_U( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;TX_DATE&#125; ,DATA_DTFROM ODWHERE ( $COLS_NOT_NULL ) NOT IN (SELECT $COLS_NOT_NULL FROM $&#123;ND&#125;); 将目标表中已经失效的数据关链 123456UPDATE $&#123;DB&#125;.$&#123;TBNAME&#125;SET $&#123;HT_E_DT&#125; = $&#123;TX_DATE&#125;WHERE ($COLS_NOT_NULL) IN (SELECT $COLS_NOT_NULL FROM WT_U)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 将新增数据插入插入到目标表中 1234567891011INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT)SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DTFROM WT_I; 5.增量历史加载算法1.将当日数据加载到临时表ND表中1234INSERT INTO $&#123;ND&#125;( $COLS )SELECT $COLSFROM $&#123;DB&#125;.$&#123;SRC_TBNAME&#125;; 将上日正常数据加载到临时表OD中(根据表主键查找当日数据) 12345INSERT INTO ODSELECT * FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE ($PRI_KEY_NAME) IN (SELECT $PRI_KEY_NAME FROM $&#123;ND&#125;)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125; 产生插入结果数据（当日数据不在昨日数据中），并将数据加载到临时表WT_I中 123456789101112131415INSERT INTO WT_I( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$TX_DATE ,$MAX_DT ,$TX_DATEFROM $&#123;ND&#125;WHERE ($COLS_NOT_NULL) NOT IN (SELECT $COLS_NOT_NULL FROM OD); 产生关链数据(昨日数据不在当日数据中,源系统已经失效数据)，并将数据加在到临时表WT_U中 123456789101112131415INSERT INTO WT_U( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT )SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;TX_DATE&#125; ,DATA_DTFROM ODWHERE ( $COLS_NOT_NULL ) NOT IN (SELECT $COLS_NOT_NULL FROM $&#123;ND&#125;); 将目标表中失效的数据关链 1234567UPDATE $&#123;DB&#125;.$&#123;TBNAME&#125;SET $&#123;HT_E_DT&#125; =$&#123;TX_DATE&#125;WHERE ($COLS_NOT_NULL) IN (SELECT $COLS_NOT_NULL FROM WT_U)AND ($PRI_KEY_NAME) IN (SELECT $PRI_KEY_NAME FROM $&#123;ND&#125;)AND $&#123;HT_E_DT&#125; &gt; $&#123;TX_DATE&#125; AND $&#123;HT_S_DT&#125; &lt;= $&#123;TX_DATE&#125;; 插入当日新增数据 1234567891011INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DT)SELECT $COLS ,$&#123;HT_S_DT&#125; ,$&#123;HT_E_DT&#125; ,DATA_DTFROM WT_I; 6.流水增量加载算法 清除当日数据 123DELETE FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE DATA_DT = &apos;$DATA_DT&apos;; 插入数据 1234567891011121314INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,DATA_DT)SELECT $COLS ,&apos;$DATA_DT&apos;FROM $&#123;DB&#125;.$&#123;SRC_TBNAME&#125;MINUS ALLSELECT $COLS ,&apos;$DATA_DT&apos;FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE DATA_DT &lt;= $&#123;TX_DATE&#125;; 7.流水全量加载算法 清除当日数据 12DELETE FROM $&#123;DB&#125;.$&#123;TBNAME&#125;WHERE DATA_DT = $DATA_DT 插入数据 123456789101112INSERT INTO $&#123;DB&#125;.$&#123;TBNAME&#125;( $COLS ,DATA_DT ,HT_S_DT ,HT_E_DT )SELECT $COLS ,&apos;$DATA_DT&apos; ,&apos;$DATA_DT&apos; ,&apos;$MAX_DT&apos;FROM $&#123;DB&#125;.$&#123;SRC_TBNAME&#125;;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://qioinglong.top/tags/数据仓库/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"月结月策_201708","slug":"月结月策_20170820190524","date":"2017-09-03T00:54:59.000Z","updated":"2019-05-24T04:39:13.423Z","comments":true,"path":"2017/09/03/月结月策_20170820190524/","link":"","permalink":"http://qioinglong.top/2017/09/03/月结月策_20170820190524/","excerpt":"2017在路上","text":"2017在路上 读书 本月阅读基本荒废：思考了自己本月读书少以及没有坚持下去的原因：1.最近带孩子，还没有找到一个平衡点。2.上下班通勤没有读书，白白浪费了时间。此后还是基于kindle阅读比较好点。3.大前研一《思考的技术》，读了大概一半左右。随时随地保持思考，不盲目做事情，多问为什么。4.《数据挖掘导论》，看到一半实在看不下去，虽然看了一半，但是自己吸收的很少，这本书适合有基础的人读。书虽然是好书，但是对于数学基础不扎实的我来说，读完，吸收完比较困难。5.李航《统计学习基础》，看了第一章，没看下去。 计划下月完成5本：1.大前研一《思考的技术》2.《数据仓库基础》3.蒙田随笔4.偷书贼5.优雅的老去 工作 本月工作比较轻松，了解的数据仓库的ODS算法，PDM算法，ODS开发过程，PDM开发过程。总体来说，比较轻松。下月计划了解数据仓库建模以及源系统模型映射。本月，对自己工作想了很多。按照目前的状态，一直做开发并不是自己最佳的选择，所以决定，往数据分析方向转型。 运动 这个月运动基本废了，下月重启跑步计划，每周三次夜跑 总结 对自己本月的状态比较不满意，下月得很努力才行，人生总有些东西需要坚持才行。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"月结月策","slug":"月结月策","permalink":"http://qioinglong.top/tags/月结月策/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"Hbase-Spark读写HBase数据","slug":"Hbase-Spark读写HBase数据20190524","date":"2017-07-16T15:54:59.000Z","updated":"2019-05-24T04:59:12.586Z","comments":true,"path":"2017/07/16/Hbase-Spark读写HBase数据20190524/","link":"","permalink":"http://qioinglong.top/2017/07/16/Hbase-Spark读写HBase数据20190524/","excerpt":"1.Spark写入数据到HBase HBase表结构 12##创建hbase表create &apos;user&apos;,&apos;basic&apos; 定义HBase定义 12345val hbaseConf=HBaseConfiguration.create()//zookeeper端口hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)//zookeeper地址hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;)","text":"1.Spark写入数据到HBase HBase表结构 12##创建hbase表create &apos;user&apos;,&apos;basic&apos; 定义HBase定义 12345val hbaseConf=HBaseConfiguration.create()//zookeeper端口hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)//zookeeper地址hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;) 指定输出格式和输出表名 123val jobConf=new JobConf(hbaseConf,this.getClass)jobConf.setOutputFormat(classOf[TableOutputFormat])jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;user&quot;) 定义转换格式 1234567将RDD[(uid:Int, name:String, age:Int)] 转换成 RDD[(ImmutableBytesWritable, Put)]def convert(triple:(Int,String,Int))=&#123; val put=new Put(Bytes.toBytes(triple._1)) put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(triple._2)) put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(triple._3)) (new ImmutableBytesWritable,put) &#125; 保存 12//使用saveAsHadoopDataset方法写入HBaselocalData.saveAsHadoopDataset(jobConf) 2.Spark写HBase示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.crm.baseimport org.apache.spark.sql.SQLContextimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.hive.HiveContextimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapred.JobConfimport org.apache.hadoop.hbase.mapred.TableOutputFormatimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.io.ImmutableBytesWritableobject TestHbaseWrite &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) val sqlContext=new SQLContext(sc) val hiveContext=new HiveContext(sc) //导入隐式转换 import sqlContext.implicits._ //定义HBase定义 val hbaseConf=HBaseConfiguration.create() hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;) hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;) //指定输出格式和输出表名 val jobConf=new JobConf(hbaseConf,this.getClass) jobConf.setOutputFormat(classOf[TableOutputFormat]) jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;user&quot;) //read RDD data from somewhere and convert val rawData = List((1,&quot;lilei&quot;,14), (2,&quot;hanmei&quot;,18), (3,&quot;someone&quot;,38)) val localData= sc.parallelize(rawData).map(convert) //使用saveAsHadoopDataset方法写入HBase localData.saveAsHadoopDataset(jobConf) &#125;//将RDD[(uid:Int, name:String, age:Int)] 转换成 RDD[(ImmutableBytesWritable, Put)] def convert(triple:(Int,String,Int))=&#123; val put=new Put(Bytes.toBytes(triple._1)) put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(triple._2)) put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(triple._3)) (new ImmutableBytesWritable,put) &#125;&#125; 3.Spark读HBase数据Spark读取HBase，我们主要使用SparkContext 提供的newAPIHadoopRDDAPI将表的内容以 RDDs 的形式加载到 Spark 中。 定义HBase定义 123val hbaseConf=HBaseConfiguration.create()hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;) 设置查询的表名 12hbaseConf.set(TableInputFormat.INPUT_TABLE, &quot;user&quot;)val userRdd=sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result]) 遍历输出 12345678userRdd.foreach&#123;case (_,result)=&gt;&#123; val key=Bytes.toInt(result.getRow) val name=Bytes.toString(result.getValue(&quot;basic&quot;.getBytes, &quot;name&quot;.getBytes)) val age =Bytes.toInt(result.getValue(&quot;basic&quot;.getBytes, &quot;age&quot;.getBytes)) println(key) println(name) println(age) &#125;&#125; 4.park读HBase示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.crm.baseimport org.apache.spark.sql.SQLContextimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.hive.HiveContextimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.mapred.TableInputFormatBaseimport org.apache.hadoop.hbase.mapreduce.TableInputFormatimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.client.Resultimport org.apache.hadoop.hbase.util.Bytes/** * Spark读取HBase，我们主要使用SparkContext 提供的newAPIHadoopRDDAPI将表的内容以 RDDs 的形式加载到 Spark 中。 */object TestHbaseRead &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) val sqlContext=new SQLContext(sc) val hiveContext=new HiveContext(sc) //导入隐式转换 import sqlContext.implicits._ //定义HBase定义 val hbaseConf=HBaseConfiguration.create() hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;) hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;) //设置查询的表名 hbaseConf.set(TableInputFormat.INPUT_TABLE, &quot;user&quot;) val userRdd=sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result]) val count=userRdd.count() println(count) //遍历输出 userRdd.foreach&#123;case (_,result)=&gt;&#123; val key=Bytes.toInt(result.getRow) val name=Bytes.toString(result.getValue(&quot;basic&quot;.getBytes, &quot;name&quot;.getBytes)) val age =Bytes.toInt(result.getValue(&quot;basic&quot;.getBytes, &quot;age&quot;.getBytes)) println(key) println(name) println(age) &#125;&#125; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://qioinglong.top/tags/Hbase/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"大数据场景学习之实时更新HBase数据库(三)","slug":"大数据场景学习之实时更新HBase数据库(三)20190524","date":"2017-07-15T13:54:59.000Z","updated":"2019-05-24T04:59:16.716Z","comments":true,"path":"2017/07/15/大数据场景学习之实时更新HBase数据库(三)20190524/","link":"","permalink":"http://qioinglong.top/2017/07/15/大数据场景学习之实时更新HBase数据库(三)20190524/","excerpt":"一.业务场景 从Kafka中读取数据，并进行一定的加工 利用phoenix将加载的数据更新到HBase中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7 hbase-1.2.5 kafka-2.12 phoenix-4.10","text":"一.业务场景 从Kafka中读取数据，并进行一定的加工 利用phoenix将加载的数据更新到HBase中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7 hbase-1.2.5 kafka-2.12 phoenix-4.10 三. Hive数据库相关表12345678910111213141516create table if not exists crm.bcst_t_ep_bal(rowkey String comment &apos;cust_no&apos;,cust_no String comment &apos;客户号 &apos;,bal Double comment &apos;存款余额 &apos;)comment &apos;对公客户余额汇总信息&apos;--ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&apos;--WITH SERDEPROPERTIES (&quot;field.delim&quot;=&quot;@|@&quot;)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:cust_no ,cf:bal &quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;crm:bcst_t_ep_bal&quot;, &quot;hbase.mapred.output.outputtable&quot; = &quot;crm:bcst_t_ep_bal&quot;); 四.Kafka交互基础类1234567891011121314151617181920212223242526272829303132333435package com.crm.queryimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.broadcast.Broadcastimport kafka.serializer.StringDecoderimport org.apache.spark.streaming.kafka.KafkaUtils/** * 与kafka交互的基础类, */trait SparkStreamingKafkaBase &#123; /** * 初始化PhoenixBroadcast */ def initPhoenixConnectorBroadcast(ssc:StreamingContext,phoenixUrl:String)=&#123; val phoenixConnectorBroadcast:Broadcast[PhoenixConnector]=&#123; println(&quot;phoenixUrl&quot; + phoenixUrl) ssc.sparkContext.broadcast(PhoenixConnector(phoenixUrl)) &#125; phoenixConnectorBroadcast &#125; /** * 初始化kafka读取 */ def initKafkaStream(ssc: StreamingContext,kafkaBroker:String, kafkaTopics:String) = &#123; val kafkaParams= Map[String,String](&quot;metadata.broker.list&quot;-&gt;kafkaBroker) //创建一个Set，里面放要读取的topic val topics = kafkaTopics.split(&quot;,&quot;).toSet val kafkaStream = KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc, kafkaParams, topics) kafkaStream &#125;&#125; 五.Phoenix连接类12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.crm.queryimport java.sql.DriverManagerimport java.sql.Connectionimport java.sql.Statementimport scala.collection.mutable.ArrayBufferclass PhoenixConnector (createConnection: () =&gt; Connection) extends Serializable&#123; lazy val conn = createConnection() def executeBatchUpdate(sqls:List[String])&#123; var stat:Statement = null try&#123; stat=conn.createStatement() sqls.foreach(sql=&gt;&#123; stat.addBatch(sql) println(&quot;sql=&gt;&quot; + sql) &#125;) stat.executeBatch() conn.commit() &#125;catch&#123; case e:Exception =&gt; println(&quot;PhoenixConnector.executeUpdate error :&quot; + e.getMessage) &#125;finally&#123; if(stat !=null)&#123; stat.close() &#125; &#125; &#125;&#125;object PhoenixConnector&#123; def apply(phoenixUrl: String): PhoenixConnector = &#123; val createProducerFunc = () =&gt; &#123; val conn = DriverManager.getConnection(phoenixUrl) conn.setAutoCommit(false) conn &#125; new PhoenixConnector(createProducerFunc) &#125; def main(args: Array[String]): Unit = &#123; var arraybuffer = new ArrayBuffer[String]() &#125;&#125; 六.实现类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.crm.queryimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.Secondsimport com.typesafe.config.ConfigFactoryimport scala.collection.mutable.ArrayBuffer/** * 通过spark steaming 读取kafka数据，并通过phoenix更新到Hbase中去 */object SparkStreamingHandlePhoenixCustBal extends SparkStreamingKafkaBase&#123; //加载配置文件 val config=ConfigFactory.load() def main(args: Array[String]): Unit = &#123; //读取启动程序的参数 val Array( batchDuration //时间窗口大小 ) =args // 获取配置信息 val kafkaBroker=config.getString(&quot;kafka-broker&quot;) val kafkaTopics=config.getString(&quot;kafkaTopics&quot;) val phoenixUrl=config.getString(&quot;phoenix-url&quot;) val checkpointPath=config.getString(&quot;checkpoint-path&quot;) val hbaseTableName=config.getString(&quot;hbaseTableName&quot;) val conf= new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;SparkStreamingHandlePhoenixCustBal&quot;) val sc = new SparkContext(conf) //创建streamingcontext val ssc=new StreamingContext(sc,Seconds(batchDuration.toInt)) //设置checkpoint //ssc.checkpoint(s&quot;$checkpointPath/SparkStreamingHandlePhoenixCustBal&quot;) // 定义phoenix广播对象 val phoenixBroadcast = initPhoenixConnectorBroadcast(ssc, phoenixUrl) // 读取kafka流数据 val kafkaParams=&quot;&quot; val topics=&quot;&quot; val kafkaStream = initKafkaStream(ssc, kafkaBroker, kafkaTopics).map(_._2) //kafkaStream.print() /** * 消息格式 P00001@|@30000 * val keyString=&quot;&quot; val valueString=&quot;&quot; val upsertSql = s&quot;UPSERT INTO $hbaseTableName ($keyString) VALUES ($valueString)&quot; */ kafkaStream.foreachRDD(rdd=&gt;&#123; var batchSql= new ArrayBuffer[String]() rdd.collect().foreach(messages=&gt;&#123; println(messages) val custNo=messages.split(&quot;@\\\\|@&quot;)(0) val bal=messages.split(&quot;@\\\\|@&quot;)(1) //val keyString=Array(&quot;key&quot;,&quot;cust_no&quot;,&quot;bal&quot;).mkString(&quot;,&quot;) val keyString=config.getString(&quot;tblKeys&quot;) val valueString=Array(&quot;&apos;&quot;+custNo+&quot;&apos;&quot;,&quot;&apos;&quot;+custNo+&quot;&apos;&quot;,&quot;&apos;&quot;+bal+&quot;&apos;&quot;).mkString(&quot;,&quot;) println(custNo) println(bal) val upsertSql = s&quot;UPSERT INTO $hbaseTableName($keyString) VALUES ($valueString)&quot; println(&quot;upsertSql&quot;+upsertSql) batchSql.append(upsertSql) &#125;) //println(&quot;batchSql &quot; +batchSql.toList) val start=System.currentTimeMillis() if(batchSql.isEmpty)&#123; println(&quot;data is empty&quot;) &#125;else&#123; println(phoenixBroadcast.value) phoenixBroadcast.value.executeBatchUpdate(batchSql.toList) &#125; var useTime = System.currentTimeMillis()-start println(s&quot;batch update time $useTime&quot;) &#125;) ssc.start() ssc.awaitTermination() // 确保所有executor都执行完了才关闭程序 sys.addShutdownHook(&#123; ssc.stop(true,true) &#125;) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"大数据场景学习","slug":"大数据场景学习","permalink":"http://qioinglong.top/tags/大数据场景学习/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"大数据场景学习之Hive与HBase(二)","slug":"大数据场景学习之Hive与HBase(二)20190524","date":"2017-07-15T12:54:59.000Z","updated":"2019-05-24T04:59:16.643Z","comments":true,"path":"2017/07/15/大数据场景学习之Hive与HBase(二)20190524/","link":"","permalink":"http://qioinglong.top/2017/07/15/大数据场景学习之Hive与HBase(二)20190524/","excerpt":"一.业务场景 从Hive中加载数据，并进行一定的加工 将加载的数据更新到HBase中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7 hbase-1.2.5","text":"一.业务场景 从Hive中加载数据，并进行一定的加工 将加载的数据更新到HBase中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7 hbase-1.2.5 三. Hive数据库相关表12345678910111213141516create table if not exists crm.bcst_t_ep_bal(rowkey String comment &apos;cust_no&apos;,cust_no String comment &apos;客户号 &apos;,bal Double comment &apos;存款余额 &apos;)comment &apos;对公客户余额汇总信息&apos;--ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&apos;--WITH SERDEPROPERTIES (&quot;field.delim&quot;=&quot;@|@&quot;)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:cust_no ,cf:bal &quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;crm:bcst_t_ep_bal&quot;, &quot;hbase.mapred.output.outputtable&quot; = &quot;crm:bcst_t_ep_bal&quot;); 四.实现类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.crm.queryimport org.apache.spark.SparkConfimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.SparkContextimport com.typesafe.config.ConfigFactoryimport org.apache.spark.sql.SQLContextimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapred.JobConfimport org.apache.hadoop.hbase.mapred.TableOutputFormatimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.spark.sql.Rowobject UpdateHbaseCustBal &#123; def main(args: Array[String]): Unit = &#123; val config = ConfigFactory.load(&quot;application.conf&quot;) val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) val sqlContext=new SQLContext(sc) val hiveContext=new HiveContext(sc) //定义HBase定义 val hbaseConf=HBaseConfiguration.create() hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;) hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;) //导入隐式转换 import sqlContext.implicits._ val sql=&quot;&quot;&quot;select COALESCE(cust_no,&apos;NULL&apos;) as rowkey, COALESCE(cust_no,&apos;NULL&apos;), sum(coalesce(bal,0)) as bal from crm.bacc_t_ep_deps where trim(cust_no) is not null group by COALESCE(cust_no,&apos;NULL&apos;) &quot;&quot;&quot; //println(sql) val bacc_t_ep_depsDF=hiveContext.sql(sql) //bacc_t_ep_depsDF.show() //指定输出格式和输出表名 val jobConf=new JobConf(hbaseConf,this.getClass) jobConf.setOutputFormat(classOf[TableOutputFormat]) val outPutTbl=config.getString(&quot;updateHbaseCustBalOutPutTbl&quot;) jobConf.set(TableOutputFormat.OUTPUT_TABLE,outPutTbl) //更新数据 val bacc_t_ep_depsRowRDD=bacc_t_ep_depsDF.rdd.map(row=&gt;&#123; (row(0).toString,row(1).toString,row(2).toString) &#125;).map(convert).saveAsHadoopDataset(jobConf) sc.stop() &#125; //rdd to hbase convert function def convert(tuple:(String,String,String))=&#123; println(tuple) val put=new Put(Bytes.toBytes(tuple._1)) //列族，列名，具体值 put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;cust_no&quot;), Bytes.toBytes(tuple._2)) put.addColumn(Bytes.toBytes(&quot;cf&quot;), Bytes.toBytes(&quot;bal&quot;), Bytes.toBytes(tuple._3)) (new ImmutableBytesWritable,put) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"大数据场景学习","slug":"大数据场景学习","permalink":"http://qioinglong.top/tags/大数据场景学习/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"大数据场景学习之Hive与关系数据库(一)","slug":"大数据场景学习之Hive与关系数据库(一)20190524","date":"2017-07-15T07:54:59.000Z","updated":"2019-05-24T04:59:16.675Z","comments":true,"path":"2017/07/15/大数据场景学习之Hive与关系数据库(一)20190524/","link":"","permalink":"http://qioinglong.top/2017/07/15/大数据场景学习之Hive与关系数据库(一)20190524/","excerpt":"一.业务场景 从关系数据库中获取任务信息 根据从数据库中的任务信息，从Hive中过滤满足条件的数据 将数据加载到回关系数据库中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7","text":"一.业务场景 从关系数据库中获取任务信息 根据从数据库中的任务信息，从Hive中过滤满足条件的数据 将数据加载到回关系数据库中 二.开发环境 MySQL hive-2.1.1 hadoop-2.7.2 spark-2.1.0-bin-hadoop2.7 三.MySQL 数据库相关表 crm_brch_topN_info 12345678CREATE TABLE `crm_brch_topN_info` ( `statt_dt` date DEFAULT NULL, `2` int(11) NOT NULL, `cust_no` text, `org_no` text, `bal` double DEFAULT NULL, `rn` int(11) DEFAULT NULL) ENGINE=MyISAM DEFAULT CHARSET=utf8 task 12345678910111213CREATE TABLE `task` ( `task_id` int(11) NOT NULL AUTO_INCREMENT, `task_name` varchar(255) DEFAULT NULL, `create_time` varchar(255) DEFAULT NULL, `start_time` varchar(255) DEFAULT NULL, `finish_time` varchar(255) DEFAULT NULL, `task_type` varchar(255) DEFAULT NULL, `task_status` varchar(255) DEFAULT NULL, `task_param` text, PRIMARY KEY (`task_id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8task_id task_name create_time start_time finish_time task_type task_status task_param2 取排名前十客户 1 22 333 33 333 &#123;&quot;brchId&quot;:&quot;88088,85018&quot;&#125; 四.配置文件application.conf12345678910111213jdbc.driver:&quot;com.mysql.jdbc.Driver&quot;jdbc.url:&quot;jdbc:mysql://hadoop01:3306/sparkpro&quot;jdbc.user:&quot;root&quot;jdbc.password:&quot;root123&quot;taskTableName:&quot;task&quot;custBrchBalTopN:&quot;crm_brch_topN_info&quot;updateHbaseCustBalOutPutTbl:&quot;crm:bcst_t_ep_bal&quot;kafka-broker:&quot;hadoop01:9092,hadoop02:9092,hadoop03:9092&quot;checkpoint-path:&quot;/user/kafka/checkpoint&quot;phoenix-url:&quot;jdbc:phoenix:hadoop01,hadoop02,hadoop03&quot;kafkaTopics:&quot;custbal,custbal_nbc&quot;hbaseTableName:&quot;&quot;&quot;&quot;crm:bcst_t_ep_bal&quot;&quot;&quot;&quot;tblKeys:&quot;&quot;&quot;&quot;key&quot;,&quot;cust_no&quot;,&quot;bal&quot;&quot;&quot;&quot; 五.pom.xml信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.yangql&lt;/groupId&gt; &lt;artifactId&gt;spark&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt; &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;scala-tools.org&lt;/id&gt; &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt; &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;scala-tools.org&lt;/id&gt; &lt;name&gt;Scala-Tools Maven2 Repository&lt;/name&gt; &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.specs&lt;/groupId&gt; &lt;artifactId&gt;specs&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-flume_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;0.13.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpcore&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.play&lt;/groupId&gt; &lt;artifactId&gt;play-json_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.joda&lt;/groupId&gt; &lt;artifactId&gt;joda-convert&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-common --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.10.0-HBase-1.2&lt;/version&gt;&lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;/scalaVersion&gt; &lt;args&gt; &lt;arg&gt;-target:jvm-1.8&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;downloadSources&gt;true&lt;/downloadSources&gt; &lt;buildcommands&gt; &lt;buildcommand&gt;ch.epfl.lamp.sdt.core.scalabuilder&lt;/buildcommand&gt; &lt;/buildcommands&gt; &lt;additionalProjectnatures&gt; &lt;projectnature&gt;ch.epfl.lamp.sdt.core.scalanature&lt;/projectnature&gt; &lt;/additionalProjectnatures&gt; &lt;classpathContainers&gt; &lt;classpathContainer&gt;org.eclipse.jdt.launching.JRE_CONTAINER&lt;/classpathContainer&gt; &lt;classpathContainer&gt;ch.epfl.lamp.sdt.launching.SCALA_CONTAINER&lt;/classpathContainer&gt; &lt;/classpathContainers&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;scalaVersion&gt;$&#123;scala.version&#125;&lt;/scalaVersion&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt;&lt;/project&gt; 六.工具类12345678910111213141516171819202122232425262728293031323334package com.crm.utilsimport play.api.libs.json.Jsonobject HandleParasUtils &#123; /** * 从Json中获取提取指定值 */ def getParam(param:String,field:String):Option[String]=&#123; val json=Json.parse(param) val result=(json \\ field).asOpt[String] result &#125; /** * 机构拼接 * 源 xxxx,yyyy * 目标 &apos;xxxx&apos;,&apos;yyyy&apos; */ def brchIdsConcat(brchIds:String):String=&#123; val splitedBrchIds=brchIds.split(&quot;,&quot;) var result=&quot;&quot; for(i &lt;- (0 until splitedBrchIds.length))&#123; result += &quot;&apos;&quot;+splitedBrchIds(i) +&quot;&apos;,&quot; &#125; result=&quot;(&quot;+result.substring(0,result.length()-1)+&quot;)&quot; result &#125; def main(args: Array[String]): Unit = &#123; println(brchIdsConcat(&quot;xxxx,yyyy,zzzzzz&quot;)) &#125;&#125; 七.实现类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.crm.queryimport org.apache.spark.sql.SparkSessionimport java.sql.DriverManagerimport org.apache.spark.SparkConfimport org.apache.spark.sql.SQLContextimport org.apache.spark.SparkContextimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.types.StructFieldimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.types.DoubleTypeimport org.apache.spark.sql.Rowimport com.project.bean.Taskimport com.project.dao.impl.DaoFactoryimport com.typesafe.config.ConfigFactoryimport com.crm.utils.HandleParasUtilsimport scala.util.Propertiesimport org.apache.spark.sql.SaveModeobject QueryBrchTopNCustomer &#123; def main(args: Array[String]): Unit = &#123; //通过参数设置程序启动参数 val Array( taskId, //任务ID statt_dt //统计日期 )=args if(taskId==null)&#123; println(&quot;请输入任务编号&quot;) sys.exit() &#125; //加载作业QueryBrchTopNCustomer得配置文件 val config = ConfigFactory.load(&quot;application.conf&quot;) val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) val sqlContext=new SQLContext(sc) val hiveContext=new HiveContext(sc) //导入隐式转换 import sqlContext.implicits._ val url=config.getString(&quot;jdbc.url&quot;) val user=config.getString(&quot;jdbc.user&quot;) val password=config.getString(&quot;jdbc.password&quot;) val targetTblName=config.getString(&quot;custBrchBalTopN&quot;) //根据任务编号将任务找出来并获得任务得参数 val taskDao= DaoFactory.getTaskDao() val task=taskDao.findByKey(taskId.toLong) val taskParam=task.task_param val brchIds=HandleParasUtils.getParam(taskParam, &quot;brchId&quot;).getOrElse(&quot;000000&quot;) /** * 源 xxxx,yyyy * 目标 &apos;xxxx&apos;,&apos;yyyy&apos; */ val whereCond=HandleParasUtils.brchIdsConcat(brchIds) hiveContext.sql(&quot;use crm&quot;) /** * 数据分组排序 */ //println(statt_dt) val sql=&quot;&quot;&quot;select to_date(&apos;&quot;&quot;&quot;+statt_dt+&quot;&quot;&quot;&apos;) as statt_dt,&quot;&quot;&quot;+ taskId+&quot;&quot;&quot;, cust_no, org_no, sum(COALESCE(bal,0)) as bal , row_number() over(partition by org_no order by sum(nvl(bal,0))) rn from crm.bacc_t_ep_deps where org_no in &quot;&quot;&quot; + whereCond + &quot;&quot;&quot; group by cust_no,org_no &quot;&quot;&quot; //println(sql) val bacc_t_ep_depsDF=hiveContext.sql(sql) bacc_t_ep_depsDF.show(20,true) //设置mysql相关参数 val props=new java.util.Properties props.setProperty(&quot;user&quot;, user) props.setProperty(&quot;password&quot;, password) //将DF数据写入到mysql bacc_t_ep_depsDF.write.mode(SaveMode.Overwrite).jdbc(url, targetTblName, props) sc.stop() &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"大数据场景学习","slug":"大数据场景学习","permalink":"http://qioinglong.top/tags/大数据场景学习/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Elasticsearch 基础","slug":"Elasticsearch 基础20190524","date":"2017-06-29T00:54:59.000Z","updated":"2019-05-24T04:59:12.177Z","comments":true,"path":"2017/06/29/Elasticsearch 基础20190524/","link":"","permalink":"http://qioinglong.top/2017/06/29/Elasticsearch 基础20190524/","excerpt":"1.相关概念 接近实时（NRT）Elasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。 集群（cluster）一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。一个集群由一个唯一的名字标识，这个名字默认就是“elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。在产品环境中显式地设定这个名字是一个好习惯，但是使用默认值来进行测试/开发也是不错的。 节点（node）一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网络中的哪些服务器对应于Elasticsearch集群中的哪些节点。一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。 索引（index）一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，如果你想，可以定义任意多的索引。 类型（type）在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。 文档（document）一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。 分片和复制（shards &amp; replicas）一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片之所以重要，主要有两方面的原因 - 允许你水平分割/扩展你的内容容量 - 允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。复制之所以重要，有两个主要原因： - 在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。 - 扩展你的搜索量/吞吐量，因为搜索可以在所有的复制上并行运行 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。","text":"1.相关概念 接近实时（NRT）Elasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。 集群（cluster）一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。一个集群由一个唯一的名字标识，这个名字默认就是“elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。在产品环境中显式地设定这个名字是一个好习惯，但是使用默认值来进行测试/开发也是不错的。 节点（node）一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网络中的哪些服务器对应于Elasticsearch集群中的哪些节点。一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。 索引（index）一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，如果你想，可以定义任意多的索引。 类型（type）在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。 文档（document）一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。 分片和复制（shards &amp; replicas）一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片之所以重要，主要有两方面的原因 - 允许你水平分割/扩展你的内容容量 - 允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。复制之所以重要，有两个主要原因： - 在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。 - 扩展你的搜索量/吞吐量，因为搜索可以在所有的复制上并行运行 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。 2.基本操作 elasticsearch 命令方式： 1curl -&lt;REST Verb&gt; &lt;Node&gt;:&lt;Port&gt;/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; 要检查集群健康 123[yangql@hadoop01 bin]$ curl &apos;hadoop01:9200/_cat/health?v&apos;epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1498747113 10:38:33 es-cluster green 当我们询问集群状态的时候，我们要么得到绿色、黄色或红色。绿色代表一切正常（集群功能齐全），黄色意味着所有的数据都是可用的，但是某些复制没有被分配（集群功能齐全），红色则代表因为某些原因，某些数据不可用。注意，即使是集群状态是红色的，集群仍然是部分可用的（它仍然会利用可用的分片来响应搜索请求），但是可能你需要尽快修复它，因为你有丢失的数据。 集群中的节点列表 12345[yangql@hadoop01 bin]$ curl &apos;hadoop01:9200/_cat/nodes?v&apos;ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name192.168.1.231 3 94 0 0.00 0.00 0.00 mdi * es01192.168.1.232 3 94 0 0.01 0.01 0.00 mdi - es02192.168.1.233 3 92 0 0.00 0.00 0.00 mdi - es03 列出所有的索引 123[yangql@hadoop01 bin]$ curl &apos;hadoop01:9200/_cat/indices?v&apos;health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open customer 2J6OutOFS9OpLlTFISH7Tw 5 1 0 0 1.2kb 650b 创建一个索引现在让我们创建一个叫做“customer”的索引，然后再列出所有的索引： 1234curl -XPUT &apos;hadoop01:9200/customer[yangql@hadoop01 bin]$ curl &apos;hadoop01:9200/_cat/indices?v&apos;health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open customer 2J6OutOFS9OpLlTFISH7Tw 5 1 0 0 1.2kb 650b 我们现在有一个叫做customer的索引，并且它有5个主分片和1份复制（都是默认值），其中包含0个文档。 创建一个文档123456789101112131415161718[yangql@hadoop01 bin]$ curl -XPUT &apos;hadoop01:9200/customer/external/1?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;name&quot;: &quot;John Doe&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125;[yangql@hadoop01 bin]$ external”类型,文档的ID是1,Elasticsearch在你想将文档索引到某个索引的时候，并不强制要求这个索引被显式地创建。在前面这个例子中，如果customer索引不存在，Elasticsearch将会自动地创建这个索引。 查询一个文档 1234567891011[yangql@hadoop01 bin]$ curl -XGET &apos;hadoop01:9200/customer/external/1?pretty&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;John Doe&quot; &#125;&#125; 删除 1234[yangql@hadoop01 bin]$ curl -XDELETE &apos;hadoop01:9200/customer?pretty&apos;&#123; &quot;acknowledged&quot; : true&#125; 替换文档 1234567891011121314151617181920212223242526272829303132333435[yangql@hadoop01 bin]$ curl -XPUT &apos;hadoop01:9200/customer/external/1?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;name&quot;: &quot;John Doe&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125;[yangql@hadoop01 bin]$ curl -XPUT &apos;hadoop01:9200/customer/external/1?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;name&quot;: &quot;Jane Doe&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : false&#125;[yangql@hadoop01 bin]$ 以上的命令将ID为1的文档的name字段的值从“John Doe”改成了“Jane Doe”。如果我们使用一个不同的ID，一个新的文档将会被索引，当前已经在索引中的文档不会受到影响。 无ID创建document在创建document的时候，ID部分是可选的。如果不指定，Elasticsearch将产生一个随机的ID来索引这个文档。Elasticsearch生成的ID会作为索引API调用的一部分被返回。 123456789101112131415161718[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/external?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;name&quot;: &quot;Jane Doe&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;AVz0W-I9v5C-xxEQKDDx&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125;[yangql@hadoop01 bin]$ 更新文档除了可以索引、替换文档之外，我们也可以更新一个文档。但要注意，Elasticsearch底层并不支持原地更新。在我们想要做一次更新的时候，Elasticsearch先删除旧文档，然后在索引一个更新过的新文档。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/external/1/_update?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot; &#125;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;result&quot; : &quot;noop&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 0, &quot;successful&quot; : 0, &quot;failed&quot; : 0 &#125;&#125;下面的例子展示了怎样将我们ID为1的文档的name字段改成“Jane Doe”的同时，给它加上age字段：[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/external/1/_update?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 20 &#125;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 3, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;&#125;更新也可以通过使用简单的脚本来进行。这个例子使用一个脚本将age加5[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/external/1/_update?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;script&quot; : &quot;ctx._source.age += 5&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 4, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;&#125; 删除文档 12345678910111213141516171819 curl -XDELETE &apos;hadoop01:9200/customer/external/2?pretty&apos; 一次删除符合某个查询条件的多个文档 [yangql@hadoop01 bin]$ curl -XDELETE &apos;hadoop01:9200/customer/external/_query?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;John&quot; &#125; &#125;&gt; &#125;&apos;&#123; &quot;found&quot; : false, &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;_query&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;not_found&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;&#125; 3.批处理 bulk API按顺序执行这些动作。如果其中一个动作因为某些原因失败了，将会继续处理它后面的动作。当bulk API返回时，它将提供每个动作的状态（按照同样的顺序），所以你能够看到某个动作成功与否。 批量插入 1234567891011121314151617181920212223242526272829303132333435363738394041424344[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/external/_bulk?pretty&apos; -d &apos;&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&gt; &#123;&quot;name&quot;: &quot;John Doe&quot; &#125;&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&gt; &#123;&quot;name&quot;: &quot;Jane Doe&quot; &#125;&gt; &apos;&#123; &quot;took&quot; : 636, &quot;errors&quot; : false, &quot;items&quot; : [ &#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 5, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : false, &quot;status&quot; : 200 &#125; &#125;, &#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true, &quot;status&quot; : 201 &#125; &#125; ]&#125; 更新删除 12345curl -XPOST &apos;hadoop01:9200/customer/external/_bulk?pretty&apos; -d &apos; &#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125; &#123;&quot;doc&quot;: &#123; &quot;name&quot;: &quot;John Doe becomes Jane Doe&quot; &#125; &#125; &#123;&quot;delete&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125; &apos; 4.查询Elasticsearch提供一种JSON风格的特定领域语言，利用它你可以执行查询。这杯称为查询DSL我们在customer索引中搜索（_search端点），并且q=*参数指示Elasticsearch去匹配这个索引中所有的文档。pretty参数，和以前一样，仅仅是告诉Elasticsearch返回美观的JSON结果 - took —— Elasticsearch执行这个搜索的耗时，以毫秒为单位 - timed_out —— 指明这个搜索是否超时 - \\_shards —— 指出多少个分片被搜索了，同时也指出了成功/失败的被搜索的shards的数量 - hits —— 搜索结果 - hits.total —— 能够匹配我们查询标准的文档的总数目 - hits.hits —— 真正的搜索结果数据（默认只显示前10个文档） - \\_score和max_score —— 现在先忽略这些字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[yangql@hadoop01 bin]$ curl &apos;hadoop01:9200/customer/_search?q=*&amp;pretty&apos;&#123; &quot;took&quot; : 2370, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 3, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;AVz0W-I9v5C-xxEQKDDx&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;Jane Doe&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;Jane Doe&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;John Doe&quot; &#125; &#125; ] &#125;&#125;请求方法体[yangql@hadoop01 bin]$ curl -XPOST &apos;hadoop01:9200/customer/_search?pretty&apos; -d &apos;&gt; &#123;&gt; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&gt; &#125;&apos;&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 3, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;AVz0W-I9v5C-xxEQKDDx&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;Jane Doe&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;Jane Doe&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;customer&quot;, &quot;_type&quot; : &quot;external&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;John Doe&quot; &#125; &#125; ] &#125;&#125; 查询所有123&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 分解以上的这个查询，其中的query部分告诉我查询的定义，match_all部分就是我们想要运行的查询的类型。match_all查询，就是简单地查询一个指定索引下的所有的文档。 除了这个query参数之外，我们也可以通过传递其它的参数来影响搜索结果。比如，下面做了一次match_all并只返回第一个文档：12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;size&quot;: 1&#125;&apos; 注意，如果没有指定size的值，那么它默认就是10。 下面的例子，做了一次match_all并且返回第11到第20个文档：123456curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 10, &quot;size&quot;: 10&#125;&apos; 其中的from参数（0-based）从哪个文档开始，size参数指明从from参数开始，要返回多少个文档。这个特性对于搜索结果分页来说非常有帮助。注意，如果不指定from的值，它默认就是0。下面这个例子做了一次match_all并且以账户余额降序排序，最后返前十个文档：12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: &#123; &quot;balance&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;&#125;&apos; 指定字段查询12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos; &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot;: [&quot;account_number&quot;, &quot;balance&quot;] &#125;&apos; 注意到上面的例子仅仅是简化了_source字段。它仍将会返回一个叫做_source的字段，但是仅仅包含account_number和balance来年改革字段。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://qioinglong.top/tags/Elasticsearch/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Elasticsearch 集群安装","slug":"Elasticsearch 集群安装20190524","date":"2017-06-29T00:54:59.000Z","updated":"2019-05-24T04:59:12.242Z","comments":true,"path":"2017/06/29/Elasticsearch 集群安装20190524/","link":"","permalink":"http://qioinglong.top/2017/06/29/Elasticsearch 集群安装20190524/","excerpt":"1.下载解压1tar xvf elasticsearch-5.4.3.tar.gz 2.配置 /config/elasticsearch.yml123456789cluster.name: es-clusternode.name: es01path.data: /home/yangql/es/datapath.logs: /home/yangql/es/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: hadoop01http.port: 9200discovery.zen.ping.unicast.hosts: [&quot;hadoop01&quot;, &quot;hadoop02&quot;,&quot;hadoop03&quot;] 查看集群http://192.168.1.231:9200/_cluster/stats?pretty","text":"1.下载解压1tar xvf elasticsearch-5.4.3.tar.gz 2.配置 /config/elasticsearch.yml123456789cluster.name: es-clusternode.name: es01path.data: /home/yangql/es/datapath.logs: /home/yangql/es/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: hadoop01http.port: 9200discovery.zen.ping.unicast.hosts: [&quot;hadoop01&quot;, &quot;hadoop02&quot;,&quot;hadoop03&quot;] 查看集群http://192.168.1.231:9200/_cluster/stats?pretty 3.错误解决3.1 unable to install syscall filter12345678910111213141516[2017-06-29T08:27:42,584][WARN ][o.e.b.JNANatives ] unable to install syscall filter:java.lang.UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in at org.elasticsearch.bootstrap.SystemCallFilter.linuxImpl(SystemCallFilter.java:350) ~[elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.SystemCallFilter.init(SystemCallFilter.java:638) ~[elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.JNANatives.tryInstallSystemCallFilter(JNANatives.java:215) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Natives.tryInstallSystemCallFilter(Natives.java:99) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:111) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:350) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:67) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.4.3.jar:5.4.3] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.4.3.jar:5.4.3] 原因：报了一大串错误，大家不必惊慌，其实只是一个警告，主要是因为你Linux版本过低造成的。解决方案：1、重新安装新版本的Linux系统2、警告不影响使用，可以忽略 3.2 启动 elasticsearch 如出现异常 can not run elasticsearch as root解决方法：创建ES 账户，修改文件夹 文件 所属用户 组 3.3 启动异常：ERROR: bootstrap checks failed1system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk 问题原因：因为Centos6不支持SecComp，而ES5.2.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。详解决方法：在elasticsearch.yml中配置bootstrap.system_call_filter为false，注意要在Memory下面:12bootstrap.memory_lock: falsebootstrap.system_call_filter: false 3.4 启动后，如果只有本地可以访问，尝试修改配置文件 elasticsearch.yml中network.host(注意配置文件格式不是以 # 开头的要空一格， ： 后要空一格)为 network.host: 0.0.0.0默认端口是 9200注意：关闭防火墙 或者开放9200端口 3.5 ERROR: bootstrap checks failed12max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]max number of threads [1024] for user [lishang] likely too low, increase to at least [2048] 解决方法：切换到root用户，编辑limits.conf 添加类似如下内容vi /etc/security/limits.conf添加如下内容:1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 3.6max number of threads [1024] for user [lish] likely too low, increase to at least [2048]解决：切换到root用户，进入limits.d目录下修改配置文件。vi /etc/security/limits.d/90-nproc.conf修改如下内容：1* soft nproc 1024 修改为1* soft nproc 2048 3.7 max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]解决：切换到root用户修改配置sysctl.confvi /etc/sysctl.conf 添加下面配置：1vm.max_map_count=655360 并执行命令：1sysctl -p","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://qioinglong.top/tags/Elasticsearch/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Phoenix安装","slug":"Phoenix安装20190524","date":"2017-06-26T04:54:59.000Z","updated":"2019-05-24T04:59:13.779Z","comments":true,"path":"2017/06/26/Phoenix安装20190524/","link":"","permalink":"http://qioinglong.top/2017/06/26/Phoenix安装20190524/","excerpt":"1.解压并配置将下载好的安装包上传到我们的主节点上12tar -xvf apache-phoenix-4.10.0-HBase-1.2-bin.tar.gzmv apache-phoenix-4.10.0-HBase-1.2-bin/ phoenix-4.10 1234#config phoenix4.10export PHOENIX_HOME=/home/yangql/app/phoenix-4.10export PHOENIX_CLASSPATH=$PHOENIX_HOMEexport PATH=$PATH:$PHOENIX_HOME/bin","text":"1.解压并配置将下载好的安装包上传到我们的主节点上12tar -xvf apache-phoenix-4.10.0-HBase-1.2-bin.tar.gzmv apache-phoenix-4.10.0-HBase-1.2-bin/ phoenix-4.10 1234#config phoenix4.10export PHOENIX_HOME=/home/yangql/app/phoenix-4.10export PHOENIX_CLASSPATH=$PHOENIX_HOMEexport PATH=$PATH:$PHOENIX_HOME/bin 2.配置hbase进入到phoenix的安装目录，找到 “phoenix-4.10.0-HBase-1.2-server.jar” ，将这个 jar 包拷贝到集群中每个节点( 主节点也要拷贝 )的 hbase 的 lib 目录下 3.重启hbase1sh /home/yangql/app/hbase-1.2.5/bin/stop-hbase.sh 4.启动phoenix123456789101112131415[yangql@hadoop01 bin]$ python sqlline.py hadoop01,hadoop02,hadoop03:2181Setting property: [incremental, false]Setting property: [isolation, TRANSACTION_READ_COMMITTED]issuing: !connect jdbc:phoenix:hadoop01,hadoop02,hadoop03:2181 none none org.apache.phoenix.jdbc.PhoenixDriverConnecting to jdbc:phoenix:hadoop01,hadoop02,hadoop03:218117/06/26 11:25:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableConnected to: Phoenix (version 4.10)Driver: PhoenixEmbeddedDriver (version 4.10)Autocommit status: trueTransaction isolation: TRANSACTION_READ_COMMITTEDBuilding list of tables and columns for tab-completion (set fastconnect to true to skip)...91/91 (100%) DoneDonesqlline version 1.2.00: jdbc:phoenix:hadoop01,hadoop02,hadoop03:21&gt; 5.使用例子 批处理方式我们建立sql 名叫 us_population.sql 内容是1CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); 建立一个文件 us_population.csv12345678910NY,New York,8143197CA,Los Angeles,3844829IL,Chicago,2842518TX,Houston,2016582PA,Philadelphia,1463281AZ,Phoenix,1461575TX,San Antonio,1256509CA,San Diego,1255540TX,Dallas,1213825CA,San Jose,912332 再创建一个文件 us_population_queries.sql1SELECT state as &quot;State&quot;,count(city) as &quot;City Count&quot;,sum(population) as &quot;Population Sum&quot; FROM us_population GROUP BY state ORDER BY sum(population) DESC; 然后一起执行1[yangql@hadoop01 phoenix]$ python psql.py hadoop01,hadoop02,hadoop03:2181 us_population.sql us_population.sql us_population_queries.sql 用hbase shell 看下会发现多出来一个 US_POPULATION 表，用scan 命令查看一下这个表的数据 1234567891011121314151617181920212223hbase(main):002:0&gt; scan &apos;US_POPULATION&apos;ROW COLUMN+CELL AZPhoenix column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x AZPhoenix column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x16MG CALos Angeles column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CALos Angeles column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00:\\xAA\\xDD CASan Diego column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CASan Diego column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x13(t CASan Jose column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CASan Jose column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x0D\\xEB\\xCC ILChicago column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x ILChicago column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00+_\\x96 NYNew York column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x NYNew York column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00|A] PAPhiladelphia column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x PAPhiladelphia column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x16S\\xF1 TXDallas column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXDallas column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x12\\x85\\x81 TXHouston column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXHouston column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x1E\\xC5F TXSan Antonio column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXSan Antonio column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x13,= 10 row(s) in 0.1780 seconds 命令行方式1[yangql@hadoop01 bin]$ python sqlline.py hadoop01,hadoop02,hadoop03:2181 退出命令行的方式是执行 !quit命令开头需要一个感叹号，使用help可以打印出所有命令hbase建立表12345678create &apos;employee&apos;,&apos;company&apos;,&apos;family&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;company:name&apos;,&apos;ted&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;company:position&apos;,&apos;worker&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;family:tel&apos;,&apos;13600912345&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;company:name&apos;,&apos;michael&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;company:position&apos;,&apos;manager&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;family:tel&apos;,&apos;1894225698&apos;scan &apos;employee&apos; 关于映射表在建立映射表之前要说明的是，Phoenix是大小写敏感的，并且所有命令都是大写，如果你建的表名没有用双引号括起来，那么无论你输入的是大写还是小写，建立出来的表名都是大写的，如果你需要建立出同时包含大写和小写的表名和字段名，请把表名或者字段名用双引号括起来你可以建立读写的表或者只读的表，他们的区别如下读写表：如果你定义的列簇不存在，会被自动建立出来，并且赋以空值只读表：你定义的列簇必须事先存在建立映射1CREATE TABLE IF NOT EXISTS &quot;employee&quot; (&quot;no&quot; CHAR(4) NOT NULL PRIMARY KEY, &quot;company&quot;.&quot;name&quot; VARCHAR(30),&quot;company&quot;.&quot;position&quot; VARCHAR(20), &quot;family&quot;.&quot;tel&quot; CHAR(11), &quot;family&quot;.&quot;age&quot; INTEGER); IF NOT EXISTS可以保证如果已经有建立过这个表，配置不会被覆盖 作为rowkey的字段用 PRIMARY KEY标定 列簇用 columnFamily.columnName 来表示 family.age 是新增的字段，我之前建立测试数据的时候没有建立这个字段的原因是在hbase shell下无法直接写入数字型，等等我用UPSERT 命令插入数据的时候你就可以看到真正的数字型在hbase 下是如何显示的 查询 1SELECT * FROM &quot;employee&quot;; 插入/更改数据插入或者更改数据在Phoenix里面是一个命令叫 UPSERT 意思是 update + insertUPSERT INTO “employee” VALUES (‘row3’,’billy’,’worker’,’16974681345’,33); 1UPSERT INTO &quot;employee&quot; VALUES (&apos;row3&apos;,&apos;billy&apos;,&apos;worker&apos;,&apos;16974681345&apos;,33);","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Phoenix","slug":"Phoenix","permalink":"http://qioinglong.top/tags/Phoenix/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hbase-phoenix安装","slug":"Hbase-phoenix安装20190524","date":"2017-06-26T04:54:59.000Z","updated":"2019-05-24T04:59:12.544Z","comments":true,"path":"2017/06/26/Hbase-phoenix安装20190524/","link":"","permalink":"http://qioinglong.top/2017/06/26/Hbase-phoenix安装20190524/","excerpt":"1.解压并配置将下载好的安装包上传到我们的主节点上12tar -xvf apache-phoenix-4.10.0-HBase-1.2-bin.tar.gzmv apache-phoenix-4.10.0-HBase-1.2-bin/ phoenix-4.10 1234#config phoenix4.10export PHOENIX_HOME=/home/yangql/app/phoenix-4.10export PHOENIX_CLASSPATH=$PHOENIX_HOMEexport PATH=$PATH:$PHOENIX_HOME/bin","text":"1.解压并配置将下载好的安装包上传到我们的主节点上12tar -xvf apache-phoenix-4.10.0-HBase-1.2-bin.tar.gzmv apache-phoenix-4.10.0-HBase-1.2-bin/ phoenix-4.10 1234#config phoenix4.10export PHOENIX_HOME=/home/yangql/app/phoenix-4.10export PHOENIX_CLASSPATH=$PHOENIX_HOMEexport PATH=$PATH:$PHOENIX_HOME/bin 2.配置hbase进入到phoenix的安装目录，找到 “phoenix-4.10.0-HBase-1.2-server.jar” ，将这个 jar 包拷贝到集群中每个节点( 主节点也要拷贝 )的 hbase 的 lib 目录下 3.重启hbase1sh /home/yangql/app/hbase-1.2.5/bin/stop-hbase.sh 4.启动phoenix123456789101112131415[yangql@hadoop01 bin]$ python sqlline.py hadoop01,hadoop02,hadoop03:2181Setting property: [incremental, false]Setting property: [isolation, TRANSACTION_READ_COMMITTED]issuing: !connect jdbc:phoenix:hadoop01,hadoop02,hadoop03:2181 none none org.apache.phoenix.jdbc.PhoenixDriverConnecting to jdbc:phoenix:hadoop01,hadoop02,hadoop03:218117/06/26 11:25:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableConnected to: Phoenix (version 4.10)Driver: PhoenixEmbeddedDriver (version 4.10)Autocommit status: trueTransaction isolation: TRANSACTION_READ_COMMITTEDBuilding list of tables and columns for tab-completion (set fastconnect to true to skip)...91/91 (100%) DoneDonesqlline version 1.2.00: jdbc:phoenix:hadoop01,hadoop02,hadoop03:21&gt; 5.使用例子 批处理方式我们建立sql 名叫 us_population.sql 内容是1CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); 建立一个文件 us_population.csv12345678910NY,New York,8143197CA,Los Angeles,3844829IL,Chicago,2842518TX,Houston,2016582PA,Philadelphia,1463281AZ,Phoenix,1461575TX,San Antonio,1256509CA,San Diego,1255540TX,Dallas,1213825CA,San Jose,912332 再创建一个文件 us_population_queries.sql1SELECT state as &quot;State&quot;,count(city) as &quot;City Count&quot;,sum(population) as &quot;Population Sum&quot; FROM us_population GROUP BY state ORDER BY sum(population) DESC; 然后一起执行1[yangql@hadoop01 phoenix]$ python psql.py hadoop01,hadoop02,hadoop03:2181 us_population.sql us_population.sql us_population_queries.sql 用hbase shell 看下会发现多出来一个 US_POPULATION 表，用scan 命令查看一下这个表的数据 1234567891011121314151617181920212223hbase(main):002:0&gt; scan &apos;US_POPULATION&apos;ROW COLUMN+CELL AZPhoenix column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x AZPhoenix column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x16MG CALos Angeles column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CALos Angeles column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00:\\xAA\\xDD CASan Diego column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CASan Diego column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x13(t CASan Jose column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x CASan Jose column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x0D\\xEB\\xCC ILChicago column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x ILChicago column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00+_\\x96 NYNew York column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x NYNew York column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00|A] PAPhiladelphia column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x PAPhiladelphia column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x16S\\xF1 TXDallas column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXDallas column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x12\\x85\\x81 TXHouston column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXHouston column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x1E\\xC5F TXSan Antonio column=0:\\x00\\x00\\x00\\x00, timestamp=1498491908701, value=x TXSan Antonio column=0:\\x80\\x0B, timestamp=1498491908701, value=\\x80\\x00\\x00\\x00\\x00\\x13,= 10 row(s) in 0.1780 seconds 命令行方式1[yangql@hadoop01 bin]$ python sqlline.py hadoop01,hadoop02,hadoop03:2181 退出命令行的方式是执行 !quit命令开头需要一个感叹号，使用help可以打印出所有命令hbase建立表12345678create &apos;employee&apos;,&apos;company&apos;,&apos;family&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;company:name&apos;,&apos;ted&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;company:position&apos;,&apos;worker&apos;put &apos;employee&apos;,&apos;row1&apos;,&apos;family:tel&apos;,&apos;13600912345&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;company:name&apos;,&apos;michael&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;company:position&apos;,&apos;manager&apos;put &apos;employee&apos;,&apos;row2&apos;,&apos;family:tel&apos;,&apos;1894225698&apos;scan &apos;employee&apos; 关于映射表在建立映射表之前要说明的是，Phoenix是大小写敏感的，并且所有命令都是大写，如果你建的表名没有用双引号括起来，那么无论你输入的是大写还是小写，建立出来的表名都是大写的，如果你需要建立出同时包含大写和小写的表名和字段名，请把表名或者字段名用双引号括起来你可以建立读写的表或者只读的表，他们的区别如下读写表：如果你定义的列簇不存在，会被自动建立出来，并且赋以空值只读表：你定义的列簇必须事先存在建立映射1CREATE TABLE IF NOT EXISTS &quot;employee&quot; (&quot;no&quot; CHAR(4) NOT NULL PRIMARY KEY, &quot;company&quot;.&quot;name&quot; VARCHAR(30),&quot;company&quot;.&quot;position&quot; VARCHAR(20), &quot;family&quot;.&quot;tel&quot; CHAR(11), &quot;family&quot;.&quot;age&quot; INTEGER); IF NOT EXISTS可以保证如果已经有建立过这个表，配置不会被覆盖 作为rowkey的字段用 PRIMARY KEY标定 列簇用 columnFamily.columnName 来表示 family.age 是新增的字段，我之前建立测试数据的时候没有建立这个字段的原因是在hbase shell下无法直接写入数字型，等等我用UPSERT 命令插入数据的时候你就可以看到真正的数字型在hbase 下是如何显示的 查询 1SELECT * FROM &quot;employee&quot;; 插入/更改数据插入或者更改数据在Phoenix里面是一个命令叫 UPSERT 意思是 update + insertUPSERT INTO “employee” VALUES (‘row3’,’billy’,’worker’,’16974681345’,33); 1UPSERT INTO &quot;employee&quot; VALUES (&apos;row3&apos;,&apos;billy&apos;,&apos;worker&apos;,&apos;16974681345&apos;,33);","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://qioinglong.top/tags/Hbase/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive中的数据倾斜","slug":"Hive中的数据倾斜20190524","date":"2017-06-15T15:54:59.000Z","updated":"2019-05-24T04:59:13.407Z","comments":true,"path":"2017/06/15/Hive中的数据倾斜20190524/","link":"","permalink":"http://qioinglong.top/2017/06/15/Hive中的数据倾斜20190524/","excerpt":"","text":"1.空值数据倾斜场景：如日志中，常会有信息丢失的问题，比如全网日志中的user_id，如果取其中的user_id和users关联，会碰到数据倾斜的问题。解决方法1： user_id为空的不参与关联1234567Select * From log aJoin users bOn a.user_id is not nullAnd a.user_id = b.user_idUnion allSelect * from log awhere a.user_id is null; 解决方法2 ：赋与空值分新的key值1234Select * from log aleft outer join users bon case when a.user_id is null thenconcat(&apos;hive&apos;,rand() ) else a.user_id end = b.user_id; 总结：方法2比方法效率更好，不但io少了，而且作业数也少了。方法1 log读取两次，jobs是2。方法2 job数是1 。这个优化适合无效id(比如-99,’’,null等)产生的倾斜问题。把空值的key变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。 2.不同数据类型关联产生数据倾斜表与表关联时，如果关联字段之间数据类型不一致，可能会导致数据倾斜解决方法：转化数据类型，保持一致 3.Join的数据偏斜数据偏斜的原因包括以下两点： Map输出key数量极少，导致reduce端退化为单机作业。 Map输出key分布不均，少量key对应大量value，导致reduce端单机瓶颈。Hive中我们使用MapJoin解决数据偏斜的问题，即将其中的某个表（全量）分发到所有Map端进行Join，从而避免了reduce。这要求分发的表可以被全量载入内存。极限情况下，Join两边的表都是大表，就无法使用MapJoin。解决方法： 情况1，考虑先对Join中的一个表去重，以此结果过滤无用信息。这样一般会将其中一个大表转化为小表，再使用MapJoin 。 情况2，考虑切分Join中的一个表为多片，以便将切片全部载入内存，然后采用多次MapJoin得到结果。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive表创建，数据加载，数据导出","slug":"Hive表创建，数据加载，数据导出20190524","date":"2017-06-15T04:54:59.000Z","updated":"2019-05-24T04:59:13.011Z","comments":true,"path":"2017/06/15/Hive表创建，数据加载，数据导出20190524/","link":"","permalink":"http://qioinglong.top/2017/06/15/Hive表创建，数据加载，数据导出20190524/","excerpt":"","text":"hive不支持用insert语句一条一条的进行插入操作，也不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。 1.创建表的三种方式 创建表的基本语法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path];data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later)primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later)array_type : ARRAY &lt; data_type &gt;map_type : MAP &lt; primitive_type, data_type &gt;struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;union_type : UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later)row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname 第一种方式 12345CREATE TABLE IF NOT EXISTS crm.weblog(ip string ,time string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\\t&apos; ; 第二种方式（包含原表数据） 12CREATE TABLE crm.weblog_commAS select ip, time, req_url from crm.weblog; 第三种方式(只含表结构) 1CREATE TABLE IF NOT EXISTS crm.weblogbak LIKE crm.weblog ; 2.向数据表内加载文件1LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 LOCAL：本地模式 OVERWRITE：目标表（或者分区）中的内容（如果有）会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中filepath有以下几种类型 相对路径，例如： 20170524/crm_cus_com.txt 绝对路径，例如： /user/hive/20170524/crm_cus_com.txt 包含模式的完整 URI，例如：hdfs://namenode:9000/user/hive/20170524/crm_cus_com.txt1hive&gt; LOAD DATA LOCAL INPATH &apos;/home/yangql/data/20170524/crm_cus_com.txt&apos; OVERWRITE INTO TABLE crm.crm_cus_com; 3.将查询结果插入Hive表 基本模式 123INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement--例子hive (crm)&gt; insert overwrite table crm_cus_com0615 select * from crm_cus_com; 多插入模式 12345678910FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...--例子from crm_cus_cominsert overwrite table crm_cus_com0615select *insert overwrite table crm_cus_com0616select *; 追加模式,不删除原来的数据 1234INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement--例子hive (crm)&gt; insert into table crm_cus_com0615 select * from crm_cus_com; 自动分区模式 1INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement 4.将查询结果写入HDFS文件系统数据写入文件系统时进行文本序列化，且每列用^A 来区分，\\n换行 123456789INSERT OVERWRITE [LOCAL] DIRECTORY directory1SELECT ... FROM ...FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2]--例子hive (crm)&gt; insert overwrite local directory &apos;/home/yangql/data/test&apos; select * from crm_cus_com;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive函数总结","slug":"Hive函数总结20190524","date":"2017-06-14T04:54:59.000Z","updated":"2019-05-24T04:59:13.074Z","comments":true,"path":"2017/06/14/Hive函数总结20190524/","link":"","permalink":"http://qioinglong.top/2017/06/14/Hive函数总结20190524/","excerpt":"Hive提供了包括数学函数，类型转换函数，条件函数，字符函数，聚合函数，表生成函数等的内置函数。","text":"Hive提供了包括数学函数，类型转换函数，条件函数，字符函数，聚合函数，表生成函数等的内置函数。 1.数学函数 函数名 返回类型 描述 round(DOUBLE a) DOUBLE 返回对a四舍五入的BIGINT值 round(DOUBLE a, INT d) DOUBLE 返回DOUBLE型,保留d位小数的DOUBLE型的近似值 bround(DOUBLE a) DOUBLE 银行家舍入法（1~4：舍，6~9：进，5-&gt;前位数是偶：舍，5-&gt;前位数是奇：进） bround(DOUBLE a, INT d) DOUBLE 银行家舍入法,保留d位小数 floor(DOUBLE a) BIGINT 向下取整，最数轴上最接近要求的值的左边的值 如：6.10-&gt;6 -3.4-&gt;-4 ceil(DOUBLE a), ceiling(DOUBLE a) BIGINT 求其不小于小给定实数的最小整数如：ceil(6) = ceil(6.1)= ceil(6.9) = 7 rand(), rand(INT seed) DOUBLE 每行返回一个DOUBLE型随机数,seed是随机因子 exp(DOUBLE a), exp(DECIMAL a) DOUBLE 返回e的a幂次方， a可为小数 n(DOUBLE a), ln(DECIMAL a) DOUBLE 以自然数为底d的对数，a可为小数 log10(DOUBLE a), log10(DECIMAL a) DOUBLE 以10为底d的对数，a可为小数 log2(DOUBLE a), log2(DECIMAL a) DOUBLE 以2为底数d的对数，a可为小数 log(DOUBLE base, DOUBLE a)/log(DECIMAL base, DECIMAL a) DOUBLE 以base为底的对数，base 与 a都是DOUBLE类型 pow(DOUBLE a, DOUBLE p)/ power(DOUBLE a, DOUBLE p) DOUBLE 计算a的p次幂ap sqrt(DOUBLE a), sqrt(DECIMAL a) DOUBLE 计算a的平方根 bin(BIGINT a) STRING 计算a的二进制STRING类型，a为BIGINT类型 hex(BIGINT a) hex(STRING a) hex(BINARY a) STRING 计算十六进制 unhex(STRING a) BINARY hex的逆方法 conv(BIGINT num, INT from_base, INT to_base), conv(STRING num, INT from_base, INT to_base) STRING 将BIGINT/STRING类型的num从from_base进制转换成to_base进制 pmod(INT a, INT b), pmod(DOUBLE a, DOUBLE b) INT or DOUBLE a对b取模 sin(DOUBLE a), sin(DECIMAL a) DOUBLE 求a的正弦值 asin(DOUBLE a), asin(DECIMAL a) DOUBLE 求d的反正弦值 cos(DOUBLE a), cos(DECIMAL a) DOUBLE 求余弦值 acos(DOUBLE a), acos(DECIMAL a) DOUBLE 求反余弦值 tan(DOUBLE a), tan(DECIMAL a) DOUBLE 求正切值 atan(DOUBLE a), atan(DECIMAL a) DOUBLE 求反正切值 degrees(DOUBLE a), degrees(DECIMAL a) DOUBLE 奖弧度值转换角度值 radians(DOUBLE a), radians(DOUBLE a) DOUBLE 将角度值转换成弧度值 positive(INT a), positive(DOUBLE a) INT or DOUBLE 返回a negative(INT a), negative(DOUBLE a) INT or DOUBLE 返回a的相反数 sign(DOUBLE a), sign(DECIMAL a) DOUBLE or INT 如果a是正数则返回1.0，是负数则返回-1.0，否则返回0.0 e() DOUBLE 数学常数e pi() DOUBLE 数学常数pi factorial(INT a) BIGINT 求a的阶乘 cbrt(DOUBLE a) DOUBLE 求a的立方根 shiftleft(TINYINT/SMALLINT/INT a, INT b)/shiftleft(BIGINT a, INT?b) INT/BIGINT 按位左移 shiftright(TINYINT/SMALLINT/INT a, INTb)/shiftright(BIGINT a, INT?b) INT/BIGINT 按拉右移 shiftrightunsigned(TINYINT/SMALLINT/INTa, INT b),/shiftrightunsigned(BIGINT a, INT b) INT/BIGINT 无符号按位右移（&lt;&lt;&lt;） greatest(T v1, T v2, …) T 求最大值 least(T v1, T v2, …) T 求最小值 2.集合函数 函数名 返回类型 描述 size(Map) int 求map的长度 size(Array) int 求数组的长度 map_keys(Map) array 返回map中的所有key map_values(Map) array 返回map中的所有value array_contains(Array, value) boolean 如该数组Array包含value返回true。，否则返回false sort_array(Array) array 按自然顺序对数组进行排序并返回 3.类型转换函数 函数名 返回类型 描述 binary(string/binary) binary 将输入的值转换成二进制 cast(expr as ) Expected “=” to follow “type” 将expr转换成type类型 如：cast(“1” as BIGINT) 将字符串1转换成了BIGINT类型，如果转换失败将返回NULL 4.日期函数 函数名 返回类型 描述 from_unixtime(bigint unixtime[, string format]) string 将时间的秒值转换成format格式（format可为“yyyy-MM-dd hh:mm:ss”,“yyyy-MM-dd hh”,“yyyy-MM-dd hh:mm”等等）如from_unixtime(1250111000,”yyyy-MM-dd”) 得到2009-03-12 unix_timestamp() bigint 获取本地时区下的时间戳 unix_timestamp(string date) bigint 将格式为yyyy-MM-dd HH:mm:ss的时间字符串转换成时间戳 如unix_timestamp(‘2009-03-20 11:30:01’) = 1237573801 unix_timestamp(string date, string pattern) bigint 将指定时间字符串格式字符串转换成Unix时间戳，如果格式不对返回0 如：unix_timestamp(‘2009-03-20’, ‘yyyy-MM-dd’) = 1237532400 to_date(string timestamp) string 返回时间字符串的日期部分 year(string date) int 返回时间字符串的年份部分 quarter(date/timestamp/string) int 返回当前时间属性哪个季度 如quarter(‘2015-04-08’) = 2 month(string date) int 返回时间字符串的月份部分 day(string date) dayofmonth(date) int 返回时间字符串的天 hour(string date) int 返回时间字符串的小时 minute(string date) int 返回时间字符串的分钟 second(string date) int 返回时间字符串的秒 weekofyear(string date) int 返回时间字符串位于一年中的第几个周内 如weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44 datediff(string enddate, string startdate) int 计算开始时间startdate到结束时间enddate相差的天数 date_add(string startdate, int days) string 从开始时间startdate加上days date_sub(string startdate, int days) string 从开始时间startdate减去days from_utc_timestamp(timestamp, string timezone) timestamp 如果给定的时间戳并非UTC，则将其转化成指定的时区下时间戳 to_utc_timestamp(timestamp, string timezone) timestamp 如果给定的时间戳指定的时区下时间戳，则将其转化成UTC下的时间戳 current_date date 返回当前时间日期 current_timestamp timestamp 返回当前时间戳 add_months(string start_date, int num_months) string 返回当前时间下再增加num_months个月的日期 last_day(string date) string 返回这个月的最后一天的日期，忽略时分秒部分（HH:mm:ss） next_day(string start_date, string day_of_week) string 返回当前时间的下一个星期X所对应的日期 如：next_day(‘2015-01-14’, ‘TU’) = 2015-01-20 以2015-01-14为开始时间，其下一个星期二所对应的日期为2015-01-20 trunc(string date, string format) string 返回时间的最开始年份或月份 如trunc(“2016-06-26”,“MM”)=2016-06-01 trunc(“2016-06-26”,“YY”)=2016-01-01 注意所支持的格式为MONTH/MON/MM, YEAR/YYYY/YY months_between(date1, date2) double 返回date1与date2之间相差的月份，如date1&gt;date2，则返回正，如果date1&lt;date2,则返回负，否则返回0.0 如：months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) = 3.94959677 1997-02-28 10:30:00与1996-10-30相差3.94959677个月 date_format(date/timestamp/string ts, string fmt) string 按指定格式返回时间date 如：date_format(“2016-06-22”,”MM-dd”)=06-22 5.条件函数 函数名 返回类型 描述 if(boolean testCondition, T valueTrue, T valueFalseOrNull) T 如果testCondition 为true就返回valueTrue,否则返回valueFalseOrNull ，（valueTrue，valueFalseOrNull为泛型） nvl(T value, T default_value) T 如果value值为NULL就返回default_value,否则返回value COALESCE(T v1, T v2, …) T 返回第一非null的值，如果全部都为NULL就返回NULL 如：COALESCE (NULL,44,55)=44/strong&gt; CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END T 如果a=b就返回c,a=d就返回e，否则返回f 如CASE 4 WHEN 5 THEN 5 WHEN 4 THEN 4 ELSE 3 END 将返回4 CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END T 如果a=ture就返回b,c= ture就返回d，否则返回e 如：CASE WHEN 5&gt;0 THEN 5 WHEN 4&gt;0 THEN 4 ELSE 0 END 将返回5；CASE WHEN 5&lt;0 THEN 5 WHEN 4&lt;0 THEN 4 ELSE 0 END 将返回0 isnull( a ) boolean 如果a为null就返回true，否则返回false isnotnull ( a ) boolean 如果a为非null就返回true，否则返回false 6.字符函数 函数名 返回类型 描述 ascii(string str) int 返回str中首个ASCII字符串的整数值 base64(binary bin) string 将二进制bin转换成64位的字符串 concat(string/binary A, string/binary B…) string 对二进制字节码或字符串按次序进行拼接 context_ngrams(array","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive整合HBase","slug":"Hive整合HBase20190524","date":"2017-06-06T06:52:31.000Z","updated":"2019-05-24T04:59:13.360Z","comments":true,"path":"2017/06/06/Hive整合HBase20190524/","link":"","permalink":"http://qioinglong.top/2017/06/06/Hive整合HBase20190524/","excerpt":"Hive和Hbase有各自不同的特征：hive是高延迟、结构化和面向分析的，hbase是低延迟、非结构化和面向编程的。Hive数据仓库在hadoop上是高延迟的。Hive集成Hbase就是为了使用hbase的一些特性。Hive继承HBase可以有效利用HBase数据库的存储特性，如行更新和列索引等。在集成的过程中注意维持HBase jar包的一致性。Hive集成HBase需要在Hive表和HBase表之间建立映射关系，也就是Hive表的列和列类型与HBase表的列族及列限定词建立关联。每一个在Hive表中的域都存在与HBase中，而在Hive表中不需要包含所有HBase中的列。HBase中的rowkey对应到Hive中为选择一个域使用 :key 来对应，列族(cf映射到Hive中的其他所有域，列为(cf:cq)。","text":"Hive和Hbase有各自不同的特征：hive是高延迟、结构化和面向分析的，hbase是低延迟、非结构化和面向编程的。Hive数据仓库在hadoop上是高延迟的。Hive集成Hbase就是为了使用hbase的一些特性。Hive继承HBase可以有效利用HBase数据库的存储特性，如行更新和列索引等。在集成的过程中注意维持HBase jar包的一致性。Hive集成HBase需要在Hive表和HBase表之间建立映射关系，也就是Hive表的列和列类型与HBase表的列族及列限定词建立关联。每一个在Hive表中的域都存在与HBase中，而在Hive表中不需要包含所有HBase中的列。HBase中的rowkey对应到Hive中为选择一个域使用 :key 来对应，列族(cf映射到Hive中的其他所有域，列为(cf:cq)。 1.创建Hive中与HBase中对应的表123456CREATE TABLE crm.user1 (rowkey string,info map&lt;STRING,STRING&gt;) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;crm:user1&quot;); 2.创建Hbase表crm:命名空间123456789create &apos;crm:user1&apos;,&#123;NAME =&gt; &apos;info&apos;,VERSIONS =&gt; 1&#125;向user表中插入一些数据：put &apos;crm:user1&apos;,&apos;1&apos;,&apos;info:name&apos;,&apos;zhangsan&apos;put &apos;crm:user1&apos;,&apos;1&apos;,&apos;info:age&apos;,&apos;25&apos;put &apos;crm:user1&apos;,&apos;2&apos;,&apos;info:name&apos;,&apos;lisi&apos;put &apos;crm:user1&apos;,&apos;2&apos;,&apos;info:age&apos;,&apos;22&apos;put &apos;crm:user1&apos;,&apos;3&apos;,&apos;info:name&apos;,&apos;wangswu&apos;put &apos;crm:user1&apos;,&apos;3&apos;,&apos;info:age&apos;,&apos;21&apos; HBase查看数据12345678910scan &apos;crm:user1&apos;hbase(main):002:0&gt; scan &apos;crm:user1&apos;ROW COLUMN+CELL 1 column=info:age, timestamp=1496762712287, value=25 1 column=info:name, timestamp=1496762712252, value=zhangsan 2 column=info:age, timestamp=1496762712375, value=22 2 column=info:name, timestamp=1496762712312, value=lisi 3 column=info:age, timestamp=1496762713400, value=21 3 column=info:name, timestamp=1496762712414, value=wangswu 3 row(s) in 0.0920 seconds Hive查看数据12345678select * from crm.user1;hive (default)&gt; select * from crm.user1;OKuser1.rowkey user1.info1 &#123;&quot;age&quot;:&quot;25&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;2 &#123;&quot;age&quot;:&quot;22&quot;,&quot;name&quot;:&quot;lisi&quot;&#125;3 &#123;&quot;age&quot;:&quot;21&quot;,&quot;name&quot;:&quot;wangswu&quot;&#125;Time taken: 0.139 seconds, Fetched: 3 row(s) 3.hive插入数据到hbase1234INSERT INTO TABLE crm.user1SELECT &apos;4&apos; AS rowkey,map(&apos;name&apos;,&apos;lijin&apos;,&apos;age&apos;,&apos;22&apos;) AS infofrom dual limit 1; 4.从Hive中创建HBase表使用HQL语句创建一个指向HBase的Hive表1234567891011CREATE TABLE hbase_table_1(key int, value string) //Hive中的表名hbase_table_1STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; //指定存储处理器WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf1:val&quot;) //声明列族，列名TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;xyz&quot;, &quot;hbase.mapred.output.outputtable&quot; = &quot;xyz&quot;); //hbase.table.name声明HBase表名，为可选属性默认与Hive的表名相同，//hbase.mapred.output.outputtable指定插入数据时写入的表，如果以后需要往该表插入数据就需要指定该值CREATE TABLE hbase_table_1(key int, value string)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf1:val&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_table_1&quot;, &quot;hbase.mapred.output.outputtable&quot; = &quot;hbase_table_1&quot;); 通过HBase shell可以查看刚刚创建的HBase表的属性123456789hbase(main):002:0&gt; desc &apos;hbase_table_1&apos;Table hbase_table_1 is ENABLED hbase_table_1 COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;cf1&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, COMPRESSION =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125; 1 row(s) in 0.4100 secondshbase(main):003:0&gt; 5.从Hive中映射HBase创建一个指向已经存在的HBase表的Hive表1234CREATE EXTERNAL TABLE hbase_table_2(key int, value string)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;cf1:val&quot;)TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;some_existing_table&quot;, &quot;hbase.mapred.output.outputtable&quot; = &quot;some_existing_table&quot;); 该Hive表一个外部表，所以删除该表并不会删除HBase表中的数据注意 建表或映射表的时候如果没有指定:key则第一个列默认就是行键 HBase对应的Hive表中没有时间戳概念，默认返回的就是最新版本的值 由于HBase中没有数据类型信息，所以在存储数据的时候都转化为String类型 6.多列及多列族的映射如下表：value1和value2来自列族a对应的b c列，value3来自列族d对应的列12345CREATE TABLE hbase_table_1(key int, value1 string, value2 int, value3 int)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,a:b,a:c,d:e&quot;); 7.Hive Map类型在HBase中的映射规则通过Hive的Map数据类型映射HBase表，这样每行都可以有不同的列组合，列名与map中的key对应，列值与map中的value对应12345CREATE TABLE hbase_table_1(value map&lt;string,int&gt;, row_key int)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;cf:,:key&quot;);","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"数据分析","slug":"数据分析20190118","date":"2017-06-04T06:52:31.000Z","updated":"2019-01-18T13:33:11.383Z","comments":true,"path":"2017/06/04/数据分析20190118/","link":"","permalink":"http://qioinglong.top/2017/06/04/数据分析20190118/","excerpt":"","text":"","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/tags/数据分析/"}],"keywords":[{"name":"数据分析","slug":"数据分析","permalink":"http://qioinglong.top/categories/数据分析/"}]},{"title":"python绘图","slug":"Python绘图20190524","date":"2017-06-04T06:52:31.000Z","updated":"2019-05-24T04:59:13.953Z","comments":true,"path":"2017/06/04/Python绘图20190524/","link":"","permalink":"http://qioinglong.top/2017/06/04/Python绘图20190524/","excerpt":"python matplotlib,numpy,生成随机数，绘制折线图，散点图，直方图，子图","text":"python matplotlib,numpy,生成随机数，绘制折线图，散点图，直方图，子图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#!/usr/bin/env python# -*- coding: utf-8 -*-# python data import&quot;&quot;&quot;@author:yangql@create:2017-06-01 16:55折线图，散点图[plot]，&quot;&quot;&quot;import matplotlib.pylab as pylimport numpy as npy#x轴数据x=[1,2,3,4,8]#y轴数据y=[5,7,6,8,9]#折线图#pyl.plot(x,y)#plot(X轴，Y轴，形式)#display#pyl.show()#散点图#pyl.plot(x,y,&apos;o&apos;)#plot(X轴，Y轴，形式)#pyl.show()#改变颜色&apos;&apos;&apos;c-cyan:青色r-redm-magenteg-greenb-bluey-yellowb-blackw-white&apos;&apos;&apos;#pyl.plot(x,y,&apos;oy&apos;)#plot(X轴，Y轴，形式)#pyl.show()#线条样式&apos;&apos;&apos;-:直线--：虚线-. -.形式: 细小虚线&apos;&apos;&apos;#pyl.plot(x,y,&apos;:&apos;)#plot(X轴，Y轴，形式)#pyl.show()&apos;&apos;&apos;点的样式s：方形h:六角形H:六角形*：星型+：加号形式x:x形式d/D:菱形p：五角行形&apos;&apos;&apos;#pyl.plot(x,y)#plot(X轴，Y轴，形式)#同一区域，绘制多条线段#x2=[1,3,8,9,10]#y2=[4,7,8,9,1]#pyl.plot(x2,y2)#加标题,x轴，Y轴标题#pyl.title(&quot;show&quot;)#pyl.xlabel(&quot;age&quot;)#pyl.ylabel(&quot;weight&quot;)#设置X轴Y轴范围#pyl.xlim(0,20)#pyl.ylim(0,30)#pyl.show()#随机数生成data=npy.random.random_integers(1,100,1000)print(data)#random_integers 最小值，最大值，个数#具有正态分布的随机数normal_data=npy.random.normal(10,1,1000)#均数，西格玛，个数print(normal_data)#直方图[hist]#pyl.hist(normal_data)#pyl.show()#设置宽度#取消轮廓histtype=&apos;stepfilled&apos;#sty=npy.arange(2,19,4)#pyl.hist(normal_data,sty,histtype=&apos;stepfilled&apos;)#pyl.show()#绘制子图#pyl.subplot(2,2,3)#行,列，当前区域#pyl.show()pyl.subplot(2,2,1)x=[11,2,3,4,8]y=[5,47,6,8,9]pyl.plot(x,y)pyl.subplot(2,2,2)x=[1,2,43,4,8]y=[5,7,63,8,9]pyl.plot(x,y)pyl.subplot(2,1,2)x=[1,42,3,4,8]y=[5,7,46,8,9]pyl.plot(x,y)pyl.show()","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://qioinglong.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"爬虫-scrapy爬取csdn","slug":"爬虫-scrapy爬取csdn20190524","date":"2017-06-04T06:52:31.000Z","updated":"2019-05-24T04:59:16.748Z","comments":true,"path":"2017/06/04/爬虫-scrapy爬取csdn20190524/","link":"","permalink":"http://qioinglong.top/2017/06/04/爬虫-scrapy爬取csdn20190524/","excerpt":"","text":"1.创建项目1234cd F:\\yangql\\pythonworkplace&gt;scrapy startproject csdncd csdnscrapy genspider csdn_crawler blog.csdn.net -t crawl scrapy crawl csdn_crawler","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://qioinglong.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"python数据导入","slug":"Python数据导入20190524","date":"2017-06-01T06:52:31.000Z","updated":"2019-05-24T04:59:14.023Z","comments":true,"path":"2017/06/01/Python数据导入20190524/","link":"","permalink":"http://qioinglong.top/2017/06/01/Python数据导入20190524/","excerpt":"python数据导入，主要用pandas package实现，可以导入以下不同类型的数据： pda.read_csv ：CSV格式 pda.read_excel：Excel格式 pda.read_sql：mysql格式 =pda.read_html：html格式 read_table：文本格式","text":"python数据导入，主要用pandas package实现，可以导入以下不同类型的数据： pda.read_csv ：CSV格式 pda.read_excel：Excel格式 pda.read_sql：mysql格式 =pda.read_html：html格式 read_table：文本格式 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# -*- coding: utf-8 -*-# python data import&quot;&quot;&quot;@author:yangql@create:2017-06-01 16:55import data by pandas&quot;&quot;&quot;import pandas as pda# import data by csvcsv_data=pda.read_csv(&quot;F:/yangql/pythonworkplace/college-man.csv&quot;,encoding=&quot;gbk&quot;)#print(csv_data.describe())#print(csv_data.sort_values(by=&quot;INCOME_FAMILY&quot;,ascending=False))#import data by xlsxls_data=pda.read_excel(u&quot;F:/yangql/pythonworkplace/大学-男.xlsx&quot;,encoding=&apos;gbk&apos;)#import data by mysqlimport pymysqlconn=pymysql.connect(host=&apos;192.168.1.231&apos;,port=3306,user=&apos;root&apos;,password=&apos;root123&apos;,database=&apos;hive&apos;)sql=&apos;select * from TBLS&apos;mysql_data=pda.read_sql(sql,conn)#print(mysql_data)#import data by html#pip install beautifulsoup4#pip install html5lib#just read tablehtml_data=pda.read_html(&quot;https://book.douban.com/&quot;)#print(html_data)#import data by texttext_data=pda.read_table(&quot;F:/yangql/pythonworkplace/manifest.json&quot;)print(text_data)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://qioinglong.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Python操作Hive数据","slug":"Hive-python操作Hive数据20190524","date":"2017-05-29T22:52:31.000Z","updated":"2019-05-24T04:59:12.860Z","comments":true,"path":"2017/05/30/Hive-python操作Hive数据20190524/","link":"","permalink":"http://qioinglong.top/2017/05/30/Hive-python操作Hive数据20190524/","excerpt":"python操作hive，此处选用pyhive package。悲催的是Windows上不能安装后不能连接，一直报错（暂时没有解决办法）12File &quot;D:\\python\\lib\\site-packages\\thrift_sasl-0.2.1-py3.6.egg\\thrift_sasl\\__init__.py&quot;, line 79, in openthrift.transport.TTransport.TTransportException: Could not start SASL: b&apos;Error in sasl_client_start (-4) SASL(-4): no mechanism available: Unable to find a callback: 2&apos; 以下的操作在CentOS下亲测可以实现：Python：3.6.1Hive：hive-2.1.11.安装12345pip install pyhivehttp://www.lfd.uci.edu/~gohlke/pythonlibs/vu0h7y4r/sasl-0.2.1-cp36-cp36m-win_amd64.whlpip install sasl-0.2.1-cp36-cp36m-win_amd64.whlpip install plypip install thrift-sasl","text":"python操作hive，此处选用pyhive package。悲催的是Windows上不能安装后不能连接，一直报错（暂时没有解决办法）12File &quot;D:\\python\\lib\\site-packages\\thrift_sasl-0.2.1-py3.6.egg\\thrift_sasl\\__init__.py&quot;, line 79, in openthrift.transport.TTransport.TTransportException: Could not start SASL: b&apos;Error in sasl_client_start (-4) SASL(-4): no mechanism available: Unable to find a callback: 2&apos; 以下的操作在CentOS下亲测可以实现：Python：3.6.1Hive：hive-2.1.11.安装12345pip install pyhivehttp://www.lfd.uci.edu/~gohlke/pythonlibs/vu0h7y4r/sasl-0.2.1-cp36-cp36m-win_amd64.whlpip install sasl-0.2.1-cp36-cp36m-win_amd64.whlpip install plypip install thrift-sasl 2.实例代码12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/env python# -*- coding: utf-8 -*-# hive util with hive server2&quot;&quot;&quot;@author:yangql@create:2017-05-08 16:55&quot;&quot;&quot;from pyhive import hivefrom TCLIService.ttypes import TOperationStateclass hiveClient: def __init__(self, host, port=10000, username=None, database=&apos;default&apos;, auth=&apos;NONE&apos;, configuration=None, kerberos_service_name=None, password=None): &apos;&apos;&apos; create connect to hive server2 &apos;&apos;&apos; self.conn=hive.connect(host=host,port=port,database=database) def hiveExe(self,sql): &apos;&apos;&apos; :param sql: :return: &apos;&apos;&apos; cursor=self.conn.cursor() cursor.execute(sql) def hiveQuery(self,sql): cursor=self.conn.cursor() cursor.execute(sql) return cursor.fetchall() def close(self): &apos;&apos;&apos; close the connection :return: &apos;&apos;&apos; self.conn.close()if __name__ == &quot;__main__&quot;: #print(&quot;this is for hive test&quot;) hiveClient=hiveClient(host=&quot;hadoop01&quot;,port=10000,database=&quot;crm&quot;) data=hiveClient.hiveQuery(&quot;select * from crm.crm_cus_com limit 10&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive列分隔符支持多字符","slug":"Hive列分隔符支持多字符20190524","date":"2017-05-25T12:54:59.000Z","updated":"2019-05-24T04:59:13.171Z","comments":true,"path":"2017/05/25/Hive列分隔符支持多字符20190524/","link":"","permalink":"http://qioinglong.top/2017/05/25/Hive列分隔符支持多字符20190524/","excerpt":"Hive 创建表默认的列分隔符是’\\001’,而且默认不支持多字符。查看网上大都说要重写InputFormat，但是没有成功。在stackoverflow上看到一个方法，亲测可以成功。在创建表时加上:123ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&apos;WITH SERDEPROPERTIES (&quot;field.delim&quot;=&quot;@|@&quot;)STORED AS TEXTFILE","text":"Hive 创建表默认的列分隔符是’\\001’,而且默认不支持多字符。查看网上大都说要重写InputFormat，但是没有成功。在stackoverflow上看到一个方法，亲测可以成功。在创建表时加上:123ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe&apos;WITH SERDEPROPERTIES (&quot;field.delim&quot;=&quot;@|@&quot;)STORED AS TEXTFILE 错误123456789101112131415161718192021222324252627282930313233343536373839404142434445Diagnostic Messages for this Task:Error: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 17 moreCaused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:137) ... 22 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe not found at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:329) at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:364) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:106) ... 22 moreCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe not found at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101) at org.apache.hadoop.hive.ql.plan.PartitionDesc.getDeserializer(PartitionDesc.java:175) at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:295) ... 24 more 错误解决1hive add jar /home/yangql/app/hive-2.1.1/lib/hive-contrib-2.1.1.jar","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala内部类与外部类及内部类作用域","slug":"Scala内部类与外部类及内部类作用域20190524","date":"2017-05-09T16:54:59.000Z","updated":"2019-05-24T04:59:14.466Z","comments":true,"path":"2017/05/10/Scala内部类与外部类及内部类作用域20190524/","link":"","permalink":"http://qioinglong.top/2017/05/10/Scala内部类与外部类及内部类作用域20190524/","excerpt":"scala内部类与外部类，以及扩大内部类作用域的两种方式，内部类访问外部类的变量的方式。 内部类与外部类实例1234567891011121314151617181920212223package com.scala.baseimport scala.collection.mutable.ArrayBufferobject Class &#123; def main(args: Array[String]): Unit = &#123; val c1=new Class val leo=c1.register(&quot;leo&quot;) c1.students+=leo val c2=new Class val jack=c2.register(&quot;jack&quot;) c2.students+=jack &#125;&#125;class Class&#123; class Student(val name:String) val students=new ArrayBuffer[Student] def register(name:String):Student=&#123; new Student(name) &#125;&#125;","text":"scala内部类与外部类，以及扩大内部类作用域的两种方式，内部类访问外部类的变量的方式。 内部类与外部类实例1234567891011121314151617181920212223package com.scala.baseimport scala.collection.mutable.ArrayBufferobject Class &#123; def main(args: Array[String]): Unit = &#123; val c1=new Class val leo=c1.register(&quot;leo&quot;) c1.students+=leo val c2=new Class val jack=c2.register(&quot;jack&quot;) c2.students+=jack &#125;&#125;class Class&#123; class Student(val name:String) val students=new ArrayBuffer[Student] def register(name:String):Student=&#123; new Student(name) &#125;&#125; 通过伴生对象扩大内部类的作用域12345678910111213141516171819202122232425package com.scala.baseimport scala.collection.mutable.ArrayBuffer/** * 通过伴生对象扩大内部类的作用域 */object Class1 &#123; class Student(val name:String) def main(args: Array[String]): Unit = &#123; val c1=new Class1 val leo=c1.register(&quot;leo&quot;) c1.students+=leo val c2=new Class1 val jack=c2.register(&quot;jack&quot;) c1.students+=jack &#125;&#125;class Class1&#123; val students=new ArrayBuffer[Class1.Student] def register(name:String):Class1.Student=&#123; new Class1.Student(name) &#125;&#125; 通过类型投影扩大内部类作用域12345678910111213141516171819202122232425package com.scala.baseimport scala.collection.mutable.ArrayBuffer/** * 通过类型投影扩大内部类作用域 */object Class2 &#123; def main(args: Array[String]): Unit = &#123; val c1=new Class2 val leo=c1.register(&quot;leo&quot;) c1.students+=leo val c2=new Class2 val jack=c2.register(&quot;jack&quot;) c1.students+=jack &#125;&#125;class Class2&#123; class Student(val name:String) val students=new ArrayBuffer[Class2#Student] def register(name:String):Student=&#123; new Student(name) &#125;&#125; 内部类引用外部类的变量12345678910111213141516171819package com.scala.base/** * 内部类引用外部类的变量 */object InnerReferOuterClass &#123; def main(args: Array[String]): Unit = &#123; val c1=new Class3(&quot;c1&quot;) val leo=c1.register(&quot;leo&quot;) println(leo.introduceMyself) &#125;&#125;class Class3(val name:String)&#123;outer=&gt; class Student(val name:String)&#123; def introduceMyself=&quot;hello I am &quot; + name + &quot;I am very happy to join class &quot; + outer.name &#125; def register(name:String)=&#123; new Student(name) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-DataSet学习","slug":"Spark-DataSet学习20190524","date":"2017-05-09T00:54:59.000Z","updated":"2019-05-24T04:59:15.025Z","comments":true,"path":"2017/05/09/Spark-DataSet学习20190524/","link":"","permalink":"http://qioinglong.top/2017/05/09/Spark-DataSet学习20190524/","excerpt":"1.DataSet相关概念Dataset是一个分布式的数据集。Dataset是Spark 1.6开始新引入的一个接口，它结合了RDD API的很多优点（包括强类型，支持lambda表达式等），以及Spark SQL的优点（优化后的执行引擎）。Dataset可以通过JVM对象来构造，然后通过transformation类算子（map，flatMap，filter等）来进行操作。Scala和Java的API中支持Dataset，但是Python不支持Dataset API。不过因为Python语言本身的天然动态特性，Dataset API的不少feature本身就已经具备了（比如可以通过row.columnName来直接获取某一行的某个字段）。R语言的情况跟Python也很类似。 Dataframe就是按列组织的Dataset。在逻辑概念上，可以大概认为Dataframe等同于关系型数据库中的表，或者是Python/R语言中的data frame，但是在底层做了大量的优化。Dataframe可以通过很多方式来构造：比如结构化的数据文件，Hive表，数据库，已有的RDD。Scala，Java，Python，R等语言都支持Dataframe。在Scala API中，Dataframe就是Dataset[Row]的类型别名。在Java中，需要使用Dataset来代表一个Dataframe。","text":"1.DataSet相关概念Dataset是一个分布式的数据集。Dataset是Spark 1.6开始新引入的一个接口，它结合了RDD API的很多优点（包括强类型，支持lambda表达式等），以及Spark SQL的优点（优化后的执行引擎）。Dataset可以通过JVM对象来构造，然后通过transformation类算子（map，flatMap，filter等）来进行操作。Scala和Java的API中支持Dataset，但是Python不支持Dataset API。不过因为Python语言本身的天然动态特性，Dataset API的不少feature本身就已经具备了（比如可以通过row.columnName来直接获取某一行的某个字段）。R语言的情况跟Python也很类似。 Dataframe就是按列组织的Dataset。在逻辑概念上，可以大概认为Dataframe等同于关系型数据库中的表，或者是Python/R语言中的data frame，但是在底层做了大量的优化。Dataframe可以通过很多方式来构造：比如结构化的数据文件，Hive表，数据库，已有的RDD。Scala，Java，Python，R等语言都支持Dataframe。在Scala API中，Dataframe就是Dataset[Row]的类型别名。在Java中，需要使用Dataset来代表一个Dataframe。 2.DataSet操作 collect：将分布式存储在集群上的分布式数据集（比如dataset），中的所有数据都获取到driver端来 first：获取数据集中的第一条数据 persist()/cache():持久化，如果要对一个dataset重复计算两次的话，那么建议先对这个dataset进行持久化再进行操作，避免重复计算 createTempView(“employee”) explain():答应执行计划，dataframe/dataset，比如执行了一个sql语句获取的dataframe，实际上内部包含一个logical plan，逻辑执行计划，设计执行的时候，首先会通过底层的catalyst optimizer，生成物理执行计划，比如说会做一些优化，比如push filter，还会通过whole-stage code generation技术去自动化生成代码，提升执行性能 DataSet.write.save：将数据保存到指定目录 printSchema()：打印结构 将DataFrame转化为DataSet 12case class Employee(name: String, age: Long, depId: Long, gender: String, salary: Long)val employeeDS=employee.as[Employee] coalesce和repartition:都是用来重新定义分区的,区别在于：coalesce，只能用于减少分区数量，而且可以选择不发生shuffle,repartiton，可以增加分区，也可以减少分区，必须会发生shuffle，相当于是进行了一次重分区操作 distinct和dropDuplicates:都是用来进行去重的,distinct，是根据每一条数据，进行完整内容的比对和去重, dropDuplicates，可以根据指定的字段进行去重 1234val employeeDistinct=employeeDS.distinct() employeeDistinct.show() val employeeDropDup=employeeDS.dropDuplicates(Seq(&quot;name&quot;)) employeeDropDup.show() except：获取在当前dataset中有，但是在另外一个dataset中没有的元素 filter：根据我们自己的逻辑，如果返回true，那么就保留该元素，否则就过滤掉该元素 intersect：获取两个数据集的交集 123employeeDS.except(employeeDS2).show()employeeDS.intersect(employeeDS2).show()employeeDS.filter(employee=&gt;employee.age&gt;35).show() map：将数据集中的每条数据都做一个映射，返回一条新数据 flatMap：数据集中的每条数据都可以返回多条数据 mapPartitions：一次性对一个partition中的数据进行处理 123456789101112131415employeeDS.map(employee=&gt;( employee.name,employee.salary,employee.salary+1000)).show()employeeDS.flatMap(employee=&gt;Seq( (employee.name,employee.salary,employee.salary+1000), (employee.name,employee.salary,employee.salary+2000) )).show()employeeDS.mapPartitions(employee=&gt;&#123; val result=scala.collection.mutable.ArrayBuffer[(String,Long,Long)]() while(employee.hasNext)&#123; var temp=employee.next() result += ((temp.name,temp.salary,temp.salary+5000)) &#125; result.iterator&#125;).show() joinWith,两个DataSet关联，指定关联条件 1employee.joinWith(department, $&quot;deptId&quot; === $&quot;id&quot;).show() sort：排序 1employeeDS.sort($&quot;salary&quot;.desc).show() randomSplit/sample 123val employeeDSArr=employeeDS.randomSplit(Array(3,10,20))employeeDSArr.foreach(ds=&gt;ds.show())employeeDS.sample(false, 0.3).show() groupBy/agg/avg/sum/max/min/count/countDistinct 12345employee .join(department, $&quot;depId&quot; === $&quot;id&quot;) .groupBy(department(&quot;name&quot;)) .agg(avg(employee(&quot;salary&quot;)), sum(employee(&quot;salary&quot;)), max(employee(&quot;salary&quot;)), min(employee(&quot;salary&quot;)), count(employee(&quot;name&quot;)), countDistinct(employee(&quot;name&quot;))) .show() collect_list/collect_set:collect_list就是将一个分组内，指定字段的值都收集到一起，不去重,collect_set，同上，但是唯一的区别是，会去重 123456789/**[1,WrappedArray(Leo, Jack),WrappedArray(Jack, Leo)][3,WrappedArray(Tom, Kattie),WrappedArray(Tom, Kattie)][2,WrappedArray(Marry, Jen, Jen),WrappedArray(Marry, Jen)]*/ employee.groupBy(employee(&quot;depId&quot;)) .agg(collect_list(employee(&quot;name&quot;)),collect_set(employee(&quot;name&quot;))) .collect() .foreach(println)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"spark实例-DataSet之统计部门员工平均薪资和平均年龄","slug":"spark实例-DataSet之统计部门员工平均薪资和平均年龄20190524","date":"2017-05-08T02:54:59.000Z","updated":"2019-05-24T04:59:16.001Z","comments":true,"path":"2017/05/08/spark实例-DataSet之统计部门员工平均薪资和平均年龄20190524/","link":"","permalink":"http://qioinglong.top/2017/05/08/spark实例-DataSet之统计部门员工平均薪资和平均年龄20190524/","excerpt":"需求分析计算部门的平均薪资和年龄 只统计年龄在20岁以上的员工 根据部门名称和员工性别为粒度来进行统计 统计出每个部门分性别的平均薪资和年龄","text":"需求分析计算部门的平均薪资和年龄 只统计年龄在20岁以上的员工 根据部门名称和员工性别为粒度来进行统计 统计出每个部门分性别的平均薪资和年龄 关键技术点 导入隐式转化import spark.implicits._ 导入spark.sql.fucntionsimport org.apache.spark.sql.functions._ 两个表的字段的连接条件，需要使用三个等号$&quot;depId&quot; === $&quot;id&quot; groupBy聚合时，指定表及相应字段groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;)) agg聚合函数agg(avg(employee(&quot;salary&quot;)), avg(employee(&quot;age&quot;))) dataframe == dataset[Row],dataframe的类型是Row，所以是untyped类型，弱类型,dataset的类型通常是我们自定义的case class，所以是typed类型，强类型 dataset开发，与rdd开发有很多的共同点。dataset采用encoder序列化 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.spark.datasetimport org.apache.spark.sql.SparkSession/** * 计算部门的平均薪资和年龄 * * 需求： * 1、只统计年龄在20岁以上的员工 * 2、根据部门名称和员工性别为粒度来进行统计 * 3、统计出每个部门分性别的平均薪资和年龄 * */object DepartmentAvgSalaryAndAgeStat extends App&#123; val spark=SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;,&quot;E:\\\\worksplace\\\\spark\\\\spark-warehouse&quot;) .getOrCreate() //导入隐式转换 import spark.implicits._ //spark sql functions import org.apache.spark.sql.functions._ /**+---+--------------------+| id| name|+---+--------------------+| 1|Technical Department|| 2|Financial Department|| 3| HR Department|+---+--------------------+ */ val department=spark.read.json(&quot;E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\department.json&quot;)/**+---+-----+------+------+------+|age|depId|gender| name|salary|+---+-----+------+------+------+| 25| 1| male| Leo| 20000|| 30| 2|female| Marry| 25000|| 35| 1| male| Jack| 15000|| 42| 3| male| Tom| 18000|| 21| 3|female|Kattie| 21000|| 30| 2|female| Jen| 28000|| 19| 2|female| Jen| 8000|+---+-----+------+------+------+ */ val employee=spark.read.json(&quot;E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\employee.json&quot;) //department.show() //employee.show() //1.过滤20岁以上的员工 val filtedEmployee=employee.filter(&quot;age&gt;20&quot;) //filtedEmployee.show()/** +---+-----+------+------+------+---+--------------------+|age|depId|gender| name|salary| id| name|+---+-----+------+------+------+---+--------------------+| 25| 1| male| Leo| 20000| 1|Technical Department|| 30| 2|female| Marry| 25000| 2|Financial Department|| 35| 1| male| Jack| 15000| 1|Technical Department|| 42| 3| male| Tom| 18000| 3| HR Department|| 21| 3|female|Kattie| 21000| 3| HR Department|| 30| 2|female| Jen| 28000| 2|Financial Department|+---+-----+------+------+------+---+--------------------+ */ // 注意：untyped join，两个表的字段的连接条件，需要使用三个等号 val joined=filtedEmployee.join(department, $&quot;depId&quot; === $&quot;id&quot;) val result=employee // 先对employee进行过滤，只统计20岁以上的员工 .filter(&quot;age&gt;20&quot;) // 需要跟department数据进行join，然后才能根据部门名称和员工性别进行聚合 // 注意：untyped join，两个表的字段的连接条件，需要使用三个等号 .join(department, $&quot;depId&quot; === $&quot;id&quot;) // 根据部门名称和员工性别进行分组 .groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;)) // 最后执行聚合函数 .agg(avg(employee(&quot;salary&quot;)), avg(employee(&quot;age&quot;))) // 执行action操作，将结果显示出来/**+--------------------+------+-----------+--------+| name|gender|avg(salary)|avg(age)|+--------------------+------+-----------+--------+| HR Department|female| 21000.0| 21.0||Technical Department| male| 17500.0| 30.0||Financial Department|female| 26500.0| 30.0|| HR Department| male| 18000.0| 42.0|+--------------------+------+-----------+--------+ */ result.show()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-通过HDFS文件实时统计","slug":"Spark实例-通过HDFS文件实时统计20190524","date":"2017-05-07T06:54:59.000Z","updated":"2019-05-24T04:59:16.226Z","comments":true,"path":"2017/05/07/Spark实例-通过HDFS文件实时统计20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-通过HDFS文件实时统计20190524/","excerpt":"通过Spark Streaming，实时监控HDFS目录，发现有文件时，实时进行计算。","text":"通过Spark Streaming，实时监控HDFS目录，发现有文件时，实时进行计算。12345678910111213141516171819202122232425package com.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.Seconds/* * * 通过HDFS文件实时统计 */object HDFSWordCount extends App &#123; val conf = new SparkConf() .setAppName(&quot;HDFSWordCount&quot;) //.setMaster(&quot;hdfs://hadoop01:9000/&quot;) val ssc = new StreamingContext(conf,Seconds(5)) val lines = ssc.textFileStream(&quot;hdfs://hadoop01:9000/wordcount_dir&quot;) val words = lines.flatMap(line=&gt;line.split(&quot; &quot;)) val paris = words.map((_,1)) val wordCount=paris.reduceByKey(_+_) wordCount.print() ssc.start() ssc.awaitTermination() ssc.stop()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-自定义聚合函数","slug":"Spark实例-自定义聚合函数20190524","date":"2017-05-07T06:54:59.000Z","updated":"2019-05-24T04:59:16.249Z","comments":true,"path":"2017/05/07/Spark实例-自定义聚合函数20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-自定义聚合函数20190524/","excerpt":"Spark自定义聚合函数时，需要实现UserDefinedAggregateFunction中8个方法： inputSchema：输入的数据类型 bufferSchema：中间聚合处理时，需要处理的数据类型 dataType：函数的返回类型 deterministic：是否是确定的 initialize：为每个分组的数据初始化 update：每个分组，有新的值进来时，如何进行分组的聚合计算 merge：由于Spark是分布式的，所以一个分组的数据，可能会在不同的节点上进行局部聚合，就是update，但是最后一个分组，在各节点上的聚合值，要进行Merge，也就是合并 evaluate：一个分组的聚合值，如何通过中间的聚合值，最后返回一个最终的聚合值实例代码：","text":"Spark自定义聚合函数时，需要实现UserDefinedAggregateFunction中8个方法： inputSchema：输入的数据类型 bufferSchema：中间聚合处理时，需要处理的数据类型 dataType：函数的返回类型 deterministic：是否是确定的 initialize：为每个分组的数据初始化 update：每个分组，有新的值进来时，如何进行分组的聚合计算 merge：由于Spark是分布式的，所以一个分组的数据，可能会在不同的节点上进行局部聚合，就是update，但是最后一个分组，在各节点上的聚合值，要进行Merge，也就是合并 evaluate：一个分组的聚合值，如何通过中间的聚合值，最后返回一个最终的聚合值实例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.spark.sqlimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._/** * Created by Administrator on 2017/3/13. * 用户自定义聚合函数 */class StrCountUDAF extends UserDefinedAggregateFunction&#123; //输入的数据类型 override def inputSchema: StructType = &#123; StructType(Array( StructField(&quot;str&quot;,StringType,true) )) &#125; //中间聚合处理时，所处理的数据类型 override def bufferSchema: StructType = &#123; StructType(Array( StructField(&quot;count&quot;,IntegerType,true) )) &#125; //函数的返回类型 override def dataType: DataType = &#123; IntegerType &#125; override def deterministic: Boolean = &#123; true &#125; //为每个分组的数据初始化 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0)=0 &#125; //指的是，每个分组，有新的值进来时，如何进行分组的聚合计算 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0)=buffer.getAs[Int](0)+1 &#125; //由于Spark是分布式的，所以一个分组的数据，可能会在不同的节点上进行局部聚合，就是update //但是最后一个分组，在各节点上的聚合值，要进行Merge，也就是合并 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0)=buffer1.getAs[Int](0) + buffer2.getAs[Int](0) &#125; //一个分组的聚合值，如何通过中间的聚合值，最后返回一个最终的聚合值 override def evaluate(buffer: Row): Any = &#123; buffer.getAs[Int](0) &#125;&#125; 聚合函数的使用 1234567891011121314151617181920212223242526272829303132package com.spark.sqlimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._object UDAF extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;DailyUVFunction&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //导入隐式转化 import sqlContext.implicits._ //构造用户的访问数据，并创建DataFrame val names=Array(&quot;tom&quot;,&quot;yangql&quot;,&quot;mary&quot;,&quot;test&quot;,&quot;test&quot;) val namesRDD = sc.parallelize(names) //将RDD转换为DataFram val namesRowRDD=namesRDD.map(name=&gt;Row(name)) val structType=StructType(Array( StructField(&quot;name&quot;,StringType,true) )) val namesDF=sqlContext.createDataFrame(namesRowRDD,structType) //注册表 namesDF.createOrReplaceTempView(&quot;names&quot;) //定义和注册自定义函数 sqlContext.udf.register(&quot;strCount&quot;,new StrCountUDAF) //使用自定义函数 val df=sqlContext.sql(&quot;select name,strCount(name) from names group by name&quot;) df.show()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Parquet元数据合并","slug":"Spark实例-Parquet元数据合并20190524","date":"2017-05-07T05:54:59.000Z","updated":"2019-05-24T04:59:16.046Z","comments":true,"path":"2017/05/07/Spark实例-Parquet元数据合并20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-Parquet元数据合并20190524/","excerpt":"当文件使用Parquet格式时，如果多次生成的文件列不同，可以进行元数据的合并，不用再像关系型数据库那样多个表关联。关键点sqlContext.read.option(&quot;mergeSchema&quot;,true)","text":"当文件使用Parquet格式时，如果多次生成的文件列不同，可以进行元数据的合并，不用再像关系型数据库那样多个表关联。关键点sqlContext.read.option(&quot;mergeSchema&quot;,true)1234567891011121314151617181920212223242526272829303132333435package com.spark.sqlimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.&#123;SQLContext, SaveMode&#125;/** * Created by Administrator on 2017/3/12. * parquet数据元合并元数据 */object ParquetSchemaMerge extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;ParquetLoadData&quot;) val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) //导入隐式转换 import sqlContext.implicits._ //创建第一个RDD val studentWithNameAndAge=Array((&quot;Tom&quot;,13),(&quot;Mary&quot;,14)) val studentWithNameAndAgeDF=sc.parallelize(studentWithNameAndAge,1).toDF(&quot;name&quot;,&quot;age&quot;) //保存 studentWithNameAndAgeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) //创建第二个RDD val studentWithNameAndGrade=Array((&quot;Yangql&quot;,&quot;A&quot;),(&quot;Test&quot;,&quot;B&quot;)) val studentWithNameAndGradeDF=sc.parallelize(studentWithNameAndGrade).toDF(&quot;name&quot;,&quot;grade&quot;) studentWithNameAndGradeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) //用mergeSchema的方式读取数据，进行元数据的合并 val studentsDF=sqlContext.read.option(&quot;mergeSchema&quot;,true).parquet(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) studentsDF.show() studentsDF.printSchema()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-操作关系型数据库数据","slug":"Spark实例-操作关系型数据库数据20190524","date":"2017-05-07T04:54:59.000Z","updated":"2019-05-24T04:59:16.187Z","comments":true,"path":"2017/05/07/Spark实例-操作关系型数据库数据20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-操作关系型数据库数据20190524/","excerpt":"Spark操作关系型数据库数据，此处为MYSQL数据库。","text":"Spark操作关系型数据库数据，此处为MYSQL数据库。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.spark.sqlimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. */object JDBCDataSrc extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) import sqlContext.implicits._ val url=&quot;jdbc:mysql://localhost:3306/sparkpro&quot; val userName=&quot;root&quot; val password=&quot;root&quot; //创建Dataframe val studenInfoDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map( &quot;url&quot;-&gt;url, &quot;dbtable&quot;-&gt;&quot;student_info&quot;, &quot;user&quot;-&gt;userName, &quot;password&quot;-&gt;password )).load() studenInfoDF.show() //创建Dataframe val studentScoreDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map( &quot;url&quot;-&gt;url, &quot;dbtable&quot;-&gt;&quot;student_score&quot;, &quot;user&quot;-&gt;userName, &quot;password&quot;-&gt;password )).load() //studentScoreDF 转换为RDD，并过滤出分数大于80分的学生 val goodStudentRDD=studentScoreDF.rdd.filter(row=&gt;(row.getAs[Int](&quot;score&quot;)&gt;=80)) // for (elem &lt;- goodStudentRDD.collect()) &#123; // print(elem) // &#125; //a RDD for studenfInfo val studenInfoRDD=studenInfoDF.rdd.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;age&quot;))) .join(goodStudentRDD.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;score&quot;)))) val studenInfoRowRDD=studenInfoRDD.map(row=&gt;Row(row._1,row._2._1.toString.toLong,row._2._2.toString.toLong)) //studenInfoRDD.foreach(println) //将RDD转化为DataFrame val studentStruct=StructType(Array( StructField(&quot;name&quot;,StringType,true), StructField(&quot;age&quot;,LongType,true), StructField(&quot;score&quot;,LongType,true) )) val studentStructDF=sqlContext.createDataFrame(studenInfoRowRDD,studentStruct) studentStructDF.write.saveAsTable(&quot;good_student&quot;) //将数据插入到数据库中&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-操作Hive数据","slug":"Spark实例-操作Hive数据20190524","date":"2017-05-07T02:54:59.000Z","updated":"2019-05-24T04:59:16.068Z","comments":true,"path":"2017/05/07/Spark实例-操作Hive数据20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-操作Hive数据20190524/","excerpt":"Spark操作Hive数据库，实现数据表创建，数据加载，以及数据查询。实例代码如下：","text":"Spark操作Hive数据库，实现数据表创建，数据加载，以及数据查询。实例代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.spark.sqlimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. */object HiveDataSource extends App&#123; val conf = new SparkConf() .setAppName(&quot;HiveDataSource&quot;) val sc = new SparkContext(conf) val hiveContext = new HiveContext(sc) //创建student_infos表 hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_INFO&quot;) //判断表是否存在，不存在则创建 hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_INFO(NAME STRING,AGE INT)&quot;) //导入数据 hiveContext.sql(&quot;LOAD DATA &quot; + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_info.txt&apos; &quot; + &quot;INTO TABLE STUDENT_INFO&quot;) //创建student_score表 hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_SCORE&quot;) //判断表是否存在，不存在则创建 hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_SCORE (NAME STRING,SCORE INT)&quot;) //导入数据 hiveContext.sql(&quot;LOAD DATA &quot; + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_score.txt&apos; &quot; + &quot;INTO TABLE STUDENT_SCORE&quot;) //查询分数大于80的学生信息,并保存到good_student信息 val goodStudentsDF=hiveContext.sql(&quot;SELECT T1.NAME,T1.AGE,T2.SCORE &quot; + &quot;FROM student_info T1 &quot; + &quot;INNER JOIN student_SCORE T2 &quot; + &quot;ON T1.NAME=T2.NAME &quot; + &quot;WHERE T2.SCORE&gt;80&quot;) hiveContext.sql(&quot;DROP TABLE IF EXISTS GOOD_STUDENT&quot;) //goodStudentsDF.saveAsTable(&quot;GOOD_STUDENT&quot;) goodStudentsDF.write.saveAsTable(&quot;GOOD_STUDENT&quot;) //查询打印 val goodStudentsRows=hiveContext.table(&quot;GOOD_STUDENT&quot;).collect() for(goodStudentsRow &lt;- goodStudentsRows)&#123; println(goodStudentsRow) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-DataFrame加载和保存数据","slug":"Spark实例-DataFrame加载和保存数据20190524","date":"2017-05-07T01:54:59.000Z","updated":"2019-05-24T04:59:15.946Z","comments":true,"path":"2017/05/07/Spark实例-DataFrame加载和保存数据20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实例-DataFrame加载和保存数据20190524/","excerpt":"Spark加载不同格式文件时，调用sqlContext.read.format(“”).load方法 1val peopleDF=sqlContext.read.format(&quot;json&quot;).load(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot;) Spark将DataFrame写入到文件中时，调用DF.write.format(“”).save方法 12345peopleDF.select(&quot;name&quot;) .write.format(&quot;parquet&quot;) //.mode(SaveMode.ErrorIfExists) .mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people&quot;)","text":"Spark加载不同格式文件时，调用sqlContext.read.format(“”).load方法 1val peopleDF=sqlContext.read.format(&quot;json&quot;).load(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot;) Spark将DataFrame写入到文件中时，调用DF.write.format(“”).save方法 12345peopleDF.select(&quot;name&quot;) .write.format(&quot;parquet&quot;) //.mode(SaveMode.ErrorIfExists) .mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people&quot;) 代码示例 123456789101112131415161718192021222324252627282930package com.spark.sqlimport org.apache.spark.sql.&#123;SQLContext, SaveMode&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. * 通用加载数据和保存数据 * 文件保存模式 * 1.SaveMode.ErrorIfExists * parquet文件的有点【列式存储】 * 1.可以跳过不符合条件的数据，只读取需要的数据，降低IO量 * 2.压缩编码可以降低磁盘的存储空间，由于同一类的数据类型是相同的，可以使用更高效的压缩编码，进一步节约存储空间 * 3.只读取需要的列，支持向量运算，能够获得更好的扫描性能 */object GenericLoadAndSave extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;GenericLoadAndSave&quot;) val sc = new SparkContext(conf) val sqlContext= new SQLContext(sc) //加载数据 val peopleDF=sqlContext.read.format(&quot;json&quot;).load(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot;) //保存到目录 peopleDF.select(&quot;name&quot;) .write.format(&quot;parquet&quot;) //.mode(SaveMode.ErrorIfExists) .mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people&quot;)&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-每天每个搜索词用户访问","slug":"Spark实战-每天每个搜索词用户访问20190524","date":"2017-05-07T00:54:59.000Z","updated":"2019-05-24T04:59:16.291Z","comments":true,"path":"2017/05/07/Spark实战-每天每个搜索词用户访问20190524/","link":"","permalink":"http://qioinglong.top/2017/05/07/Spark实战-每天每个搜索词用户访问20190524/","excerpt":"1.需求分析根据平台的日志记录，统计处每天每个搜索词访问的UV，并把排名前三的搜索词及其UV打印出来。 2.数据格式日期 用户 搜索词 城市 平台 版本","text":"1.需求分析根据平台的日志记录，统计处每天每个搜索词访问的UV，并把排名前三的搜索词及其UV打印出来。 2.数据格式日期 用户 搜索词 城市 平台 版本123456789101112131415162017-03-13,leo,barbecue,beijing,android,1.02017-03-13,leo,barbecue,beijing,android,1.02017-03-13,leo,barbecue,beijing,android,1.02017-03-13,leo,cloth,beijing,android,1.02017-03-13,leo2,cloth,beijing,android,1.02017-03-13,jack,barbecue,shanghai,android,1.12017-03-13,leo,paper,beijing,ios,1.02017-03-13,tom,barbecue,beijing,android,1.22017-03-13,leo,cup,beijing,android,1.02017-03-13,mary,barbecue,beijing,android,1.22017-03-13,leo,barbecue,beijing,ios,1.32017-03-13,leo,cup,beijing,android,1.02017-03-13,leo1,cup,beijing,android,1.02017-03-13,leo2,cup,beijing,android,1.02017-03-13,leo3,cup,beijing,android,1.02017-03-13,leo4,cup,beijing,android,1.0 3.实现分析 针对原始数据（HDFS）获得输入的RDD 使用filter算子，去针对RDD输入的数据，进行数据过滤，过滤出符合条件的数据 将数据转换为“日期_搜索词，用户”格式，对他进行分组，对每天每个搜索词用户进行去重复操作，并统计去重后的数据，即为每天每个搜索词的UV 最后获得“日期_搜索词，UV” 4.代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package com.spark.sqlimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types.&#123;LongType, StringType, StructField, StructType&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.mutable.ListBufferobject DailyTop3KeyWord extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;DailyTop3KeyWord&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //导入隐式转化 import sqlContext.implicits._ //伪造一份数据，查询条件，实际情况是通过MYSQL关系数据库查询 var queryPara = Map( &quot;city&quot; -&gt; List(&quot;beijing&quot;), &quot;platform&quot; -&gt; List(&quot;android&quot;), &quot;version&quot; -&gt; List(&quot;1.0&quot;,&quot;1.1&quot;,&quot;1.2&quot;,&quot;1.3&quot;) ) //将查询条件封装为一个BroadCast变量 val queryParaBroadCast = sc.broadcast(queryPara) //读取数据 val searchLogRDD=sc.textFile(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\searchLog.txt&quot;) //使用广播变量进行筛选 /** println(queryParaBroadCast.value.get(&quot;city&quot;)) val city = queryParaBroadCast.value.get(&quot;city&quot;).get val version = queryParaBroadCast.value.get(&quot;version&quot;).get println(version) if(version.contains(&quot;1.0&quot;))&#123; println(&quot;1.0&quot;) &#125;else&#123; println(&quot;no&quot;) &#125; */ //通过广播变量筛选符合条件的数据 val filtedSearchLogRDD=searchLogRDD.filter(log=&gt;&#123; val queryParamValue=queryParaBroadCast.value val citys=queryParamValue.get(&quot;city&quot;).get; val platforms=queryParamValue.get(&quot;platform&quot;).get; val versions=queryParamValue.get(&quot;version&quot;).get; val city=log.split(&quot;,&quot;)(3); val platform=log.split(&quot;,&quot;)(4); val version=log.split(&quot;,&quot;)(5); var flag=true; if(!citys.contains(city))&#123; flag=false &#125; if(!platforms.contains(platform))&#123; flag=false &#125; if(!versions.contains(version))&#123; flag=false &#125; flag &#125;) //将过滤出来的日志映射成“日期_搜索词,用户” val dateKeywordRDD=filtedSearchLogRDD.map(row=&gt;( row.split(&quot;,&quot;)(0)+&quot;_&quot;+row.split(&quot;,&quot;)(2),row.split(&quot;,&quot;)(1) )) //dateKeywordRDD.foreach(println) //进行分组，获取每天每个个搜索词，有哪些用户搜索了，（没有去重） val dateKeywordUserRDD=dateKeywordRDD.groupByKey() dateKeywordUserRDD.collect().foreach(println) /**(2017-03-13_cloth,CompactBuffer(leo, leo2))(2017-03-13_cup,CompactBuffer(leo, leo, leo1, leo2, leo3, leo4))(2017-03-13_barbecue,CompactBuffer(leo, leo, leo, tom, mary)) */ //将相同行数合并，并计算用户访问的次数，“日期_搜索词,UV” val dateKeywordUVRDD=dateKeywordUserRDD.map(row=&gt;&#123; val dateKeyWord=row._1 val users=row._2.iterator val distinctUsers=new ListBuffer[String] while(users.hasNext)&#123; val user=users.next().toString() if(!distinctUsers.contains(user))&#123; distinctUsers.append(user) &#125; &#125; val uv=distinctUsers.size (dateKeyWord,uv) &#125;) //将“日期_搜索词,UV”转换为DataFrame val dateKeywordUVRowRDD=dateKeywordUVRDD.map(row=&gt;Row(row._1.split(&quot;_&quot;)(0),row._1.split(&quot;_&quot;)(1),row._2.toString.toLong)) val structType=StructType(Array( StructField(&quot;date&quot;,StringType,true), StructField(&quot;keyword&quot;,StringType,true), StructField(&quot;uv&quot;,LongType,true) )) val dateKeywordUVDF=sqlContext.createDataFrame(dateKeywordUVRowRDD,structType) //注册临时函数 dateKeywordUVDF.createOrReplaceTempView(&quot;daily_keyword_uv&quot;) //利用spark sql开窗函数，统计每天搜索UV排名前三的搜索词 val dailyTop3KeyWordDF=sqlContext.sql(&quot;&quot; + &quot;select date,&quot; + &quot; keyword,&quot; + &quot; uv &quot; + &quot; from &quot; + &quot; (select &quot; + &quot; date,&quot; + &quot; keyword,&quot; + &quot; uv,&quot; + &quot; row_number() over(partition by date order by uv desc ) rn &quot; + &quot; from daily_keyword_uv) &quot; + &quot; where rn &lt;=3&quot;) dailyTop3KeyWordDF.show() sc.stop()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hbase安装","slug":"Hbase安装20190524","date":"2017-04-19T04:54:59.000Z","updated":"2019-05-24T04:59:12.653Z","comments":true,"path":"2017/04/19/Hbase安装20190524/","link":"","permalink":"http://qioinglong.top/2017/04/19/Hbase安装20190524/","excerpt":"1. 解压缩hbase的软件包1[yangql@hadoop01 app]$ tar -xvf hbase-1.2.5-bin.tar.gz 2. 进入hbase的配置目录，在hbase-env.sh文件里面加入java环境变量.1export JAVA_HOME=/opt/jdk1.8.0_91 3. 关闭HBase自带的Zookeeper,使用Zookeeper集群1export HBASE_MANAGES_ZK=false","text":"1. 解压缩hbase的软件包1[yangql@hadoop01 app]$ tar -xvf hbase-1.2.5-bin.tar.gz 2. 进入hbase的配置目录，在hbase-env.sh文件里面加入java环境变量.1export JAVA_HOME=/opt/jdk1.8.0_91 3. 关闭HBase自带的Zookeeper,使用Zookeeper集群1export HBASE_MANAGES_ZK=false 4. 编辑hbase-site.xml ，添加配置文件123456789101112131415161718&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01,hadoop02,hadoop03&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/yangql/app/hbase-1.2.5/tmp/zk/data&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 5. 编辑配置目录下面的文件regionservers12hadoop02hadoop03 6.配置环境变量123# config for hbaseexport HBASE_HOME=/home/yangql/app/hbase-1.2.5export PATH=$PATH:$PATH/bin 7.把Hbase复制到其他机器12345[yangql@hadoop01 app]$ scp -r hbase-1.2.5/ yangql@hadoop02:/home/yangql/app/[yangql@hadoop01 app]$ scp -r hbase-1.2.5/ yangql@hadoop03:/home/yangql/app/scp .bash_profile yangql@hadoop02:/home/yangqlscp .bash_profile yangql@hadoop03:/home/yangqlsource .bash_profile 8.启动 hbase hadoop 123456sh /home/yangql/app/hadoop-2.7.2/sbin/start-all.sh7843 Master7892 Jps7756 ResourceManager7406 NameNode7599 SecondaryNameNode 启动zookeeper 1234567sh /home/yangql/app/zookeeper-3.4.6/bin/zkServer.sh start7843 Master9541 Jps7756 ResourceManager7406 NameNode8126 QuorumPeerMain7599 SecondaryNameNode 启动HBase 123456789101112131415161718[yangql@hadoop01 bin]$ sh start-hbase.shstarting master, logging to /home/yangql/app/hbase-1.2.5/logs/hbase-yangql-master-hadoop01.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop02: starting regionserver, logging to /home/yangql/app/hbase-1.2.5/bin/../logs/hbase-yangql-regionserver-hadoop02.outhadoop03: starting regionserver, logging to /home/yangql/app/hbase-1.2.5/bin/../logs/hbase-yangql-regionserver-hadoop03.outhadoop03: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0hadoop03: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop02: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0hadoop02: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.07843 Master9541 Jps8806 HMaster7756 ResourceManager7406 NameNode8126 QuorumPeerMain7599 SecondaryNameNode[yangql@hadoop01 bin]$ 9. 验证123456789101112131415161718[yangql@hadoop01 bin]$ sh hbase shell2017-04-19 00:47:18,635 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.5, rd7b05f79dee10e0ada614765bb354b93d615a157, Wed Mar 1 00:34:48 CST 2017hbase(main):001:0&gt; status1 active master, 0 backup masters, 2 servers, 0 dead, 1.0000 average loadhbase(main):002:0&gt; version1.2.5, rd7b05f79dee10e0ada614765bb354b93d615a157, Wed Mar 1 00:34:48 CST 2017hbase(main):003:0&gt; listTABLE 0 row(s) in 0.0990 seconds=&gt; []hbase(main):004:0&gt; 10. web查看集群状态12http://192.168.1.231:16010/master-statushttp://192.168.1.232:16030/rs-status","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://qioinglong.top/tags/Hbase/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive-《编程指南》学习笔记","slug":"Hive-《编程指南》学习笔记20190524","date":"2017-04-17T00:54:59.000Z","updated":"2019-05-24T04:59:12.781Z","comments":true,"path":"2017/04/17/Hive-《编程指南》学习笔记20190524/","link":"","permalink":"http://qioinglong.top/2017/04/17/Hive-《编程指南》学习笔记20190524/","excerpt":"基础操作 hive.metastore.warehouse.dir表存储所位于的顶级文件目录，默认是/user/hive/warehouse 可以为不同的用户指定不同的目录，避免相互影响.set hive.metastore.warehouse.dir=/user/myname/hive/warehouse hive默认数据是derby，他会在每个命令启动的当前目录创建metadata_db目录。可以设置hive-site.xml 1javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/home/yangql/app/hive/metastore_db;create=true hive配置连接mysql数据库 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;","text":"基础操作 hive.metastore.warehouse.dir表存储所位于的顶级文件目录，默认是/user/hive/warehouse 可以为不同的用户指定不同的目录，避免相互影响.set hive.metastore.warehouse.dir=/user/myname/hive/warehouse hive默认数据是derby，他会在每个命令启动的当前目录创建metadata_db目录。可以设置hive-site.xml 1javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/home/yangql/app/hive/metastore_db;create=true hive配置连接mysql数据库 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; hive —service name 启动某个服务 选项 名称 描述 cli 命令行界面 用户定义表，执行查询，默认服务 hiveserver Hive Server 监听来自其他进程的Thrift连接的一个守护进程 hwi hive web界面 jar hadoop jar的一个扩展 metastore 启动一个扩展的Hive元数据服务 rcfilecat 一个可以打印出RCFile格式文件工具的内容 —auxpath:选项允许用户一个以冒号分割的附属Jar包，这些文件中包含有用户可能需要的自定义扩张。—config 文件目录，允许用户覆盖$HIVE_HOME/conf中默认的属性配置，而指向一个新的配置文件目录。 hive -h 显示帮助信息 1234567891011121314151617yangql@hadoop01 conf]$ hive -hwhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/app/kafka-2.12/bin:/home/yangql/app/flume-1.7.0/bin)Unrecognized option: -husage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) hive中变量和属性命名空间 hivevar 用户自定义变量 hiveconf hive相关的配置文件 system Java定义的配置属性 env shell环境变量 定义一个变量hive --define foo=bar;在cli中，变量时先被替换掉，才提交给查询处理器 1234567891011hive&gt; set hivevar:foo;hivevar:foo=barhive&gt; create table test(id INT,$&#123;hivevar:foo&#125; string);OKTime taken: 1.769 secondshive&gt; desc test;OKid int bar string Time taken: 0.238 seconds, Fetched: 2 row(s)hive&gt; hiveconf配置hive行为的所有属性，例如配置答应出当前数据的名字 123456789[yangql@hadoop01 conf]$ hive --hiveconf hive.cli.print.current.db=truewhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/app/kafka-2.12/bin:/home/yangql/app/flume-1.7.0/bin)Logging initialized using configuration in file:/home/yangql/app/hive-2.1.1/conf/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive (default)&gt; set hiveconf:hive.cli.print.current.db &gt; ;hiveconf:hive.cli.print.current.db=truehive (default)&gt; 一次使用命令 1234[yangql@hadoop01 conf]$ hive -e &quot;select * from t2&quot;;-S:静默方法//将查询结果输出到文件中[yangql@hadoop01 bin]$ hive -S -e &quot;select * from t3 limit 3&quot; &gt; test.txt 查询变量的方法 12345[yangql@hadoop01 bin]$ hive -S -e &quot;set&quot; | grep warehousewhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/app/kafka-2.12/bin:/home/yangql/app/flume-1.7.0/bin)hive.metastore.warehouse.dir=/user/hive/warehousehive.warehouse.subdir.inherit.perms=true[yangql@hadoop01 bin]$ 从文件执行查询 123[yangql@hadoop01 bin]$ hive -f querysql.hql可以进入后，再用source命令：hive&gt; source querysql.hql hive构建src表，相当于关系数据库里dual表 123create table src(s STRING)echo &quot;one row&quot;&gt;/home/yangql/myfile.txt[yangql@hadoop01 bin]$ hive -e &quot;LOAD DATA LOCAL INPATH &apos;/home/yangql/myfile.txt&apos; INTO TABLE SRC&quot;; hivesrc文件Hive自动在HOME目录下寻找名为.hiverc的文件，在提示符出现之前执行这个文件. 1set hive.cli.print.current.db=true 历史命令,hive 会将最近10000条行命令记录到文件$HOME/.hivehistory中 hive 内使用shell命令 ! command ; hive 内使用hadoop命令 dfs -ls 开启打印字段名称：hive.cli.print.header=true 基本数据类型 数据类型 长度 TINYINT 1BYTE SMALLINT 2BYTE INT 4BYTE BIGINT 8BYTE BOOLEAN TRUE FALSE FLOAT 单精度 DOUBLE 双精度 STRING 字符串 TIMESTAMP 整数，浮点数，字符串 BINARY 字节数组 集合数据类型有三种，STRUCT(‘john’,’Doe’)，MAP(‘first’,’John’,’last’,’Doe’)，ARRAY(‘John’,’Doe’),使用例子 1234567create table employee(name STRING,salary FLOAT,subemloyers ARRAY&lt;STRING&gt;,deducation MAP&lt;STRING,FLOAT&gt;,address STRUCT&lt;street:STRING,citr:STRING,state:STRING&gt;) hive中默认的记录和字段分隔符 分隔符 描述 \\n 记录分隔符 ^A CTRL-A 列分隔符，在create table中八进制\\001表示 ^B STRUCT，ARRAY,MAP分割，在create table中八进制\\002表示 ^C MAP键值对分割，在create table中八进制\\003表示 hive 读时模式，在加载数据的时候，并不去验证数据与模式是否匹配，而是在查询时进行（schema on read）数据定义 hive不支持行级插入，更新，删除操作，也不支持事务 hive中的数据库本质上仅仅只是一个目录或者命名空间，如果没有显示指定，默认数据库default,创建数据库如下(if not exists可选).hive会给每个数据库创建一个目录,default除外。也可以通过location指定目录 12345678910111213141516--创建hive (default)&gt; create database if not exists financials;--查看hive (default)&gt; show databases;--加正则表达式查看hive (default)&gt; show databases like &apos;pactera*&apos;;--指定目录hive (default)&gt; create database specialdir location &apos;/user/hive/warehouse&apos;;--添加注释hive (default)&gt; create database financials01 comment &apos;test&apos;;--查看数据库注释hive (default)&gt; desc database financials01--为数据库增加键值对信息hive (default)&gt; create database financialextend with dbproperties(&apos;createor&apos;=&apos;yangql&apos;,&apos;date&apos;=&apos;20170414&apos;);--显示扩展信息hive (default)&gt; desc database extended financialextend; 切换数据库use databaseName 删除数据库 drop database if exists databaseName ,if exists是可选的。默认情况下，hive不允许删除一个包含表的数据库。有两种方式 先删除数据库下所有表，再删除数据库 加上casecase关键字。hive会将数据库下的所有表删除，然后删除数据库 修改数据库，修改数据库只能修改dbproperties，数据库其它元数据信息不能修改。 1hive (default)&gt; alter database pactera set dbproperties(&apos;edited_by&apos;,&apos;Joe&apos;); 创建表 123456create table employees(name STRING comment &apos;employee Name&apos;,salary FLOAT comment &apos;salary&apos;)comment &apos;desc of table&apos;tblproperties(&apos;creator&apos;=&apos;me&apos;,&apos;created_at&apos;=&apos;2017-04-15&apos;); 拷贝表模式 1create table if not exists employees2 like employees; show tables:显示当前数据库下所有表，show tables in pactera:显示某个数据库下所有表 查看表的扩展信息 12desc extended employees; --显示的信息多desc formatted employees; --显示的信息可读性好 管理表，也称内部表，hive会控制着数据的生命周期，当我们删除一个表时，hive也会删除表对应的数据。 外部表，数据被多个工具共享时，可创建外部表，删除表后，数据不会被删除，删除的只是表的元数据，创建外部表如下: 1234567create external table if not exists stocks(stock_id STRING,price FLOAT)row format delimitedfields terminated by &apos;,&apos;location &apos;/user/hive/warehouse/stocks&apos;; 分区表该表了hive对数据存储的组织方式，创建分区表 1234567891011121314151617181920create table employees(name string,salary float,address struct&lt;street:string,city:string,state:string,zip:int&gt;)partitioned by (country string,state string);--查看分区的信息show partitions employees;--查看某个指定的分区show partitions employees partition(country=&apos;US&apos;,state=&apos;AK&apos;)--设置参数 hive.mapred.mode=strictset hive.mapred.mode=strict=truehive (mydb)&gt; set hive.mapred.mode=strict;hive (mydb)&gt; set hive.mapred.mode &gt; ;hive.mapred.mode=stricthive (mydb)&gt; select * from employees;FAILED: SemanticException Queries against partitioned tables without a partition filter are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.large.query is set to false and that hive.mapred.mode is not set to &apos;strict&apos; to enable them. No partition predicate for Alias &quot;employees&quot; Table &quot;employees&quot;hive (mydb)&gt; 将hive设置为strict后，对分区进行查询，如果没有指定where语句，将会禁止提交这个任务。 自定义表的存储格式12345678910create table employee02(name string,salary float)row format delimitedfields terminated by &apos;\\001&apos;collection items terminated by &apos;\\002&apos;map keys terminated by &apos;\\003&apos;lines terminated by &apos;\\n&apos;stored as textfile; row format delimitedfields terminated by ‘\\001’ —列分割collection items terminated by ‘\\002’ —集合分割map keys terminated by ‘\\003’ —map 分割lines terminated by ‘\\n’ —行分割stored as textfile; —存储格式 alter table修改表，修改的只是元数据，并不会修改数据本身。1234567891011121314151617181920212223242526272829303132333435--重命名表名hive (mydb)&gt; alter table employee02 rename to employee03;--增加分区alter table log_message add if not existspartition(year=2017,month=1,day=1) location &apos;/logs/2017/01/01&apos;partition(year=2017,month=1,day=2) location &apos;/logs/2017/01/02&apos;--修改分区的路径alter table log_message partition(year=2017,month=1,day=1) set location &apos;/logs/2017/01/01&apos;--删除分区alter table log_message drop if exists partition(year=2017,month=1,day=1)--修改字段的列,修改列名，移动到某个字段之后，fisrt移动到最前alter table employee03 change column salary salary_total float comment &apos;total salary&apos; after name;alter table employee03 change column salary_total total_salary double comment &apos;total salary&apos; first;--增加列alter table employee03 add columns(app_name string,session_id string)--移除替换列alter table employee03 replace columns(name string,salary double,app_name string,session_id string);--修改表的属性alter table employee03 set tblproperties(&apos;notes&apos;=&apos;the process id is no longer captured&apos;)--表中文件被改，增加钩子hive -e &quot;alter table log_message touch partition(year=2012,moonth=1,day=1);&quot;--将分区内文件打成一个hadoop压缩包，HAR.降低文件系统中的文件数以及减轻namenode压力，而不会减少任何的存储空间,反向操作,unarchivealter table log_message archive partition(year=2012,month=01,day=01)--防止分区被删除alter table log_message partition(year=2012,month=01,day=01) enable no_drop;--禁止查询分区数据alter table log_message partition(year=2012,month=01,day=01) enable offline; 数据操作 将数据加载到表中，如果目录不存在的话，会先创建目录，然后再将数据拷贝到目录下.通常指定的路径是一个目录，而不是单个文件.这使得用户可以组织数据到多文件中,同时可以在不修改hive脚本的前提下修改文件名 1load data loca inpath &apos;$&#123;env:HOME&#125;/employee&apos; overwrite into table employees partition(country=&apos;US&apos;,statue=&apos;CA&apos;) load data local inpath:拷贝本地数据到位于分布式文件系统上的目标位置 load data inpath:转移数据到目标位置(转移)，要求源文件，目标文件，目录应该在同一个集群中。 overwrite:目标文件夹之前存在的文件会先被删除。如果没有使用overwrite关键字，而目标文件下已经存在相同文件名时，会保留之前的文件并重命名问，文件名_序号 inpath目录下不能再包含目录 hive并不会验证用户装载的数据和表的模式是否匹配，但是会验证文件格式是否与表定义的是否一直。如表定义时定义的存储格式是sequencefile,那么装载进去的文件也应该是sequencefile才对。 通过查询语句向表中插入数据，适用场景：目标数据的文件格式，分隔符与当前需要的数据格式，分隔符不想同时，可以使用这种方式。其中overwrite表示覆盖，into追加 1234567891011insert overwrite table employeespartition(country=&apos;US&apos;,state=&apos;OR&apos;)select * from staged_employee se where se.cnty=&apos;US&apos; and se.st=&apos;OR&apos;;--一次查询，多次插入from staged_employee seinsert overwrite table employeespartition(country=&apos;US&apos;,state=&apos;OR&apos;)select * from staged_employee se where se.cnty=&apos;US&apos; and se.st=&apos;OR&apos;insert overwrite table employeespartition(country=&apos;US&apos;,state=&apos;CA&apos;)select * from staged_employee se where se.cnty=&apos;US&apos; and se.st=&apos;CA&apos; 动态分区插入,hive会根据最后两列来确认分区字段country,state的值，源表字段值和输出分区值之间的关系是根据位置而不是根据命名来的。用户也可以混合使用动态分区和静态分区,但是静态分区必须在动态分区之前 1234567insert overwrite table employeespartition(country,state)select ...,cnty,st from staged_employee;--混合使用动态分区和静态分区insert overwrite table employeespartition(country=&apos;US&apos;,state)select ...,cnty,st from staged_employee where cnty=&apos;US&apos;; 动态分区的属性 12345678910--开启和关闭动态分区hive.exec.dynamic.partition=true--设置成nostrict，表示允许所有分区都是动态的hive.exec.dynamic.partition.mode=nostrict--每个node创建的最大分区数hive.exec.max.dynamic.partitions.pernode=100--一个动态分区创建语句可以创建的最大分区数hive.exec.max.dynamic.partitions=1000--全局可以创建的最大文件数hive.exec.max.created.files=100000 单个查询语句中创建表并加载数据，适用场景：从一个大表中选取部分需要的数据 1create table ca_employee as select name,salary,address from employees where state=&apos;CA&apos;; 导出数据，如果文件的格式满足用户需求，直接将文件拷贝到本地就行 1hdfs dfs -cp source_path target_path 如果不满足需求1insert overwrite local directory &apos;/home/yangql/data/employees&apos; select * from user_info; 可以指定多个输出文件夹目录123from staged_employee seinsert overwrite directory &apos;directory1&apos; select * from employees where name=&apos;1&apos;insert overwrite directory &apos;directory2&apos; select * from employees where name=&apos;2&apos; 导出数据如果不满足用户的格式时，可以先创建一个临时表，将数据插入到临时表中，再从临时表中导出。查询 查询字段是集合时，会以JSON格式显示。取集合数据时，使用.ARRAY[0],Map时。MAP[key],STRUCT时。STRUCT.name 使用正则表达式指定列 表生成函数：explode 什么情况下hive可以避免进行Mapreduce 本地查询select * from tableName,Hive读取对应存储目录下的文件 where条件只有分区字段的情况 set hive.exec.mode.local.auto=true;Hive会尝试使用本地模式执行其它的操作 RLIKE使用正则表达式 其它SQL方言中in exists可以使用left semi join（左半开连接）实现，但是不能查询右边表的字段。对于左表中的一条记录，在右边表中一旦找到匹配记录，就会停止扫描 123select t1.* from user_info t1left semi join ids t2on t1.user_ids=t2.user_ids map-side join ,所有表中只有一张小表时，那么可以在最大的表通过mapper的时候将小表完全放到内存中，0.7以前通过/*+ MAPJOIN(表别名) */，0.7后，设置参数 123set hive.auto.convert.join=true--设置可以配置使用这个优化的小表的大小set hive.mapjoin.smalltable.filesize=25000000 right outer join,full outer join 不支持这个优化 order by 全局排序，sorted by 局部排序 distribute by:默认情况下，Mapreduce计算框架会依据map输入的键计算相应的哈希值，然后按照得到哈希键值对均匀分发到多个reducer中去，这也意味着，当我们使用sort by时，不同的reducer的输出内容会有明显的重叠。如果我们希望同一类型的交易数据在同一个 reducer处理。我们可以使用distribute by ，如将同一性别的用户放到同一reducer上处理,distribute by 必须在sort by之前 123select * from user_infodistribute by sexsort by sex; cluster by :相当于 distribute by sex , sort by sex ,两个语句中涉及到的列完全相同，采用的是升序排列。那cluster by 将相当于是这两个语句的简写 12select * from user_infocluster by sex; 类型转换 cast(value as type),将value转换为相应的数据类型。当不能转换时，返回NULL值。将浮点数转为整数round,floor,ceil。 抽样查询， tablesample(bucket 3 out of 10 on rand()) s。分母10表示数据将会被散列的桶的个数，分子表示将会选择的桶的个数。 1234--如果按照rand()函数进行随机抽样，这个函数会返回一个随机数select * from user_info tablesample(bucket 3 out of 10 on rand()) s;--按照指定的列进行随机抽样，同一语句返回的结果是一样的。select * from user_info tablesample(bucket 3 out of 10 on user_ids) s; 数据块抽样,按照抽样百分比进行抽样。这种是基于行数的。按照数据路径下的数据块百分比进行抽样。这种抽样的最小单元是HDFS的一个数据块。如果数据的大小小于普通块大小128MB。那么将会返回所有行。 1select * from user_info tablesample(0.1 percent) s; 基于百分比的抽样方式提供的变量。用于控制基于数据块的调优的种子信息。12345&lt;property&gt; &lt;name&gt;hive.sample.seednumber&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;A number used to percentage sampling. By changing this number, user will change the subsets of data sampled.&lt;/description&gt;&lt;/property&gt; 分桶表的数据裁剪,数据将会被聚集成10个buckets12create table user_info_ids(user_ids string) clustered by (user_ids) into 10 buckets;insert overwrite table user_info_ids select user_ids from user_info; 视图 创建视图,创建Hive视图的两个作用1.降低查询语句的复杂度，2.隐藏敏感信息。Hive先解析视图，然后再使用解析结果再来解析整个查询语句 1create view user_info_female as select * from user_info where sex=&apos;female&apos;; 删除视图:drop view if exists xxx 索引 创建索引 123456create index user_info_index1 on table user_info(sex)as &apos;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&apos;with deferred rebuildidxproperties(&apos;creator&apos;=&apos;me&apos;)in table user_info_indexcomment &apos;index&apos;; 创建bitmap索引 123456create index user_info_index2 on table user_info(sex)as &apos;BITMAP&apos;with deferred rebuildidxproperties(&apos;creator&apos;=&apos;me&apos;)in table user_info_index2comment &apos;index&apos;; 重建索引：如果用户指定了deferred rebuild,那么新索引将呈现空白状态，在任何时候，都可以进行第一次索引创建或者使用alter index对索引进行重建 1alter index user_info_index2 on user_info rebuild; 显示索引 1show formatted index on user_info; 删除索引 1drop index if exists user_info_index2 on user_info; 调优 explain如何将查询转化为Mapreduce 限制调整，在多数情况下，limit语句还是要执行所有语句，然后再返回部分结果，可以配置hive属性，当使用limit时，对源数据进行抽样。1234&lt;property&gt; &lt;name&gt;hive.limit.optimize.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 其它，hive.limit.row.max.size hive.limit.optimize.limit.file两个参数 Join优化：将大表放在JOIN最右边，或者直接使用/ streamtable(table_name)/指定。如果一个表中很小，可以完整载入到内存中，此时Hive可以执行一个map-side JOIN.减少reduce的过程。 启动本地模式，设置属性hive.exec.model.local.auto=true启动本地模式。（有时候当Hive输入数据量很小时，为查询触发执行任务的时间消耗可能比实际执行job的时间还多）。如果要设置所有用户使用这个配置，可以在hive-site.xml文件中进行配置 1234&lt;property&gt; &lt;name&gt;hive.exec.model.local.auto&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 并行执行：Hive会将一个查询转化为一个或多个阶段。这样的阶段可以是Mapreduce阶段、抽样阶段、合并阶段、limit阶段，或者Hive执行阶段过程中可能需要的其他阶段。默认情况下，Hive每次只能执行一个阶段。但是有些阶段是可以并行执行，这样就可能使得整个Job的执行时间缩短。设置参数hive.exec.parallel,开启并发执行 1234&lt;property&gt; &lt;name&gt;hive.exec.parallel&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 严格模式：防止用户执行那些可能产生不好影响查询。可以设置hive.mapred.mode 值为strict 1set hive.mapred.mode=strict 可以禁止3种类型的查询 对于分区表，除非where语句中含有分区字段的过滤条件来限制数据范围，否则不允许执行，用户不允许扫描所有分区的数据。通常分区表都有非常大的数据集，且增长迅速 对于使用了order by 子句的查询，要求必须使用limit语句。因为order by子句为了执行排序过程会将所有数据分发到同一个reducer中处理，强制要求用户增加limit防止reducer额外执行很长一段时间 限制笛卡尔积的查询，两个表关联条件写在where中时，Hive并不会执行优化 调整mapper和reducer个数：Hive通过查询划分为一个或多个Mapreduce任务达到并行的目的。确定最佳的mapper个数和reducer个数取决于多个变量，数据量的大小，对数据执行的操作类型等。过多的mapper和reducer任务，就会导致启动阶段，调度和运行Job过程中产生过多的开销。而数量设置过少，就没有充分的利用好集群内在的并行性。hive.exec.reducers.bytes.per.reducer默认是1G，可以修改其大小。如果只是根据输入量的大小，可能导致reducer不准确，当mapper后数据量多，reducer不够用，mapper数据量小，reducer过多。我们可以设置参数限制reducer个数,hive默认的reducer个数是31mapred.reduce.tasks 当在集群上处理大任务时，为了控制资源的利用情况，属性hive.exec.reducers.max可以设置reducer的最大值。一个Hadoop集群可以提供的mapper和reducer资源个数（插槽）是固定的。某个大Job可能会消耗完所有的插槽，从而导致其它的Job无法执行，通过设置属性hive.exec.reducers.max可以阻止某个查询消耗太多的reducer资源。建议大小=（集群总Reducer槽位个数 * 1.5） JVM重用：适用于小文件场景和task特别多的场景。Hadoop默认的配置是使用派生的JVM来执行map和reduce任务。这是JVM的启动过程可能会造成很大的开销。尤其是执行的Job包含成百上千个task时，JVM重用可以使得JVM实例在同一个JOB中重用N次。N值可以在 hive-site.xml中配置。 mapred.job.reuse.jvm.num.tasks 10 此功能的缺点时，JVM会一直占用适用到的task插槽，以便进行重用，知道任务完成后才能释放。 索引：索引可以加快含有Group By语句的查询计算速度。 动态分区的调整：开启动态分区严格模式时，必须保证至少有一个分区是静态的，限制查询创建最大分区数 123456789101112131415--分区模式&lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;strict&lt;/value&gt;&lt;/property&gt;-- 最大分区数&lt;property&gt; &lt;name&gt;hive.exec.max.dynamic.partitions&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;-- datanode上可以一次打开的文件个数&lt;property&gt; &lt;name&gt;hive.exec.max.dynamic.partitions.pernode&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt;&lt;/property&gt; 虚拟列：Hive提供了两种虚拟列，一种用于将要进行划分的输入文件名，一种用于文件中的块内偏移量。当Hive产生了非预期或null的返回结果时，可以通过这些虚拟列查询 12set hive.exec.rowoffset=true;select input_file_name,block_offset_inside_file,line from hive_text where line like &apos;%hive%&apos; limit 2; 第三种虚拟列提供了文件的行偏移量。1234&lt;property&gt; &lt;name&gt;hive.exec.rowoffset&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 文件格式和压缩方法 查询hadoop编解码器 1hive -e &quot;set io.compression.codecs&quot; 使用压缩的优势：最小化磁盘所需要的空间。减少磁盘和网络的IO操作，不过文件的压缩和解压过程会增加CPU开销 开启中间数据的压缩：对中间数据进行压缩可以减少job中map和reduce task间的数据传输量。开启中间数据压缩的配置 1234&lt;property&gt; &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 输出压缩 12345678&lt;property&gt; &lt;name&gt;hive.exec.compress.output&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* &lt;/description&gt; &lt;/property&gt; sequence file存储格式压缩文件可以节约存储空间，但是通常这些文件是不能分割的。sequence file存储格式可以将一个文件划分为多个块，然后采用一种分割的方式对块进行压缩。如果要在hive中使用sequence file存储格式，在create table 中通过 stored as sequencefile 1create table sequence_file_tb stored as sequencefile; sequence file提供了三种压缩方式，None，RECORD，BLOCK，其中RECORD级别是默认的。BLOCK级别的压缩性能最好且是可以分割的。可以在hive-site.xml中定义1234&lt;property&gt; &lt;name&gt;mapred.output.compression.type&lt;/name&gt; &lt;value&gt;BLOCK&lt;/value&gt; &lt;/property&gt; 压缩实战 1234567891011121314hive (mydb)&gt; select * from user_info;--开启中间压缩hive (mydb)&gt;set hive.exec.compress.intermediate=truehive (mydb)&gt; select * from user_info;--时间变少hive (mydb)&gt;create table user_info_0417 as select * from user_info;--输出结果没有压缩hive (mydb)&gt; set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.GZipCodec;--设置压缩类型--开启输出结果压缩hive (mydb)&gt; set hive.exec.compress.output=true;--输出的文件已经被压缩create table user_info_041702 as select * from user_info;hive (mydb)&gt; set hive.exec.compress.output=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;create table user_info_041703 as select * from user_info; 存档分区：Hadoop中有一种存储格式为HAR，HAR文件内部可以有文件和文件夹。HAR文件可以减少NameNode消耗较大的代价来管理这些文件。Har访问效率低，Har文件没有被压缩，因此也不会节约存储空间。 123456789101112--创建分区hive (mydb)&gt; create table hive_text(line STRING) partitioned by (folder STRING);--增加分区hive (mydb)&gt; alter table hive_text add partition(folder=&apos;docs&apos;);--加载数据load data local inpath &apos;$&#123;env:HIVE_HOME&#125;/README.txt&apos; into table hive_text partition(folder=&apos;docs&apos;);hive (mydb)&gt; load data local inpath &apos;$&#123;env:HIVE_HOME&#125;/RELEASE_NOTES.txt&apos; into table hive_text partition(folder=&apos;docs&apos;);--将表转换为一个归档表set hive.archive.enabled=true;hive (mydb)&gt; alter table hive_text archive partition(folder=&apos;docs&apos;);--重新将har文件提取出来alter table hive_text unarchive partition(folder=&apos;docs&apos;) 开发 hive-log4j2.properties：CLI和其它本地执行组件的日志 hive-exec-log4j2.properties：控制Mapreduce task内日志 函数 show functions; 显示当前加载的函数 desc function year;查看函数的帮助文档 desc function extended year;：查看函数的详情 函数的分类 标准函数UDF 聚合函数UDAF 表生成函数UDTF,表生成函数接受零个或多个输入，产生多列或多行输出 array函数就是将一列输入转化为一个数组输出 1234hive (mydb)&gt; select array(1,2,3) from user_info limit 1;OKc0[1,2,3] explode:以array输入，然后对数组中的数据进行迭代，返回多行结果，一行一个数组元素值.explode无法产生其它的列 1234567hive (mydb)&gt; select explode(array(1,2,3)) from user_info limit 3;OKcol123Time taken: 0.423 seconds, Fetched: 3 row(s) 123select user_ids,sub from user_infolateral view explode(array(1,2,3)) subView as sublimit 3 创建一个自定义函数UDF 实现UDF类 打包成Jar文件 加载JAR文件：ADD JAR /home/yangql/zodiac.jar create temporary function zodiac as ‘org.com.yangql.KKKKK’如果用户需要频繁的使用自定义函数。需要将相关语句加入到 .hiverc文件中 删除UDF drop temporary function if exists zodiac; Hive文件及记录格式 从表中读取数据时，Hive会使用InputFormat，向表中写入数据时，使用OutputFormat 文本文件可以与其它的工具功效数据，但是存储空间大。二进制文件可以节约存储空间。也可以提高I/O性能。 sequence file:stored as sequencefile RCFILE:列式存储 SerDe序列化与反序列化 xpath访问xml文件12345678hive (mydb)&gt; select xpath(&apos;&lt;a&gt;&lt;b id=&quot;foo&quot;&gt;b1&lt;/b&gt;&lt;b id=&quot;bar&quot;&gt;b2&lt;/b&gt;&lt;/a&gt;&apos;,&apos;//@id&apos;) from src;OKc0[&quot;foo&quot;,&quot;bar&quot;]hive (mydb)&gt; select xpath_double(&apos;&lt;a&gt;&lt;b&gt;1&lt;/b&gt;&lt;c&gt;2&lt;/c&gt;&lt;/a&gt;&apos;,&apos;a/b+a/c&apos;) from src;OKc03.0 Thrift服务 Hive具有一个可选的组件叫做HiveServer或者HiveThrift，允许通过端口访问Hive，用于跨语言的服务无开发 启动thriftserver:hive --service hiveserver2 Hive中的权限 开启授权模式 123456789101112131415&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;enable or disable the Hive client authorization&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.createtable.owner.grants&lt;/name&gt; &lt;value&gt;all&lt;value/&gt; &lt;description&gt; The privileges automatically granted to the owner whenever a table gets created. An example like &quot;select,drop&quot; will grant select and drop privilege to the owner of the table. Note that the default gives the creator of a table no access to the table (but see HIVE-8067). &lt;/description&gt;&lt;/property&gt; 对用户授权，用户就是操作系统的用户 确认名字：set system:user.name 授权： 查看： show grant user yangql; 对组授权 对role授权","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark之SparkContext","slug":"Spark之SparkContext和Master20190524","date":"2017-04-12T07:54:59.000Z","updated":"2019-05-24T04:59:16.548Z","comments":true,"path":"2017/04/12/Spark之SparkContext和Master20190524/","link":"","permalink":"http://qioinglong.top/2017/04/12/Spark之SparkContext和Master20190524/","excerpt":"","text":"1.SparkContext2.Master","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark两种提交方式Yarn-client and Yarn-cluster","slug":"Spark两种提交方式Yarn-client and Yarn-cluster20190524","date":"2017-04-12T02:54:59.000Z","updated":"2019-05-24T04:59:15.857Z","comments":true,"path":"2017/04/12/Spark两种提交方式Yarn-client and Yarn-cluster20190524/","link":"","permalink":"http://qioinglong.top/2017/04/12/Spark两种提交方式Yarn-client and Yarn-cluster20190524/","excerpt":"Spark支持三种集群部署方式(Standalone,Mesos,Yarn),其中Master服务(Spark Standalone,Mesos Master,Yarn ResourceManager)决定哪些应用可以运行，在那个节点上运行，以及什么时候运行。Slave服务（Yarn NodeManager）运行在每个节点上，节点控制着Executor进程，同时监控作业的运行状态以及资源的消耗。Spark运行在Yarn上，有两种模式，Yarn-Client和Yarn-Cluster。通常情况下，Yarn-Cluster用于生产环境，Yarn-Client用于交互、调试。","text":"Spark支持三种集群部署方式(Standalone,Mesos,Yarn),其中Master服务(Spark Standalone,Mesos Master,Yarn ResourceManager)决定哪些应用可以运行，在那个节点上运行，以及什么时候运行。Slave服务（Yarn NodeManager）运行在每个节点上，节点控制着Executor进程，同时监控作业的运行状态以及资源的消耗。Spark运行在Yarn上，有两种模式，Yarn-Client和Yarn-Cluster。通常情况下，Yarn-Cluster用于生产环境，Yarn-Client用于交互、调试。 1.Appliaction Master在Yarn中，每个application都有一个Application Master进程，它是Appliaction启动的第一个容器，它负责从ResourceManager中申请资源，分配资源，同时通知NodeManager来为Application启动container，Application Master避免了需要一个活动的client来维持，启动Applicatin的client可以随时退出，而由Yarn管理的进程继续在集群中运行。当在Yarn下运行Spark作业时，每个Spark Executor作为一个Yarn 容器(container)在运行，同时支持多个任务在同一个容器中运行，节省了任务的启动时间。 2.Yarn-client在Yarn-client模式下，AM仅仅从Yarn中申请资源分配给Executor，之后client会跟容器(Container)通信进行作业调度。Client不能离开.如下图所示：执行流程： 客户端提交作业给ResourceManager(RM) RM在本地NodeManager(NM)启动container并将AM分配给该NM NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向本地启动的AM注册汇报并完成相应的任务3.Yarn-Cluster在Yarn-cluster模式下，driver运行在AM上，AM进程同时负责驱动Application和从Yarn中申请资源，该进程运行在Yarn container内，所以启动AM的client可以立即关闭而不必持续到Application的生命周期，如下图所示：执行流程： 客户端生成作业信息提交给ResourceManager(RM) RM在某一个NodeManager(由Yarn决定)启动container并将Application Master(AM)分配给该NodeManager(NM) NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向NM上的Application Master注册汇报并完成相应的任务","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark宽依赖与窄依赖","slug":"Spark宽依赖与窄依赖20190524","date":"2017-04-11T14:25:59.000Z","updated":"2019-05-24T04:59:15.816Z","comments":true,"path":"2017/04/11/Spark宽依赖与窄依赖20190524/","link":"","permalink":"http://qioinglong.top/2017/04/11/Spark宽依赖与窄依赖20190524/","excerpt":"Spark中RDD的高效与DAG（有向无环图）有很大的关系，在DAG调度中需要对计算的过程划分Stage，划分的依据就是RDD之间的依赖关系。RDD之间的依赖关系分为两种，宽依赖(wide dependency/shuffle dependency)和窄依赖（narrow dependency）","text":"Spark中RDD的高效与DAG（有向无环图）有很大的关系，在DAG调度中需要对计算的过程划分Stage，划分的依据就是RDD之间的依赖关系。RDD之间的依赖关系分为两种，宽依赖(wide dependency/shuffle dependency)和窄依赖（narrow dependency） 1.窄依赖窄依赖就是指父RDD的每个分区只被一个子RDD分区使用，子RDD分区通常只对应常数个父RDD分区，如下图所示【其中每个小方块代表一个RDD Partition】 窄依赖有分为两种： 一种是一对一的依赖，即OneToOneDependency 还有一个是范围的依赖，即RangeDependency，它仅仅被org.apache.spark.rdd.UnionRDD使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同 2.宽依赖宽依赖就是指父RDD的每个分区都有可能被多个子RDD分区使用，子RDD分区通常对应父RDD所有分区，如下图所示【其中每个小方块代表一个RDD Partition】 3.窄依赖与窄依赖比较 宽依赖往往对应着shuffle操作，需要在运行的过程中将同一个RDD分区传入到不同的RDD分区中，中间可能涉及到多个节点之间数据的传输，而窄依赖的每个父RDD分区通常只会传入到另一个子RDD分区，通常在一个节点内完成。 当RDD分区丢失时，对于窄依赖来说，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重新计算与子RDD分区对应的父RDD分区就行。这个计算对数据的利用是100%的 当RDD分区丢失时，对于宽依赖来说，重算的父RDD分区只有一部分数据是对应丢失的子RDD分区的，另一部分就造成了多余的计算。宽依赖中的子RDD分区通常来自多个父RDD分区，极端情况下，所有父RDD都有可能重新计算。如下图，par4丢失，则需要重新计算par1,par2,par3,产生了冗余数据par5 4.宽依赖，窄依赖函数 窄依赖的函数有：map, filter, union, join(父RDD是hash-partitioned ), mapPartitions, mapValues 宽依赖的函数有：groupByKey, join(父RDD不是hash-partitioned ), partitionBy","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark 工作原理及RDD","slug":"Spark 工作原理及RDD20190524","date":"2017-04-10T00:54:59.000Z","updated":"2019-05-24T04:59:14.914Z","comments":true,"path":"2017/04/10/Spark 工作原理及RDD20190524/","link":"","permalink":"http://qioinglong.top/2017/04/10/Spark 工作原理及RDD20190524/","excerpt":"Spark是一种开源的分布式并行计算框架，Spark拥有Hadoop Mapreduce计算框架的优点。但是与Hadoop Mapreduce最大的不同就是：Hadoop Mapreduce分为两个阶段，map 和 reduce，两个阶段完了，就完了，在一个作业里能做的事情很少。而Spark是基于内存迭代计算的，可以分为N个阶段，一个阶段完了可以继续下一阶段的处理，而且Spark作业的中间结果可以保存到内存中，不用再频繁去HDFS或其它数据源读取数据。","text":"Spark是一种开源的分布式并行计算框架，Spark拥有Hadoop Mapreduce计算框架的优点。但是与Hadoop Mapreduce最大的不同就是：Hadoop Mapreduce分为两个阶段，map 和 reduce，两个阶段完了，就完了，在一个作业里能做的事情很少。而Spark是基于内存迭代计算的，可以分为N个阶段，一个阶段完了可以继续下一阶段的处理，而且Spark作业的中间结果可以保存到内存中，不用再频繁去HDFS或其它数据源读取数据。 1. Spark术语Spark框架图如下： Cluster Manager：资源管理，在集群上获取资源的外部服务，目前主要有三种，Spark原生的资源管理Standalone,mesos,hadoop yarn Application：用户编写的应用程序 Driver：应用程序中运行的main函数并创建SparkContext。创建的SparkContext负责与Cluster Manager通信，进行资源的申请，任务的分配与监控，SparkContext代表Driver Worker：集群中可以运行应用程序的节点 Executor：应用程序在Worker的进程，负责执行task Task:被Executor执行的工作单元，是运行Application最小的单位，多个task组合成一个stage，Task的调度和管理由TaskScheduler负责 Job：包含多个Task组成的并行计算 Stage：Stage：每个Job的Task被拆分成很多组Task, 作为一个TaskSet，命名为Stage。Stage的调度和划分由DAGScheduler负责。Stage又分为Shuffle Map Stage和Result Stage两种。Stage的边界就在发生Shuffle的地方。 RDD：Spark的基本数据操作抽象，可以通过一系列算子进行操作。RDD是Spark最核心的东西，可以被分区、被序列化、不可变、有容错机制，并且能并行操作的数据集合。存储级别可以是内存，也可以是磁盘。 DAGScheduler：根据Job构建基于Stage的DAG（有向无环任务图），并提交Stage给TaskScheduler TaskScheduler：将Stage提交给Worker（集群）运行，每个Executor运行什么在此分配。 共享变量：Application在整个运行过程中，可能需要一些变量在每个Task中都使用，共享变量用于实现该目的。Spark有两种共享变量：一种缓存到各个节点的广播变量；一种只支持加法操作，实现求和的累加变量。 宽依赖：或称为ShuffleDependency, 宽依赖需要计算好所有父RDD对应分区的数据，然后在节点之间进行Shuffle。 窄依赖：或称为NarrowDependency，指某个RDD，其分区partition x最多被其子RDD的一个分区partion y依赖。窄依赖都是Map任务，不需要发生shuffle。因此，窄依赖的Task一般都会被合成在一起，构成一个Stage。2.Spark工作流程图 spark-submit 提交了应用程序的时候，提交spark应用的机器会通过反射的方式，创建和构造一个Driver进程，Driver进程执行Application程序， Driver根据sparkConf中的配置初始化SparkContext,在SparkContext初始化的过程中会启动DAGScheduler和taskScheduler taskSheduler通过后台进程，向Master注册Application，Master接到了Application的注册请求之后，会使用自己的资源调度算法，在spark集群的worker上，通知worker为application启动多个Executor。 Executor会向taskScheduler反向注册。 Driver完成SparkContext初始化 application程序执行到Action时，就会创建Job。并且由DAGScheduler将Job划分多个Stage,每个Stage 由TaskSet 组成 DAGScheduler将TaskSet提交给taskScheduler taskScheduler把TaskSet中的task依次提交给Executor Executor在接收到task之后，会使用taskRunner来封装task（TaskRuner主要将我们编写程序，也就是我们编写的算子和函数进行拷贝和反序列化）,然后，从Executor的线程池中取出一个线程来执行task。就这样Spark的每个Stage被作为TaskSet提交给Executor执行，每个Task对应一个RDD的partition,执行我们的定义的算子和函数。直到所有操作执行完为止。 3.RDD RDD是Spark提供的核心抽象，全称为Resillient Distributed Dataset，即弹性分布式数据集 RDD在抽象上来说是一种元素集合，包含了数据。它是被分区的，分为多个分区，每个分区分布在集群中的不同节点上，从而让RDD中的数据可以被并行操作。 RDD通常通过Hadoop上的文件，即HDFS文件或者Hive表，来进行创建；有时也可以通过应用程序中的集合来创建。 RDD最重要的特性就是，提供了容错性，可以自动从节点失败中恢复过来。即如果某个节点上的RDD partition，因为节点故障，导致数据丢了，那么RDD会自动通过自己的数据来源重新计算该partition。这一切对使用者是透明 RDD的数据默认情况下存放在内存中的，但是在内存资源不足时，Spark会自动将RDD数据写入磁盘","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark性能并行度","slug":"Spark性能并行度20190524","date":"2017-04-06T00:54:59.000Z","updated":"2019-05-24T04:59:16.385Z","comments":true,"path":"2017/04/06/Spark性能并行度20190524/","link":"","permalink":"http://qioinglong.top/2017/04/06/Spark性能并行度20190524/","excerpt":"1. Spark的并行度spark作业中，各个stage的task的数量，代表了spark作业在各个阶段stage的并行度！当分配完所能分配的最大资源了，然后对应资源去调节程序的并行度，如果并行度没有与资源相匹配，那么导致你分配下去的资源都浪费掉了。同时并行运行，还可以让每个task要处理的数量变少。合理设置并行度，可以充分利用集群资源，减少个task处理数据量，而增加性能加快运行速度。","text":"1. Spark的并行度spark作业中，各个stage的task的数量，代表了spark作业在各个阶段stage的并行度！当分配完所能分配的最大资源了，然后对应资源去调节程序的并行度，如果并行度没有与资源相匹配，那么导致你分配下去的资源都浪费掉了。同时并行运行，还可以让每个task要处理的数量变少。合理设置并行度，可以充分利用集群资源，减少个task处理数据量，而增加性能加快运行速度。 2. 如何去提高并行度2.1 设置task数量将task数量设置成与Application总cpu core 数量相同（理想情况，150个core，分配150 task）官方推荐，task数量，设置成Application总cpu core数量的2~3倍（150个cpu core，设置task数量为300~500）与理想情况不同的是：有些task会运行快一点，比如50s就完了，有些task可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core数量相同，也可能会导致资源的浪费，比如150 task，10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。 2.2 设置Application的并行度参数spark.defalut.parallelism默认是没有值的，如果设置了值，是在shuffle的过程才会起作用12new SparkConf().set(&quot;spark.defalut.parallelism&quot;,&quot;10&quot;)val rdd2 = rdd1.reduceByKey(_+_) //rdd2的分区数就是10，rdd1的分区数不受这个参数的影响 2.3 增加block读取的数据在HDFS上时，增加block数，默认情况下split与block是一对一的，而split又与RDD中的partition对应，所以增加了block数，也就提高了并行度。 2.4 重新设置 partition1RDD.repartition 给RDD重新设置partition的数量 2.5 指定partition12val rdd2 = rdd1.reduceByKey(_+_,10) val rdd3 = rdd2.map.filter.reduceByKey(_+_) reduceByKey的算子指定partition的数量 2.6 增加父RDD数量1val rdd3 = rdd1.join（rdd2） rdd3里面partiiton的数量是由父RDD中最多的partition数量来决定，因此使用join算子的时候，增加父RDD中partition的数量 2.7 shuffle过程中partitions的数量1spark.sql.shuffle.partitions //spark sql中shuffle过程中partitions的数量 3. 例子现在已经在spark-submit 脚本里面，给我们的spark作业分配了足够多的资源，比如50个executor ，每个executor 有10G内存，每个executor有3个cpu core 。 基本已经达到了集群或者yarn队列的资源上限。task没有设置，或者设置的很少，比如就设置了，100个task 。 50个executor ，每个executor 有3个core ，也就是说Application 任何一个stage运行的时候，都有总数150个cpu core ，可以并行运行。但是，你现在只有100个task ，平均分配一下，每个executor 分配到2个task，ok，那么同时在运行的task，只有100个task，每个executor 只会并行运行 2个task。 每个executor 剩下的一个cpu core 就浪费掉了！你的资源，虽然分配充足了，但是问题是， 并行度没有与资源相匹配，导致你分配下去的资源都浪费掉了。合理的并行度的设置，应该要设置的足够大，大到可以完全合理的利用你的集群资源； 比如上面的例子，总共集群有150个cpu core ，可以并行运行150个task。那么你就应该将你的Application 的并行度，至少设置成150个，才能完全有效的利用你的集群资源，让150个task ，并行执行，而且task增加到150个以后，即可以同时并行运行，还可以让每个task要处理的数量变少； 比如总共 150G 的数据要处理， 如果是100个task ，每个task 要计算1.5G的数据。 现在增加到150个task，每个task只要处理1G数据。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark性能调优之资源","slug":"Spark性能调优之资源20190524","date":"2017-04-05T00:54:59.000Z","updated":"2019-05-24T04:59:16.454Z","comments":true,"path":"2017/04/05/Spark性能调优之资源20190524/","link":"","permalink":"http://qioinglong.top/2017/04/05/Spark性能调优之资源20190524/","excerpt":"性能调优的第一要点是分配和增加更多的资源，在一定范围内，资源的增加和性能的提升，是成正比的。所以在写完一个Spark作业后，最优的资源配置是性能调优的第一步，在此基础上，当你能分配的资源达到最大，无法再分配更多的资源时，才去考虑其它性能调优的点，如共享变量，kyro序列化等。我们从需要分配那些资源、在哪里分配资源以及分配资源后性能为何会得到提升三个方面来讨论这个主题。","text":"性能调优的第一要点是分配和增加更多的资源，在一定范围内，资源的增加和性能的提升，是成正比的。所以在写完一个Spark作业后，最优的资源配置是性能调优的第一步，在此基础上，当你能分配的资源达到最大，无法再分配更多的资源时，才去考虑其它性能调优的点，如共享变量，kyro序列化等。我们从需要分配那些资源、在哪里分配资源以及分配资源后性能为何会得到提升三个方面来讨论这个主题。 1. 需要分配的资源通常我们在完成一个Spark作业后，使用spark-submit命令提交作业，在spark-submit命令中，有一系列的参数可供我们配置，用于分配资源。通常我们提交作业的脚本如下：1234567./bin/spark-submit \\--class con.spark.core.WordCountCluster \\--num-executors 3 \\ 配置executor的数量--driver-memory 1g \\ 配置driver的内存（影响不大）--executor-memory 1g \\ 配置每个executor的内存大小--executor-cores 3 \\ 配置每个executor的cpu core数量/usr/local/SparkTest.jar \\ 从以上的命令我们可以看出，我们需要分配的资源有如下几个： executor cpu per executor memory per executor driver memory 2. 如何分配才能达到最佳对于Spark Standalone来说：比如你有20台机器，4G内存，2个CPU core。如果你有20个executor，那么每个executor能使用的内存为2G，每个executor能使用2个CPU core。对于Yarn资源队列。我们需要去查看我们要提交到的资源队列，还有多少资源可供我们使用。如资源队列有500G内存，100个CPU core。那如果我们有50个executor，那每个executor能使用的内存就只有10G，2个CPU资源分配总的原则就是：能使用的资源有多大，就尽量去调节到最大的大小。 3.资源为什么会影响性能在Driver上提交一个作业后，SparkContext，DAGScheduler，TaskScheduler会将我们的算子，切割成大量的task，提交到worker上的executor上面去执行。 增加executor如果executor数量比较少，那么，能够并行执行的task数量就比较少，就意味着，我们的Application的并行执行的能力就很弱。比如有3个executor，每个executor有2个cpu core，那么同时能够并行执行的task，就是6个。6个执行完以后，再换下一批6个task。增加了executor数量以后，那么，就意味着，能够并行执行的task数量，也就变多了。比如原先是6个，现在可能可以并行执行10个，甚至20个，100个。那么并行能力就比之前提升了数倍，数十倍。相应的，性能（执行的速度），也能提升数倍~数十倍。 增加每个executor的cpu core增加每个executor的cpu core就是增加了执行的并行能力。原本20个executor，每个才2个cpu core。能够并行执行的task数量，就是40个task。加到了5个。能够并行执行的task数量，就是100个task。执行的速度，提升了2.5倍。 增加每个executor的内存量第一：如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO。第二：对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。第三：对于task的执行，可能会创建很多对象。如果内存比较小，就会导致JVM频繁的进行垃圾回收(GC),导致性能变慢。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark实例-操作KafKa数据","slug":"Spark实例-操作KafKa数据20190524","date":"2017-04-03T06:54:59.000Z","updated":"2019-05-24T04:59:16.100Z","comments":true,"path":"2017/04/03/Spark实例-操作KafKa数据20190524/","link":"","permalink":"http://qioinglong.top/2017/04/03/Spark实例-操作KafKa数据20190524/","excerpt":"Spark操作kafka数据，有两种连接方式，直连Direct和Receiver方式","text":"Spark操作kafka数据，有两种连接方式，直连Direct和Receiver方式 1.Direct 方式12345678910111213141516171819202122232425262728293031323334353637package com.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.Secondsimport org.apache.spark.streaming.kafka.KafkaUtilsimport kafka.serializer.StringDecoder/**bin/kafka-topics.sh --create --zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 --replication-factor 1 --partitions 1 --topic wordcountbin/kafka-console-producer.sh --broker-list 192.168.1.231:9092,192.168.1.232:9092,192.168.1.233:9092 --topic wordcount */object KafkaDirect extends App&#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;KafkaDirect&quot;) val ssc = new StreamingContext(conf,Seconds(10)) //创建一份kafka的参数 val kafkaParams= Map[String,String](&quot;metadata.broker.list&quot;-&gt;&quot;hadoop01:9092,hadoop02:9092,hadoop03:9092&quot;) //创建一个Set，里面放要读取的topic val topics = Set[String](&quot;wordcount&quot;) val lineMap = KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc, kafkaParams, topics) val lines = lineMap.map(_._2) val words = lines.flatMap(_.split(&quot; &quot;)) val paris =words.map(word=&gt;(word,1)) val wordCounts = paris.reduceByKey(_+_) wordCounts.print ssc.start() ssc.awaitTermination() ssc.stop()&#125; 2.Receiver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.spark.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.SparkContextimport org.apache.spark.streaming.Minutes/** * 通过Kafka Receiver方法hi实现 * 启动zooKeeper：sh zkServer.sh start * 启动Kafka： * bin/kafka-server-start.sh config/server.properties * 创建 topicbin/kafka-topics.sh --create --zookeeper hadoop01:2181 --replication-factor 1 --partitions 1 --topic test 查看：bin/kafka-topics.sh --list --zookeeper hadoop01:2181 发送消息bin/kafka-console-producer.sh --broker-list 192.168.1.231:9092 --topic test 接受消息 bin/kafka-console-consumer.sh --bootstrap-server hadoop01:9092 --topic test --from-beginning */object KafkaReciever extends App &#123; val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;KafkaDirect&quot;) val sc = new SparkContext(conf) val ssc = new StreamingContext(sc,Seconds(2)) val zkQuorum = &quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot; val group = &quot;test-consumer-group&quot; val topics = &quot;test&quot; val numThreads = 1 val topicMap = topics.split(&quot;,&quot;).map((_,numThreads.toInt)).toMap val lineMap = KafkaUtils.createStream(ssc,zkQuorum,group,topicMap) val lines = lineMap.map(_._2) val words = lines.flatMap(_.split(&quot; &quot;)) val pair = words.map(x =&gt; (x,1)) val wordCounts = pair.reduceByKey(_+_) wordCounts.print ssc.checkpoint(&quot;E:/words/checkpoint&quot;) ssc.start ssc.awaitTermination&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Task未序列化(Task not serializable)问题分析及解决","slug":"Spark-Task未序列化(Task not serializable)问题分析及解决20190524","date":"2017-04-03T01:54:59.000Z","updated":"2019-05-24T04:59:15.341Z","comments":true,"path":"2017/04/03/Spark-Task未序列化(Task not serializable)问题分析及解决20190524/","link":"","permalink":"http://qioinglong.top/2017/04/03/Spark-Task未序列化(Task not serializable)问题分析及解决20190524/","excerpt":"在Spark程序中，在map等算子内部使用了外部定义的变量和函数，从而引发Task未序列化问题。其中最普遍的是当引用了某个类的成员函数或变量时，会导致这个类的所有成员（整个类）都需要支持序列化。虽然许多情形下，类使用了extends Serializable声明支持序列化，但是由于某些字段不支持序列化，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。出现如下错误如下：","text":"在Spark程序中，在map等算子内部使用了外部定义的变量和函数，从而引发Task未序列化问题。其中最普遍的是当引用了某个类的成员函数或变量时，会导致这个类的所有成员（整个类）都需要支持序列化。虽然许多情形下，类使用了extends Serializable声明支持序列化，但是由于某些字段不支持序列化，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。出现如下错误如下：123456789101112131415161718192021222324Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298) at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288) at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108) at org.apache.spark.SparkContext.clean(SparkContext.scala:2094) at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:379) at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:378) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) at org.apache.spark.rdd.RDD.flatMap(RDD.scala:378) at com.project.session.UserVisitAnalySpark$.randomExtractSession(UserVisitAnalySpark.scala:591) at com.project.session.UserVisitAnalySpark$.main(UserVisitAnalySpark.scala:105) at com.project.session.UserVisitAnalySpark.main(UserVisitAnalySpark.scala)Caused by: java.io.NotSerializableException: com.project.dao.impl.SessionRandomExtractDAOImplSerialization stack: - object not serializable (class: com.project.dao.impl.SessionRandomExtractDAOImpl, value: com.project.dao.impl.SessionRandomExtractDAOImpl@76410abc) - field (class: com.project.session.UserVisitAnalySpark$$anonfun$27, name: sessionRandomExtractDAO$1, type: interface com.project.dao.SessionRandomExtractDAO) - object (class com.project.session.UserVisitAnalySpark$$anonfun$27, &lt;function1&gt;) at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40) at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46) at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100) at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295) ... 12 more 此博客进行了原因分析以及解决办法：http://blog.csdn.net/sogerno1/article/details/45935159","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark琐碎记录","slug":"Spark琐碎记录20190524","date":"2017-04-03T00:54:59.000Z","updated":"2019-05-24T04:59:16.323Z","comments":true,"path":"2017/04/03/Spark琐碎记录20190524/","link":"","permalink":"http://qioinglong.top/2017/04/03/Spark琐碎记录20190524/","excerpt":"本文主要针对Spark一些特性记录，随时保持更新","text":"本文主要针对Spark一些特性记录，随时保持更新 写sc.textFile(file,num).cache()时，最好默认不设置num数，程序会根据系统环境自动去配置。还有cache缓存，如果RDD在后面只调用一次，可以不用添加缓存。（内存使用过多可能造成内存溢出） 提交spark任务时，spark-submit —master yarn-cluster—driver-memory 5g —executor-memory 2g —num-executors 20 。除了设置资源大小，最好控制executors个数，防止程序无止境消耗内存资源量，影响其他调度正常运行","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark二次排序","slug":"Spark二次排序20190524","date":"2017-04-03T00:54:59.000Z","updated":"2019-05-24T04:59:15.599Z","comments":true,"path":"2017/04/03/Spark二次排序20190524/","link":"","permalink":"http://qioinglong.top/2017/04/03/Spark二次排序20190524/","excerpt":"Spark sortByKey只针对单个字段，如果要对多个字段进行排序，就只能自己单独实现一个排序类。关键点： 继承Ordered[T]接口 继承Serializable 实现compare方法 使用时需要实例化定义的排序类实现三个字段A,B,C排序，先按照A排序，如果A相同，再以B排序，B相同，以C排序。代码如下：","text":"Spark sortByKey只针对单个字段，如果要对多个字段进行排序，就只能自己单独实现一个排序类。关键点： 继承Ordered[T]接口 继承Serializable 实现compare方法 使用时需要实例化定义的排序类实现三个字段A,B,C排序，先按照A排序，如果A相同，再以B排序，B相同，以C排序。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.spark.coreimport org.apache.spark.sql.SparkSession/** * 二次排序 */class SecondSortKey(val first:Int,val second:Int,val third:Int) extends Ordered[SecondSortKey] with Serializable&#123; def compare(that:SecondSortKey):Int=&#123; if(this.first-that.first!=0)&#123; this.first-that.first &#125;else if(this.second-that.second!=0)&#123; this.second - that.second &#125;else&#123; this.third-that.third &#125; &#125;&#125;object SecondSortKey&#123; def main(args: Array[String]): Unit = &#123; val spark=SparkSession .builder().appName(&quot;SecondSortKey&quot;) .master(&quot;local&quot;) .getOrCreate() import spark.implicits._ //读取文件，生成RDD val lines=spark.sparkContext.textFile(&quot;E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\sort.txt&quot;) lines.foreach(println) /** 1 1 3 1 1 2 3 6 1 1 3 4 2 1 6 */ //&lt;SecondSortKey,line&gt; val paris=lines.map(line=&gt;( new SecondSortKey(line.split(&quot; &quot;)(0).toInt, line.split(&quot; &quot;)(1).toInt, line.split(&quot; &quot;)(2).toInt), line )) //排序 val sortedParis=paris.sortByKey() //将排序后的行映射回来 val sortedLines=sortedParis.map(paris=&gt;(paris._2)) //打印 sortedLines.foreach(println) //output /** * 1 1 2 1 1 3 1 3 4 2 1 6 3 6 1 */ &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"SparkSQL之DataSet","slug":"SparkSQL之DataSet20190524","date":"2017-04-01T06:54:59.000Z","updated":"2019-05-24T04:59:15.235Z","comments":true,"path":"2017/04/01/SparkSQL之DataSet20190524/","link":"","permalink":"http://qioinglong.top/2017/04/01/SparkSQL之DataSet20190524/","excerpt":"DataSet介绍DataSet是一个分布式数据集，从Spark 1.6开始引入，它集合了RDD API中的很多优点（强类型，lambda表达式），以及SparkSQL的优点（优化后的执行引擎）。DataSet可以通过JVM object来创建，然后通过transformation类的算子来进行操作。从Spark2.0后，SparkSQL的统一入口修改为SparkSession，SQLContext和HiveContext未来会被放弃掉。通过以下代码来创建SparkSession","text":"DataSet介绍DataSet是一个分布式数据集，从Spark 1.6开始引入，它集合了RDD API中的很多优点（强类型，lambda表达式），以及SparkSQL的优点（优化后的执行引擎）。DataSet可以通过JVM object来创建，然后通过transformation类的算子来进行操作。从Spark2.0后，SparkSQL的统一入口修改为SparkSession，SQLContext和HiveContext未来会被放弃掉。通过以下代码来创建SparkSession123456789101112val spark = SparkSession .builder()//用到了Java里面的构造器设计模式 .appName(&quot;SparkSQLDemo&quot;) .master(&quot;local&quot;) //spark2.0必须设置spark.sql.warehouse.dir，需要设置spark.sql的元数据仓库的目录 //默认已经启用 .config(&quot;spark.sql.warehouse.dir&quot;, &quot;E:\\\\worksplace\\\\spark\\\\spark-warehouse&quot;) //启用hive支持 .enableHiveSupport() .getOrCreate() //导入隐式转换 import spark.implicits._ 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.spark.datasetimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.Row/** * SparkSession,DataFrame,Dataset */object SparkSQLDemo&#123; //构造sparkSession case class Person(name:String,age:BigInt) case class Record(key:Int,value:String) def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder()//用到了Java里面的构造器设计模式 .appName(&quot;SparkSQLDemo&quot;) .master(&quot;local&quot;) //spark2.0必须设置spark.sql.warehouse.dir，需要设置spark.sql的元数据仓库的目录 //默认已经启用 .config(&quot;spark.sql.warehouse.dir&quot;, &quot;E:\\\\worksplace\\\\spark\\\\spark-warehouse&quot;) //启用hive支持 .enableHiveSupport() .getOrCreate() //导入隐式转换 import spark.implicits._ //读取json文件，构造一个untype弱类型的DataFrame val df = spark.read.json(&quot;E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot;) df.show()//println打印数据 df.printSchema()//打印元数据 df.select(&quot;name&quot;).show()//select操作，典型的弱类型，untype操作 df.select($&quot;name&quot;, $&quot;age&quot;+1).show() df.filter($&quot;age&quot;&gt;19).show()//filter 过滤满足条件的数据 df.groupBy(&quot;age&quot;).count().show()//group by 分组，再聚合 //将df创建为临时视图，并使用sql查询 df.createOrReplaceTempView(&quot;people&quot;) val sqlDf=spark.sql(&quot;select * from people&quot;) sqlDf.show() //基于jvm object来创建dataset //首先构造一个case class val caseClassDS=Seq(Person(&quot;yangql&quot;,18)).toDS() caseClassDS.show() //基于原始数据类型构造dataset val primitiveDS=Seq(0,1,2,3).toDS() primitiveDS.map(_+1).show() //基于已有的数据化文件，构造dataset //spark.read.json首先获得的是DataFrame，其次使用as[Person]之后，将一个DataFrame转换为dataset val peopleDS=spark.read.json(&quot;E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot;).as[Person] peopleDS.show() //hive println(&quot;--=============hive=====================--&quot;) //创建Hive表 spark.sql(&quot;create table if not exists src(key INT,value STRING)&quot;) spark.sql(&quot;LOAD DATA LOCAL INPATH E:\\\\worksplace\\\\spark\\\\src\\\\main\\\\resources\\\\people.json INTO TABLE SRC&quot;) spark.sql(&quot;select * from src&quot;).show() spark.sql(&quot;select count(*) from src&quot;).show() val hiveDF=spark.sql(&quot;select key,value from src where key&lt;10 order by key&quot;) val hiveDS=sqlDf.map&#123; case Row(key:Int,value:String)=&gt;s&quot;key:$key,value:$value&quot; &#125; hiveDS.show() val recordDF=spark.createDataFrame((1 to 10).map(value=&gt;&#123; Record(value,s&quot;val_$value&quot;) &#125;)) recordDF.show() &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"SparkSQL之根据不同数据来源创建DataFrame","slug":"SparkSQL之根据不同数据来源创建DataFrame20190524","date":"2017-04-01T00:54:59.000Z","updated":"2019-05-24T04:59:15.290Z","comments":true,"path":"2017/04/01/SparkSQL之根据不同数据来源创建DataFrame20190524/","link":"","permalink":"http://qioinglong.top/2017/04/01/SparkSQL之根据不同数据来源创建DataFrame20190524/","excerpt":"1. 介绍本文主要介绍根据不同数据来源创建DataFrame，主要有JDBC数据源，Hive数据源，JSON数据源，Parquet数据源。","text":"1. 介绍本文主要介绍根据不同数据来源创建DataFrame，主要有JDBC数据源，Hive数据源，JSON数据源，Parquet数据源。 2. JDBC数据源创建DataFrame关键点： 连接数据库 使用SQLContext.read.format(“jdbc”) options 参数配置 123456options(Map( &quot;url&quot;-&gt;url, &quot;dbtable&quot;-&gt;&quot;student_score&quot;, &quot;user&quot;-&gt;userName, &quot;password&quot;-&gt;password )) 将DataFrame数据保存到关系数据库中 1studentStructDF.write.saveAsTable(&quot;good_student&quot;) 实例代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.spark.sqlimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. */object JDBCDataSrc extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JDBCDataSrc&quot;) val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) import sqlContext.implicits._ val url=&quot;jdbc:mysql://localhost:3306/sparkpro&quot; val userName=&quot;root&quot; val password=&quot;root&quot; //创建Dataframe val studenInfoDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map( &quot;url&quot;-&gt;url, &quot;dbtable&quot;-&gt;&quot;student_info&quot;, &quot;user&quot;-&gt;userName, &quot;password&quot;-&gt;password )).load() studenInfoDF.show() //创建Dataframe val studentScoreDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map( &quot;url&quot;-&gt;url, &quot;dbtable&quot;-&gt;&quot;student_score&quot;, &quot;user&quot;-&gt;userName, &quot;password&quot;-&gt;password )).load() //studentScoreDF 转换为RDD，并过滤出分数大于80分的学生 val goodStudentRDD=studentScoreDF.rdd.filter(row=&gt;(row.getAs[Int](&quot;score&quot;)&gt;=80)) // for (elem &lt;- goodStudentRDD.collect()) &#123; // print(elem) // &#125; //a RDD for studenfInfo val studenInfoRDD=studenInfoDF.rdd.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;age&quot;))) .join(goodStudentRDD.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;score&quot;)))) val studenInfoRowRDD=studenInfoRDD.map(row=&gt;Row(row._1,row._2._1.toString.toLong,row._2._2.toString.toLong)) //studenInfoRDD.foreach(println) //将RDD转化为DataFrame val studentStruct=StructType(Array( StructField(&quot;name&quot;,StringType,true), StructField(&quot;age&quot;,LongType,true), StructField(&quot;score&quot;,LongType,true) )) val studentStructDF=sqlContext.createDataFrame(studenInfoRowRDD,studentStruct) studentStructDF.write.saveAsTable(&quot;good_student&quot;) //将数据插入到数据库中&#125; 3. Hive 数据源创建DataFrame关键点： 创建HiveContext hiveContext.sql() 方法创建表，删除表，加载数据，查询SQL返回DataFrame 将数据保存到Hive中 1goodStudentsDF.write.saveAsTable(&quot;GOOD_STUDENT&quot;) 示例代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.spark.sqlimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. */object HiveDataSource extends App&#123; val conf = new SparkConf() .setAppName(&quot;HiveDataSource&quot;) val sc = new SparkContext(conf) val hiveContext = new HiveContext(sc) //创建student_infos表 hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_INFO&quot;) //判断表是否存在，不存在则创建 hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_INFO(NAME STRING,AGE INT)&quot;) //导入数据 hiveContext.sql(&quot;LOAD DATA &quot; + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_info.txt&apos; &quot; + &quot;INTO TABLE STUDENT_INFO&quot;) //创建student_score表 hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_SCORE&quot;) //判断表是否存在，不存在则创建 hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_SCORE (NAME STRING,SCORE INT)&quot;) //导入数据 hiveContext.sql(&quot;LOAD DATA &quot; + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_score.txt&apos; &quot; + &quot;INTO TABLE STUDENT_SCORE&quot;) //查询分数大于80的学生信息,并保存到good_student信息 val goodStudentsDF=hiveContext.sql(&quot;SELECT T1.NAME,T1.AGE,T2.SCORE &quot; + &quot;FROM student_info T1 &quot; + &quot;INNER JOIN student_SCORE T2 &quot; + &quot;ON T1.NAME=T2.NAME &quot; + &quot;WHERE T2.SCORE&gt;80&quot;) hiveContext.sql(&quot;DROP TABLE IF EXISTS GOOD_STUDENT&quot;) //goodStudentsDF.saveAsTable(&quot;GOOD_STUDENT&quot;) goodStudentsDF.write.saveAsTable(&quot;GOOD_STUDENT&quot;) //查询打印 val goodStudentsRows=hiveContext.table(&quot;GOOD_STUDENT&quot;).collect() for(goodStudentsRow &lt;- goodStudentsRows)&#123; println(goodStudentsRow) &#125;&#125; 4. JSON数据源创建DataFrame关键点： sqlContext.read.json()方法读取Json文件，创建DataFrame 示例代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.spark.sqlimport org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.types._import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. * 从JSON数据源中读取数据 */object JsonDataSource extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;JsonDataSource&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //导入隐式转化 import sqlContext.implicits._ //创建学生成绩DataFrame val studentScoreDF=sqlContext.read.json(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\studentscore.json&quot;) //create a temporary table studentScoreDF.createOrReplaceTempView(&quot;student_score&quot;) val goodStudentScoreDF = sqlContext.sql(&quot;select * from student_score where score &gt;= 80&quot;) val goodStudentName = goodStudentScoreDF.rdd.map(row =&gt; row(0)).collect() goodStudentName.foreach(println) //googStudentScoreDF.show() //创建学生信息DataFrame val studentInfoJson=Array(&quot;&#123;\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;age\\&quot;:10&#125;&quot; + &quot;&#123;\\&quot;name\\&quot;:\\&quot;Tom1\\&quot;,\\&quot;age\\&quot;:11&#125;&quot; + &quot;&#123;\\&quot;name\\&quot;:\\&quot;Tom2\\&quot;,\\&quot;age\\&quot;:12&#125;&quot; + &quot;&#123;\\&quot;name\\&quot;:\\&quot;Tom3\\&quot;,\\&quot;age\\&quot;:13&#125;&quot;) val studentInfoDF=sqlContext.read.json(sc.parallelize(studentInfoJson)) //create a temporary table of student info studentInfoDF.createOrReplaceTempView(&quot;student_info&quot;) var sql = &quot;select name,age from student_info where name in (&quot; for(i &lt;- 0 until goodStudentName.length)&#123; sql += &quot;&apos;&quot; + goodStudentName(i) + &quot;&apos;&quot; if(i &lt; goodStudentName.length-1)&#123; sql += &quot;,&quot; &#125; &#125; sql += &quot;)&quot; println(&quot;sql========&quot; + sql) val goodStudentInfoDF=sqlContext.sql(sql) //将分数大于80分的学生成绩信息与基本信息进行Join val goodStudentRDD = goodStudentScoreDF.rdd.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Long](&quot;score&quot;))) .join(goodStudentInfoDF.rdd.map(row =&gt; (row.getAs[String](&quot;name&quot;),row.getAs[Long](&quot;age&quot;)))) //将RDD转换为DataFrame val goodStudentRowRDD=goodStudentRDD.map(row=&gt;Row(row._1,row._2._1.toString().toLong,row._2._2.toString.toLong)) val studentStruct=StructType(Array( StructField(&quot;name&quot;,StringType,true), StructField(&quot;score&quot;,LongType,true), StructField(&quot;age&quot;,LongType,true) )) val goodStudentDF=sqlContext.createDataFrame(goodStudentRowRDD,studentStruct) goodStudentDF.show()&#125; 5. Parquet数据源创建DataFrame关键点： sqlContext.read.parquet()方法创建DataFrame 将DataFrame数据写入到Parquet 12studentWithNameAndAgeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) 示例代码 12345678910111213141516171819202122package com.spark.sqlimport org.apache.spark.sql.SQLContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. */object ParquetLoadData extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;ParquetLoadData&quot;) val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) val peopleDF=sqlContext.read.parquet(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people&quot;) peopleDF.show() //将dataframe转化为RDD并打印出来 peopleDF.rdd.map(row =&gt; &quot;name:&quot; + row(0)) .collect() .foreach(username =&gt; println(username))&#125; 用mergeSchema的方式读取数据，进行元数据的合并 1val studentsDF=sqlContext.read.option(&quot;mergeSchema&quot;,true).parquet(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) 实例代码 1234567891011121314151617181920212223242526272829303132333435package com.spark.sqlimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.&#123;SQLContext, SaveMode&#125;/** * Created by Administrator on 2017/3/12. * parquet数据元合并元数据 */object ParquetSchemaMerge extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;ParquetLoadData&quot;) val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) //导入隐式转换 import sqlContext.implicits._ //创建第一个RDD val studentWithNameAndAge=Array((&quot;Tom&quot;,13),(&quot;Mary&quot;,14)) val studentWithNameAndAgeDF=sc.parallelize(studentWithNameAndAge,1).toDF(&quot;name&quot;,&quot;age&quot;) //保存 studentWithNameAndAgeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) //创建第二个RDD val studentWithNameAndGrade=Array((&quot;Yangql&quot;,&quot;A&quot;),(&quot;Test&quot;,&quot;B&quot;)) val studentWithNameAndGradeDF=sc.parallelize(studentWithNameAndGrade).toDF(&quot;name&quot;,&quot;grade&quot;) studentWithNameAndGradeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append) .save(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) //用mergeSchema的方式读取数据，进行元数据的合并 val studentsDF=sqlContext.read.option(&quot;mergeSchema&quot;,true).parquet(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\student&quot;) studentsDF.show() studentsDF.printSchema()&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"SparkSQL RDD转换DataFrame方法","slug":"SparkSQL RDD转换DataFrame方法20190524","date":"2017-03-31T00:54:59.000Z","updated":"2019-05-24T04:59:15.089Z","comments":true,"path":"2017/03/31/SparkSQL RDD转换DataFrame方法20190524/","link":"","permalink":"http://qioinglong.top/2017/03/31/SparkSQL RDD转换DataFrame方法20190524/","excerpt":"DataFrame可以创建临时视图，DataFrame相当于关系数据库上的一个表，我们可以利用DataFrame的这个特性，使用SQL语句进行一系列的操作。将SparkRDD转化为DataFram主要有两种方式。第一：通过反射的方式将RDD转换为DataFrame，第二：通过编程的方式将RDD转换为DataFrame","text":"DataFrame可以创建临时视图，DataFrame相当于关系数据库上的一个表，我们可以利用DataFrame的这个特性，使用SQL语句进行一系列的操作。将SparkRDD转化为DataFram主要有两种方式。第一：通过反射的方式将RDD转换为DataFrame，第二：通过编程的方式将RDD转换为DataFrame 通过反射的方式将RDD转换为DataFrame通过反射的方式将RDD转换为DataFrame，有以下几个步骤： 定义一个case calss，成员包括需要使用的列，如： 1case class Student(id:Int,name:String,age:Int) 创建一个RDD中，数据是一个以case class 封装的对象,以case class reflection 123val peoples=sc.textFile(fileName) .map(lines=&gt;lines.split(&quot;,&quot;)) .map(arrs=&gt;Student(arrs(0).trim().toInt,arrs(1),arrs(2).trim().toInt)) 使用toDF函数，将RDD转换为DataFrame 1val peopleDF=peoples.toDF() 创建临时表 1peopleDF.createOrReplaceTempView(&quot;people&quot;) 使用sql对数据进行分析，返回一个DataFrame对象 1val teenagerDF=sqlContext.sql(&quot;select * from people where age &lt; 20&quot;) 可以将DataFrame转换为RDD，使用RDD的特性 1234val teenagerRDD=teenagerDF.rddteenagerRDD.map(row=&gt;Student(row(0).toString().toInt,row(1).toString(),row(2).toString().toInt)) .collect() .foreach(stu=&gt;println(&quot;id: &quot;+stu.id + &quot; name: &quot; + stu.name + &quot; age: &quot;+stu.age)) 实例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.spark.sqlimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContext/** * 通过反射的方式将RDD转换为DataFrame */object RDD2DataFrameReflection extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;RDD2DataFrameReflection&quot;) val sc = new SparkContext(conf) val sqlContext= new SQLContext(sc) //导入隐式转化 import sqlContext.implicits._ //定义一个case class case class Student(id:Int,name:String,age:Int) //create a rdd,并且以case class reflection val fileName=&quot;E:/spark-scala-study/spark-scala-study/resouce/people.txt&quot; val peoples=sc.textFile(fileName) .map(lines=&gt;lines.split(&quot;,&quot;)) .map(arrs=&gt;Student(arrs(0).trim().toInt,arrs(1),arrs(2).trim().toInt)) //convet to dataframe val peopleDF=peoples.toDF() //peopleDF.show() //create a tempory table peopleDF.createOrReplaceTempView(&quot;people&quot;) //create a new dataframe val teenagerDF=sqlContext.sql(&quot;select * from people where age &lt; 20&quot;) //teenagerDF.show() //covert the dataframe to a rdd val teenagerRDD=teenagerDF.rdd //对RDD构造case class teenagerRDD.map(row=&gt;Student(row(0).toString().toInt,row(1).toString(),row(2).toString().toInt)) .collect() .foreach(stu=&gt;println(&quot;id: &quot;+stu.id + &quot; name: &quot; + stu.name + &quot; age: &quot;+stu.age)) //row.getAs方法，对指定列名做操作 teenagerRDD.map(row=&gt;Student(row.getAs(&quot;id&quot;),row.getAs(&quot;name&quot;),row.getAs(&quot;age&quot;))) .collect() .foreach(stu=&gt;println(&quot;id: &quot;+stu.id + &quot; name: &quot; + stu.name + &quot; age: &quot;+stu.age)) //getValuesmap teenagerRDD.map(row=&gt;&#123; val map=row.getValuesMap(Array(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;)); Student(map(&quot;id&quot;),map(&quot;name&quot;),map(&quot;age&quot;)) &#125;) .collect() .foreach(stu=&gt;println(&quot;id: &quot;+stu.id + &quot; name: &quot; + stu.name + &quot; age: &quot;+stu.age))&#125; 通过编程的方式将RDD转换为DataFrame 构造一个元素为Row的普通RDD,类：org.apache.spark.sql.Row 123val peropleRDD = sc.textFile(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.txt&quot;) .map(line=&gt;line.split(&quot;,&quot;)) .map(arr=&gt;Row(arr(0).toString.toInt,arr(1),arr(2).toString.toInt)) 编程方式动态构造元数据，使用类StructType，StructField 12345val peopleStruc=StructType(Array( StructField(&quot;id&quot;,IntegerType,true), StructField(&quot;name&quot;,StringType,true), StructField(&quot;age&quot;,IntegerType,true) )) 将RDD转换位DataFrame 1val peopleDF=sqlContext.createDataFrame(peropleRDD,peopleStruc) 在DataFrame上使用SQL进行分析 1val teenagerDF=sqlContext.sql(&quot;select * from people where age &lt; 20&quot;) 实例代码 12345678910111213141516171819202122232425262728293031323334353637package com.spark.sqlimport org.apache.spark.sql.types.&#123;IntegerType, StructField, StructType,StringType&#125;import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/12. * 通过编程的方式将RDD转换为DataFrame */object RDD2DataFrameProgrammatically extends App&#123; val conf = new SparkConf() .setAppName(&quot;RDD2DataFrameProgrammatically&quot;) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //crate a RDD,构造元素为Row的普通RDD val peropleRDD = sc.textFile(&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.txt&quot;) .map(line=&gt;line.split(&quot;,&quot;)) .map(arr=&gt;Row(arr(0).toString.toInt,arr(1),arr(2).toString.toInt)) //编程方式动态构造元数据 val peopleStruc=StructType(Array( StructField(&quot;id&quot;,IntegerType,true), StructField(&quot;name&quot;,StringType,true), StructField(&quot;age&quot;,IntegerType,true) )) //将RDD转换位DataFrame val peopleDF=sqlContext.createDataFrame(peropleRDD,peopleStruc) peopleDF.show() //create a temporary table peopleDF.createOrReplaceTempView(&quot;people&quot;) //create a df val teenagerDF=sqlContext.sql(&quot;select * from people where age &lt; 20&quot;) //create a rdd teenagerDF.rdd.collect().foreach(row =&gt; println(row))&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"柯里化","slug":"Scala柯里化20190524","date":"2017-03-29T16:54:59.000Z","updated":"2019-05-24T04:59:14.427Z","comments":true,"path":"2017/03/30/Scala柯里化20190524/","link":"","permalink":"http://qioinglong.top/2017/03/30/Scala柯里化20190524/","excerpt":"概念柯里化(Currying)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数为参数的函数。定义一个函数1def add(x:Int,y:Int)=x+y 调用时：add(1,2)将函数变形为：1def add(x:Int)(y:Int) = x + y 调用时：add(1)(2)以上的过程就是柯里化","text":"概念柯里化(Currying)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数为参数的函数。定义一个函数1def add(x:Int,y:Int)=x+y 调用时：add(1,2)将函数变形为：1def add(x:Int)(y:Int) = x + y 调用时：add(1)(2)以上的过程就是柯里化 实现过程add(1)(2) 实际上是依次调用两个普通函数（非柯里化函数），第一次调用使用一个参数 x，返回一个函数类型的值，第二次使用参数y调用这个函数类型的值。实质上最先演变成这样一个方法：12def add(x:Int)=(y:Int)=&gt;x+y// 接收一个x为参数，返回一个匿名函数，该匿名函数的定义是：接收一个Int型参数y，函数体为x+y 调用12val result = add(1)//返回一个result，那result的值一个匿名函数：(y:Int)=&gt;1+y 继续调用result1val sum = result(2) 实例代码12345678910package com.scala.base/** * 柯里化 */object CurringTest extends App&#123; def add(x:Int,y:Int)=x+y def add1(x:Int)(y:Int) = x + y println(add(1,2)) println(add1(1)(2))&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala的Symbol类型","slug":"Scala的Symbol类型20190524","date":"2017-03-29T12:54:59.000Z","updated":"2019-05-24T04:59:14.199Z","comments":true,"path":"2017/03/29/Scala的Symbol类型20190524/","link":"","permalink":"http://qioinglong.top/2017/03/29/Scala的Symbol类型20190524/","excerpt":"相比较于String类型，Symbol类型有两个比较明显的特点:节省内存和快速比较 String的internString类内部维护一个字符串池(strings pool)，当调用String的intern()方法时，如果字符串池中已经存在该字符串，则直接返回池中字符串引用，如果不存在，则将该字符串添加到池中，并返回该字符串对象的引用。执行过intern()方法的字符串，我们就说这个字符串被拘禁了(interned)。默认情况下，代码中的字符串字面量和字符串常量值都是被拘禁的，例如：1234String s1 = &quot;abc&quot;;String s2 =new String(&quot;abc&quot;);//返回trueSystem.out.println(s1 == s2.intern()); 同值字符串的intern()方法返回的引用都相同，例如：1234567String s2 = new String(&quot;abc&quot;);String s3 = new String(&quot;abc&quot;);//返回trueSystem.out.println(s2.intern() == s3.intern());//返回falseSystem.out.println(s2 == s3);","text":"相比较于String类型，Symbol类型有两个比较明显的特点:节省内存和快速比较 String的internString类内部维护一个字符串池(strings pool)，当调用String的intern()方法时，如果字符串池中已经存在该字符串，则直接返回池中字符串引用，如果不存在，则将该字符串添加到池中，并返回该字符串对象的引用。执行过intern()方法的字符串，我们就说这个字符串被拘禁了(interned)。默认情况下，代码中的字符串字面量和字符串常量值都是被拘禁的，例如：1234String s1 = &quot;abc&quot;;String s2 =new String(&quot;abc&quot;);//返回trueSystem.out.println(s1 == s2.intern()); 同值字符串的intern()方法返回的引用都相同，例如：1234567String s2 = new String(&quot;abc&quot;);String s3 = new String(&quot;abc&quot;);//返回trueSystem.out.println(s2.intern() == s3.intern());//返回falseSystem.out.println(s2 == s3); 节省内存在Scala中，Symbol类型的对象是被拘禁的(interned)，任意的同名symbols都指向同一个Symbol对象，避免了因冗余而造成的内存开销。而对于String类型，只有编译时确定的字符串是被拘禁的(interned)。12345val s=&apos;testprintln(s==&apos;test)//output trueprintln(s==Symbol(&quot;test&quot;))//output true 快速比较由于Symbol类型的对象是被拘禁的(interned)，任意的同名symbols都指向同一个Symbol对象，而不同名的symbols一定指向不同的Symbol对象，所以symbols对象之间可以使用操作符==快速地进行相等性比较，常数时间内便可以完成，而字符串的equals方法需要逐个字符比较两个字符串，执行时间取决于两个字符串的长度，速度很慢。(实际上，String.equals方法会先比较引用是否相同，但是在运行时产生的字符串对象，引用一般是不同的) Symbol 应用Symbol类型一般用于快速比较，例如用于Map类型：Map,根据一个Symbol对象，可以快速查询相应的Data, 而Map的查询效率则低很多。实例代码123456789101112131415161718192021class Foo(val name:String,val age:Int,val sex:Symbol) &#123;&#125;object Foo&#123; def apply(name:String,age:Int,sex:Symbol)=new Foo(name,age,sex)&#125;var fooList=Foo(&quot;name1&quot;,10,&apos;male):: Foo(&quot;name2&quot;,10,&apos;female):: Foo(&quot;name3&quot;,10,&apos;male)::Nil var stringList=fooList.foldLeft(List[String]())( (z,f)=&gt;&#123; var title=f.sex match &#123; case &apos;male =&gt; &quot;Mr.&quot; case &apos;female =&gt; &quot;Ms.&quot; &#125; z :+ s&quot;$title $&#123;f.name&#125; $&#123;f.age&#125;&quot; &#125; ) println(stringList) s”$title ${f.name} ${f.age}”：其中s是一个函数12345val name=&quot;Tim&quot;println(s&quot;$name&quot;)//output Timprintln(s&quot;1+1=$&#123;1+1&#125;&quot;)//output 1+1=2","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-zip相关算子","slug":"Spark-zip相关算子20190524","date":"2017-03-29T07:54:59.000Z","updated":"2019-05-24T04:59:15.443Z","comments":true,"path":"2017/03/29/Spark-zip相关算子20190524/","link":"","permalink":"http://qioinglong.top/2017/03/29/Spark-zip相关算子20190524/","excerpt":"zip 算子zip函数用于将两个RDD组合成key/value形式的RDD，这里默认两个RDD的数量(partitions)以及元素数量都相同，否则将抛出异常java.lang.IllegalArgumentException: Can&#39;t zip RDDs with unequal numbers of partitions:函数定义：1def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] 实例代码：123456789101112val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;ZipTest&quot;)val sc = new SparkContext(conf)val rdd1=sc.makeRDD(1 to 5,2)val rdd2=sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;),2)rdd1.zip(rdd2).foreach(print)//output//(1,A)(2,B)(3,C)(4,D)(5,E)rdd2.zip(rdd1).foreach(print)//output//(A,1)(B,2)(C,3)(D,4)(E,5)","text":"zip 算子zip函数用于将两个RDD组合成key/value形式的RDD，这里默认两个RDD的数量(partitions)以及元素数量都相同，否则将抛出异常java.lang.IllegalArgumentException: Can&#39;t zip RDDs with unequal numbers of partitions:函数定义：1def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] 实例代码：123456789101112val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;ZipTest&quot;)val sc = new SparkContext(conf)val rdd1=sc.makeRDD(1 to 5,2)val rdd2=sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;),2)rdd1.zip(rdd2).foreach(print)//output//(1,A)(2,B)(3,C)(4,D)(5,E)rdd2.zip(rdd1).foreach(print)//output//(A,1)(B,2)(C,3)(D,4)(E,5) zipPartitions 算子zipPartitions函数将多个RDD按照partition组合成为新的RDD，该函数需要组合的RDD具有相同的分区数，但对于每个分区内的元素数量没有要求。根据参数的个数可以分为三类，参数preservesPartitioning的作用：是否保留父RDD的partitioner分区信息。映射方法f参数N个RDD的迭代器。 参数是一个RDD：12def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B]) RDD分区元素分布：123456789101112val rdd1=sc.makeRDD(1 to 5,2)val rdd2=sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;),2)val rdd1=sc.makeRDD(1 to 5,2)//rdd1两个分区中元素分布： val rdd1ite=rdd1.mapPartitionsWithIndex((x,iter)=&gt;&#123; var result=List[String]() while(iter.hasNext)&#123; result ::= (&quot;part_&quot;+x+&quot;|&quot;+iter.next()) &#125; result.iterator &#125;).collect()//output:part_0|2,part_0|1,part_1|5,part_1|4,part_1|3 rdd1.zipPartitions元素分布:123456789101112131415rdd1.zipPartitions(rdd2)&#123; (rdd1Iter,rdd2Iter)=&gt;&#123; var result = List[String]() while(rdd1Iter.hasNext &amp;&amp; rdd2Iter.hasNext)&#123; result::=(rdd1Iter.next() + &quot;_&quot; + rdd2Iter.next()) &#125; result.iterator &#125;&#125;.collect().foreach(println)//output//2_B//1_A//5_E//4_D//3_C 参数是两个RDD：12def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C]) 实例代码：123456789101112131415161718val rdd1=sc.makeRDD(1 to 5,2)val rdd2=sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;),2)val rdd3=sc.makeRDD(1 to 10,2)rdd1.zipPartitions(rdd2,rdd3)( (iter1,iter2,iter3)=&gt;&#123; var result=List[String]() while(iter1.hasNext &amp;&amp; iter2.hasNext &amp;&amp; iter3.hasNext)&#123; result ::= iter1.next() + &quot;_&quot; + iter2.next() + &quot;_&quot; +iter3.next() &#125; result.iterator &#125; ).collect().foreach(println)//output2_B_21_A_15_E_84_D_73_C_6 参数是三个RDD：12def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) zipWithIndex该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。1def zipWithIndex(): RDD[(T, Long)] 代码示例:12345678val rdd1=sc.makeRDD(1 to 5,2)rdd1.zipWithIndex().collect().foreach(println)//output(1,0)(2,1)(3,2)(4,3)(5,4) zipWithUniqueId该函数将RDD中元素和一个唯一ID组合成键/值对，该唯一ID生成算法如下： 每个分区中第一个元素的唯一ID值为：该分区索引号， 每个分区中第N个元素的唯一ID值为：(前一个元素的唯一ID值) + (该RDD总的分区数)1def zipWithUniqueId(): RDD[(T, Long)] 代码示例：12345678val rdd1=sc.makeRDD(1 to 5,2)rdd1.zipWithUniqueId().collect().foreach(println)//output(1,0)(2,2)(3,1)(4,3)(5,5) rdd1总分区数为2,第一个分区第一个元素ID为0，第二个分区第一个元素ID为1,第一个分区第二个元素ID为0+2=2，第一个分区第三个元素ID为2+2=4,第二个分区第二个元素ID为1+2=3，第二个分区第三个元素ID为3+2=5.","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala跳出循环的3中方法","slug":"Scala跳出循环的3中方法20190524","date":"2017-03-29T03:54:59.000Z","updated":"2019-05-24T04:59:14.528Z","comments":true,"path":"2017/03/29/Scala跳出循环的3中方法20190524/","link":"","permalink":"http://qioinglong.top/2017/03/29/Scala跳出循环的3中方法20190524/","excerpt":"Java里经常会用到continue和break，Scala里并没有这俩个语法。介绍三种跳出循环的方式,使用boolean变量、使用return、使用Breaks对象的break方法","text":"Java里经常会用到continue和break，Scala里并没有这俩个语法。介绍三种跳出循环的方式,使用boolean变量、使用return、使用Breaks对象的break方法 使用boolean变量while代码:123456789101112131415//while代码 var result:Int=0 var flag:Boolean=true while(flag)&#123; println(&quot;output: &quot; + result) result += 1 if(result==5)&#123; flag=false &#125; &#125;//output: 0//output: 1//output: 2//output: 3//output: 4 for代码：123456789101112131415//for 代码var result:Int=0var flag:Boolean=truefor(i&lt;-0 until 10 if flag)&#123; println(&quot;output: &quot; + i) if(i==5)&#123; flag=false &#125;&#125;//output: 0//output: 1//output: 2//output: 3//output: 4//output: 5 使用returnwhile代码：123456789101112var x:Int=10while(x &gt;= 0)&#123; println(&quot;output:&quot;+x) x-=1 if(x == 5) return&#125;/*output:10output:9output:8output:7output:6*/ for代码:12345678910for(x &lt;- 0 until 10)&#123; println(&quot;output : &quot;+ x) if(x==5) return&#125;/*output : 0output : 1output : 2output : 3output : 4output : 5*/ 使用Breaks对象的break方法while代码1234567891011121314import scala.util.control.Breaks._var x=10breakable(while(x&gt;0)&#123; println(&quot;output: &quot; + x) x-=1 if(x==5) break()&#125; )/*output: 10output: 9output: 8output: 7output: 6*/ for代码：12345678910111213import scala.util.control.Breaks._breakable(for(x &lt;- 0 until 10)&#123; println(&quot;output:&quot;+x) if(x==5) break()&#125;)/* output:0output:1output:2output:3output:4output:5*/","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark 共享变量","slug":"Spark-共享变量20190524","date":"2017-03-29T00:54:59.000Z","updated":"2019-05-24T04:59:15.644Z","comments":true,"path":"2017/03/29/Spark-共享变量20190524/","link":"","permalink":"http://qioinglong.top/2017/03/29/Spark-共享变量20190524/","excerpt":"共享变量概念默认情况下，如果在一个算子中使用到了一个外部变量，那么这个变量值会被拷贝到每个task任务中。此时每个task都只能操作自己的那份变量副本。如果多个task想要共享变量，这种情况是做不到的。Spark为此提供了两种共享变量，一种是BroadCast Variable(广播变量)，另一种是Accumulator（累加变量）。BroadCast会将使用到的变量，仅仅为每个节点拷贝一份，而不是为每个task拷贝一个副本，这样做的好处是优化性能，减少网络传输以及内存消耗。SparkContext.broadCast()方法创建广播变量。用value()方法读取值。Accumulator则可以让多个task共同操作一个变量，变量可以进行累加操作","text":"共享变量概念默认情况下，如果在一个算子中使用到了一个外部变量，那么这个变量值会被拷贝到每个task任务中。此时每个task都只能操作自己的那份变量副本。如果多个task想要共享变量，这种情况是做不到的。Spark为此提供了两种共享变量，一种是BroadCast Variable(广播变量)，另一种是Accumulator（累加变量）。BroadCast会将使用到的变量，仅仅为每个节点拷贝一份，而不是为每个task拷贝一个副本，这样做的好处是优化性能，减少网络传输以及内存消耗。SparkContext.broadCast()方法创建广播变量。用value()方法读取值。Accumulator则可以让多个task共同操作一个变量，变量可以进行累加操作 共享变量原理 广播变量实例12345678910111213141516171819202122232425package com.spark.coreimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject BroadCastVariable&#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;BroadCastVariable&quot;) val sc = new SparkContext(conf) //创建一个共享变量-广播变量 只读，不能写 val factor:Int=3 val broadCastFactor=sc.broadcast(factor) //定义一个RDD val numberArray=Array(1,2,3,4) val numbers=sc.parallelize(numberArray, 2) val multiNumbers=numbers.map(_*broadCastFactor.value) multiNumbers.foreach(println) &#125;&#125; 累加器实例123456789101112131415161718192021222324package com.spark.coreimport org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject AccumulatorVariable &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;BroadCastVariable&quot;) val sc = new SparkContext(conf) //定义一个共享变量-accumulator val sum=sc.accumulator(0) //定义一个RDD val numberArray=Array(1,2,3,4) val numbers=sc.parallelize(numberArray, 2) numbers.foreach(num=&gt;sum+=num) println(sum) &#125;&#125; 自定义广播变量自定义广播变量时，需要实现 Trait AccumulatorParam[T]中的三个方法，zero,addAccumulator,addInPlace,在调用时采用val sessionAggrStatAccumulator=sc.accumulator(&quot;&quot;)(new SessionAggrStatAccumulator)实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.project.sessionimport org.apache.spark.AccumulatorParamimport com.project.dao.Constantsimport com.project.util.StringUtils/** * session聚合统计Accumulator */class SessionAggrStatAccumulator extends AccumulatorParam[String]&#123; /** * 用于对数据的初始化 */ override def zero(initialValue:String):String=&#123; Constants.SESSION_COUNT + &quot;=0|&quot; + Constants.TIME_PERIOD_1s_3s + &quot;=0|&quot; + Constants.TIME_PERIOD_4s_6s + &quot;=0|&quot; + Constants.TIME_PERIOD_7s_9s + &quot;=0|&quot; + Constants.TIME_PERIOD_10s_30s + &quot;=0|&quot; + Constants.TIME_PERIOD_30s_60s + &quot;=0|&quot; + Constants.TIME_PERIOD_1m_3m + &quot;=0|&quot; + Constants.TIME_PERIOD_3m_10m + &quot;=0|&quot; + Constants.TIME_PERIOD_10m_30m + &quot;=0|&quot; + Constants.TIME_PERIOD_30m + &quot;=0|&quot; + Constants.STEP_PERIOD_1_3 + &quot;=0|&quot; + Constants.STEP_PERIOD_4_6 + &quot;=0|&quot; + Constants.STEP_PERIOD_7_9 + &quot;=0|&quot; + Constants.STEP_PERIOD_10_30 + &quot;=0|&quot; + Constants.STEP_PERIOD_30_60 + &quot;=0|&quot; + Constants.STEP_PERIOD_60 + &quot;=0&quot; &#125; override def addAccumulator(v1:String, v2:String):String=&#123; add(v1,v2) &#125; override def addInPlace(v1:String, v2:String):String=&#123; add(v1,v2) &#125; /** * session统计计算逻辑 * v1:连接字符串 * V2范围区间 * 返回更新后的字符串 */ private[this] def add(v1:String,v2:String):String=&#123; // 校验：v1为空的话，直接返回v2 if(StringUtils.isEmpty(v1))&#123; return v2 &#125; // 使用StringUtils工具类，从v1中，提取v2对应的值，并累加1 val oldValue = StringUtils.getFieldFromConcatString(v1, &quot;\\\\|&quot;, v2); if(oldValue != null) &#123; // 将范围区间原有的值，累加1 val newValue = oldValue.toInt + 1; // 使用StringUtils工具类，将v1中，v2对应的值，设置成新的累加后的值 return StringUtils.setFieldInConcatString(v1, &quot;\\\\|&quot;, v2, newValue.toString()); &#125; return v1 &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"快学Scala","slug":"Scala-读书笔记《快学Scala》20190524","date":"2017-03-28T02:54:59.000Z","updated":"2019-05-24T04:59:14.252Z","comments":true,"path":"2017/03/28/Scala-读书笔记《快学Scala》20190524/","link":"","permalink":"http://qioinglong.top/2017/03/28/Scala-读书笔记《快学Scala》20190524/","excerpt":"1.可变参数列表12345678//定义一个函数,可变长度参数 def sum(args:Int*):Int=&#123; var result = 0 for(arg &lt;- args)&#123; result += arg &#125; result &#125;","text":"1.可变参数列表12345678//定义一个函数,可变长度参数 def sum(args:Int*):Int=&#123; var result = 0 for(arg &lt;- args)&#123; result += arg &#125; result &#125; val s = sum(1 to 5)//错误如果sum函数被调用时传入的是单个参数，那么该参数就应该是单个整数，而不是一个整数区间。解决这个问题的办法就是告诉编译器你希望这个参数被当作参数序列处理正确的做法123val s =sum(1 to 5:_*)``` 在递归调用中，用`_*`将其转化为参数序列 //递归调用 def recursiveSum(args:Int):Int={ if(args.length==0) 0 else args.head + recursiveSum(args.tail:_) }1234### 2.过程 ###scala对于不返回值得函数有特殊的表示方法，如果函数提包含在括号中，但是前面没有=号，那么返回值就是Unit类型，这样的函数被称为过程。 打印------------ println(“-“ * 10)1### 3.懒值 ### lazy val wordCount=Source.fromFile(“D:/test.txt”).mkString1234567如果程序从不访问变量wordCount,那么文件永远不会被打开，懒值对于开销较大的初始化语句十分有用。### 4.异常 ###throw 表达式有特殊的类型Nothing，在if/else 表达式中，如果一个分支的类型是Nothing，if/else表达式的类型就是另一个分支的类型。## 2.数组 ##不可变数组使用Array 可变数组使用ArrayBuffer val arrayBuffer =ArrayBufferInt arrayBuffer += 3 val array=Array(4,5,6) arrayBuffer ++= array arrayBuffer.trimEnd(3) println(arrayBuffer.mkString(“,”)) 12345678910111213+=:在尾端添加元素 ++=：追加集合 trimEnd(n):移除最后的n个元素 toArray:转化为数组 toBuffer():转化为数组缓冲 0 to n:[0,n] 0 until n:[0,n) 0 until(n,2):每两个元素一跳 (0 until n).reverse### 1.数组转化 ###数组转化并不会修改原始数组，而是会产生一个全新的数组 使用for 推导式 val a = Array(1,2,3,4) val result = for(item &lt;- a) yield item*2 println(result.mkString(“,”))1### 2.数组守卫 ### for(elem &lt;- a if elem % 2 ==0 ) yield elem * 212345678array.sum array.min array.max println(a.sorted) //升序 println(a.sortWith(_&lt;_))//升序 println(a.sortWith(_&gt;_))//降序 ### 3.多维数组 ### val matrix = Array.ofDimDouble matrix(2)(1)=42 println(matrix(2)(1))12## 映射 ## val scores=Map(“key1”-&gt;”values”,”key2”-&gt;”value2”)val scores =Map((key1,value1),(key2,value2))//获取值scores(“key1”)contains//判断是否含有某个元素scores.getOrElse(“key1”,”0”)//如果没有，就取默认值//如果可变score(“bog”)=”bog”//如果存在就更新，不存在就新增+=：添加多个关系-=：移除某个键和对应的值迭代映射for((k,v) &lt;- map)map.keySet:获取keymap.values:获取值反转键和值:for((k,v)&lt;-map) yield(v,k)12345678910### 元组 ###val tuple=（1，2,3）元组访问:tuple._1 tuple._2 tuple._3 使用模式匹配来获取元组的组员， val (first,second,third) = tuple如果不需要某个值，可以在不需要的部件上使用`_` val (first,second,_)=tuple println(&quot;New YourK&quot;.partition(_.isUpper)): partition():返回元组（满足条件，不满足条件）使用元组的好处就是可以将多个值绑在一起进行处理，可以使用zip来处理。 val array=Array(“key1”,”key2”,”key3”) val count=Array(1,2,3) val paris =array.zip(count) for((k,v) &lt;- paris) println(k + “ “ + v)123## 类 ##1.定义一个类时，定义一个公有变量是，默认生成JVM类时，变量为私有，getter setter方法是公有的。定义私有变量时，变量，getter setter方法都是私有的。gettersetter 分别叫做age age_= class Person { private var privateage=0 def age=privateage def age_=(newAge:Int){ privateage=newAge }}123452.如果字段是私有的，则getter and setter 方法也是私有的 3.如果字段是val的，则只有getter方法生成 4.如果你不需要任何getter setter方法，可以将字段声明为`private[this]` 5.如果定义方法没有(),则在调用时，也不能加() 6.方法可以访问该类所有对象的私有字段 class Counter{ private var value =0 def isLess(other:Counter) = value &lt; other.value//other同样也是Counter对像}17.pirvate[this] 限制只能是当前对象才能访问 class Counter1{ private[this] var value =0 def isLess(other:Counter1) = value &lt; other.value//错误，只能访问当前对象}18.private[类名]修饰符可以定义仅有指定的类的方法可以访问给定的字段，这里的类名必须是当前定义的类，或者是包含该类的外部类。 class Counter1{ private[Counter1] var value =0 def isLess(other:Counter1) = value &lt; other.value//错误，只能访问当前对象}19.Bean属性，生成getXXX,setXX形式的方法，在属性前面加上`@BeanPropery` import scala.beans.BeanPropertyclass Persons{ @BeanProperty var name:String=”default”}//生成四个方法1.name:String2.name_=(newValue:String):Unit3.getName():String4.setName(newValue:String):Unit1234![image](E:\\03_学习资料\\06_IT学习\\02_编程语言\\07_Scala\\scala字段生成方法.png)10.辅助构造器,- 辅助构造器的名称为this- 每一个辅助构造器都必须以一个对先前已经定义的其它辅助构造器或者主构造器开始。 class Person { var age=0 var name =”” def this(name:String){//一个辅助构造器 this()//调用柱构造器 this.name=name } def this(name:String,age:Int){//另外一个辅助构造器 this()//也可以是 this(name) this.name=name this.age=age }}//==============================================package com.scala.base object TestClass extends App{ val p1 = new Person()//主辅助器 val p2 = new Person(“test”,1)//第一个辅助器 val p3 = new Person(“test”)//第二个辅助器 println(p1.name + “ “ + p1.age) println(p2.name + “ “ + p2.age) println(p3.name + “ “ + p3.age)} 1234567891011121314151617181920 11.主构造器 - 每个类都有主构造器，主构造器并不以this方法定义。而是: class Person(val name:String,val age:Int) - 针对主构造器参数生成的字段和方法 |柱构造器参数|生成的字段和方法||---|---|name:Stirng|对象私有字段，如果没有方法使用name，那么就没有这个字段private val/var name:String|私有字段，私有的getter setterval/var name:String|私有字段，公有的getter setter@BeanPropery val/var name:String|私有字段，JavaBean版本的setter getter- class peroson private(val id:Int):主构造器变为私有## 对象 ##1.对象的作用(object)- 作为存放工具函数或者常量的地方- 高效地共享单个不可变实例- 需要用单个实例来协调某个服务2.伴生对象，类的伴生对象可以被访问，但并不在作用域中，3.枚举 object TrafficLightColor extends Enumeration{ //val red,yello,green = Value val red = Value(0,”Red”) val yellow = Value(1,”yellow”) val green = Value(2,”green”)}12345678910```object TestClass extends App&#123; println(TrafficLightColor.red) println(TrafficLightColor.yellow) for(c &lt;- TrafficLightColor.values)&#123; println(c.id + &quot; &quot; + c) &#125; println(TrafficLightColor(0)) println(TrafficLightColor.withName(&quot;green&quot;))&#125; 类 extends关键字，继承类。 final 修饰类时，类不能被扩展1.类型检查和转化 isInstanceOf:测试某个对象是否属于某个特定的类 asInstanceOf:将引用转化为子类的引用 p.getClass=classOf(类名) ：测试P指向的是某个对象但不是其子类。 抽象类:abstract,在子类中重写抽象方法时，不需要使用override关键字 val 定义完了 引用对象的地址就不会变了 def 定义后 每调用一次就会被重新执行一次 123456abstract class Person0322(val name:String)&#123; def id:Int //没有方法体，这是一个抽象方法&#125;class Employee0322(name:String) extends Person0322(name)&#123; def id = name.hashCode()&#125; 抽象字段：没有初始值的字段 匿名类 123 val alien = new Person(&quot;Fred&quot;)&#123; def greeting=&quot;dddddd&quot; + name&#125; 提前定义： 123class Ant extends &#123; override val range=2&#125; with Animal 文件及正则表达式1.scala.io.Source.fromFile(“文件名”,”字符编码”)Source.fromURLsource.fromString 读取二进制文件：scala没有提供，需要使用java的类库 12345val file = new File(fileNmae)val in = new FileInputStream(file)val bytes = new Array[Byte](file.length.toInt)in.read(bytes)in.close() 写入文件使用java PrintWriter 序列化 1234@SerialVersionUID(42L) class Counter extends Serializable&#123; private var value =0 def isLess(other:Counter) = value &lt; other.value//other同样也是Counter对像&#125; sys.process提供用于与shell交互。 1234import sys.process._&quot;ls -al ..&quot; ! //返回上层目录的所有文件 !!两个感叹号是字符串形式返回&quot;ls -al ..&quot; #| &quot;grep xxx&quot; ! //#|管道#&gt;重定向 正则表达式类：scala.util.matching.Regex 1234567891011121314151617 import scala.util.matching.Regex val numPattern=&quot;[0-9]+&quot;.r for(matching &lt;- numPattern.findAllIn(&quot;dsafdsa sa33 333&quot;))&#123;//匹配所有 println(matching) &#125; //转成数组 val array=numPattern.findAllIn(&quot;dd321 &quot;).toArray //匹配收个 val matchfirst=numPattern.findFirstIn(&quot;dasfda asf3&quot;) //检查开始时候匹配 val firstmatch=numPattern.findPrefixOf(&quot;&quot;) //替换 numPattern.replaceAllIn(&quot;&quot;, &quot;&quot;) numPattern.replaceFirstIn(&quot;&quot;, &quot;&quot;) //正则表达式组 val numGroupPatter=&quot;([0-9]+) ([a-z]+)&quot;&#125; 特质 特质可以同时拥有抽象方法和具体方法，不需要将方法声明为abstract,未被实现的方法默认就是抽象的 继承多个特质时，使用with语句 scala只能有一个超类，但是可以有多个特质 子类实现特质的方法时，不需要写override 123456789101112package com.scala.basetrait Logger &#123; def log(msg:String) //这是一个抽象方法。没有实现&#125;package com.scala.baseclass ConsoleLogger extends Logger&#123; def log(msg:String)&#123; //不需要写override println(msg) &#125;&#125; 特质可以带部分实现，带有特质的对象 12val test=new Account with ConsoleLoggertest.log(&quot;test&quot;) 继承多个特质时，特质从最后一个开始处理 1234val acct1 = new Account with TimestampLogger with shorterLoggerval acct2 = new Account with shorterLogger with TimestampLoggeracct1.log(&quot;acct1&quot;)acct1.log(&quot;acct2&quot;) 在特质中重新抽象方法 12345678910trait Logger &#123; def log(msg:String) //这是一个抽象方法。没有实现&#125;trait TimestampLogger extends Logger&#123; println(&quot;TimestampLogger&quot;) abstract override def log(msg:String)&#123;//scala 认为TimestampLogger是抽象的，必须要混入一个具体log方法，所以必须要加上abstrat 和 override关键字 super.log(new java.util.Date() + &quot; &quot; +msg) &#125;&#125; 当作富接口使用的特质 123456trait Logger &#123; def log(msg:String) //这是一个抽象方法。没有实现 def info(msg:String)&#123;log(&quot;info &quot;+ msg)&#125; def warn(msg:String)&#123;log(&quot;warn&quot; + msg)&#125; def error(msg:String)&#123;log(&quot;error&quot; + msg)&#125;&#125; 特质中的字段可以是具体的，也可以是抽象的，如果你给出了初始值，那么字段就是具体的。任何通过这种方式混入的字段都会自动成为该类自己的字段，特质中未被初始化的字段在子类中必须重写 特质不能有构造器参数，每个特质都有一个无参数的构造器（缺少构造器参数是特质与类之间唯一的技术差别） 12345//提前定义val acc = new &#123;val filenmae =&quot;file.txt&quot;&#125;with SavingAccount with FileLogger 自身类型：当特质以如下代码开始定义时， this:类型=&gt;,它便只能被混入指定类型的子类 1234def mulOne(x:Int)=(y:Int)=&gt;&#123; x*y&#125;println(mulOne(7)(8)) 高阶函数val fun = ceil _ //( _ 将fun方法转化为函数 ) 柯里化：将原来接受两个参数的函数变成新的接受一个参数的函数过程，新的函数返回一个以原有第二个参数作为参数的类型 unioin【|/++】 intersect【&amp;】 diff【&amp;~/—】 一般而言，+用于将元素添加到无先后次序的集合，+:和:+将元素添加到有先后次序的集合的开头和末尾。++ 一次添加多个元素 模式匹配1234567891011121314ch match&#123;case &apos;+&apos; =&gt; sign=1case &apos;-&apos; =&gt; sign=-1case _ if ch==1 =&gt; kk //守卫是以任何boolean条件csse x:Int=&gt;Integer.ParseInt(x) //类型匹配，在匹配类型的时候，必须给出一个变量名，否则，你将会拿对象本身来进行匹配case _ =&gt; sign=0&#125;arry match&#123;case Array(0) =&gt; &quot;0&quot; //匹配包含0的数组case Array(x,y) =&gt; x + y //匹配任何包含两个元素的数组case Array(0,_*) =&gt; &quot;0 ....&quot;//匹配包含任何以0开始的数组case _ =&gt; &quot;something&quot;&#125; 样例类时一种特殊的类，经过优化被用于模式匹配 copy方法和带名参数 12val amt=Account(&quot;test&quot;)val price=amt.copy(&quot;teset1&quot;) sealed 密封类","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"spark函数-subtractByKey,subtract","slug":"Spark函数-subtractByKey,subtract20190524","date":"2017-03-21T02:55:59.000Z","updated":"2019-05-24T04:59:15.759Z","comments":true,"path":"2017/03/21/Spark函数-subtractByKey,subtract20190524/","link":"","permalink":"http://qioinglong.top/2017/03/21/Spark函数-subtractByKey,subtract20190524/","excerpt":"subtractByKeyRDD1.subtractByKey(RDD2):返回一个新的RDD，内容是：RDD1 key中存在的，RDD2 key中不存在的实例如下：","text":"subtractByKeyRDD1.subtractByKey(RDD2):返回一个新的RDD，内容是：RDD1 key中存在的，RDD2 key中不存在的实例如下：123456789101112131415package com.spark.coreimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject RDDTest extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;RDDTest&quot;) val sc = new SparkContext(conf) val rdd = sc.parallelize(Array((1,2),(3,4),(3,6),(9,10))) val other1 = sc.parallelize(Array((3,9))) rdd.subtractByKey(other1).foreach(println)&#125; 打印结果如下：12(1,2)(9,10) subtractRDD1.subtract(RDD2):返回一个新的RDD，内容是：RDD1中存在的，RDD2中不存在的实例如下：1234567891011121314package com.spark.coreimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject RDDTest extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;RDDTest&quot;) val sc = new SparkContext(conf) val rdd = sc.parallelize(Array((1,2),(3,4),(3,6),(9,10))) val other1 = sc.parallelize(Array((3,4))) rdd.subtract(other1).foreach(println)&#125; 打印结果如下：123(9,10)(1,2)(3,6)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"spark函数-cogroup","slug":"Spark函数-cogroup20190524","date":"2017-03-21T02:54:59.000Z","updated":"2019-05-24T04:59:15.715Z","comments":true,"path":"2017/03/21/Spark函数-cogroup20190524/","link":"","permalink":"http://qioinglong.top/2017/03/21/Spark函数-cogroup20190524/","excerpt":"cogroup：将多个RDD中同一个Key对应的Value组合到一起。最多可以组合四个RDD。当对应RDD中不存在对应Key的元素（自然就不存在Value了），在组合的过程中将RDD对应的位置设置为CompactBuffer()了，而不是去掉了。实例如下：","text":"cogroup：将多个RDD中同一个Key对应的Value组合到一起。最多可以组合四个RDD。当对应RDD中不存在对应Key的元素（自然就不存在Value了），在组合的过程中将RDD对应的位置设置为CompactBuffer()了，而不是去掉了。实例如下：12345678910111213141516package com.spark.coreimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject RDDTest extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;RDDTest&quot;) val sc = new SparkContext(conf) val rdd = sc.parallelize(Array((1,2),(3,4),(3,6))) val other1 = sc.parallelize(Array((3,9))) val other2 = sc.parallelize(Array((4,9))) val other3 = sc.parallelize(Array((6,9))) rdd.cogroup(other1,other2,other3).foreach(println(_))&#125; 打印结果如下：1234(4,(CompactBuffer(),CompactBuffer(),CompactBuffer(9),CompactBuffer()))(1,(CompactBuffer(2),CompactBuffer(),CompactBuffer(),CompactBuffer()))(6,(CompactBuffer(),CompactBuffer(),CompactBuffer(),CompactBuffer(9)))(3,(CompactBuffer(4, 6),CompactBuffer(9),CompactBuffer(),CompactBuffer()))","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark SQL性能优化","slug":"Spark SQL性能优化20190524","date":"2017-03-17T00:54:59.000Z","updated":"2019-05-24T04:59:14.873Z","comments":true,"path":"2017/03/17/Spark SQL性能优化20190524/","link":"","permalink":"http://qioinglong.top/2017/03/17/Spark SQL性能优化20190524/","excerpt":"","text":"Spark SQL 性能优化的一些建议： 在SQLContext.setConf()中设置shuffle过程中的并行度，spark.sql.shuffle.partitions 在Hive数据仓库建设过程中，合理设置数据类型，比如可设置为INT的，就不要设置为BIGINT。减少因为数据类型导致的不必要的开销 编写SQL时，明确给出列名，不要写 select * 方式 并行处理查询结果，对于Spark SQL的查询结果，如果数据量比较大，那么久不要一次性collect()到Driver再处理，使用foreach算子，并行处理查询结果。 缓存表，对于一条SQL语句中可能多次使用到的表，可以对其进行缓存，使用 SQLContext.cacheTable(TableName),或者DataFrame.cache().Spark SQL 会用内存列存储的格式进行表的缓存。然后Spark SQL 就可以仅仅扫描需要使用的列，并且自动优化压缩，来最小化内存使用和GC开销。SQLContext.uncacheTable(TableName)可以将表从缓存中移除。用SQLContext.setConf 设置 spark.sql.inMemoryColumnarStorage.batchSize参数(默认10000)，可以配置列存储的单位。 广播Join表，spark.sql.autoBroadCastJoinThreshold,默认(10M),在内存够用的情况下，提高其大小，可以将Join表的较小的表广播出去，而不用进行网络数据传输了。 钨丝计划,spark.sql.tungsten.enable,默认是True，自动管理内存.","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"SparkSQL之DataFrame","slug":"SparkSQL之DataFrame20190524","date":"2017-03-15T00:54:59.000Z","updated":"2019-05-24T04:59:15.181Z","comments":true,"path":"2017/03/15/SparkSQL之DataFrame20190524/","link":"","permalink":"http://qioinglong.top/2017/03/15/SparkSQL之DataFrame20190524/","excerpt":"1.DataFrame相关概念 Spark SQL是Spark中一个处理结构化数据的模块。它提供了被称为DataFrames的编程抽象，并能作为一个分布式的SQL查询引擎 DataFrame是一个由命名列组成的分布式数据集。它从概念上讲相当于关系数据库里的一张表，或R/Python里的数据框架，但内部有很多优化。 DataFrame能通过广泛的数据源构建，比如：结构化数据文件，Hive数据表，外部数据库或已有的RDDs。 使用SQLContext对象，应用程序能从已有RDD、从Hive数据表或从其它数据源创建DataFrame","text":"1.DataFrame相关概念 Spark SQL是Spark中一个处理结构化数据的模块。它提供了被称为DataFrames的编程抽象，并能作为一个分布式的SQL查询引擎 DataFrame是一个由命名列组成的分布式数据集。它从概念上讲相当于关系数据库里的一张表，或R/Python里的数据框架，但内部有很多优化。 DataFrame能通过广泛的数据源构建，比如：结构化数据文件，Hive数据表，外部数据库或已有的RDDs。 使用SQLContext对象，应用程序能从已有RDD、从Hive数据表或从其它数据源创建DataFrame 2.DataFrame方法 入口：val sc = new SparkContext(conf) show：展示数据 1df.show() show(numRows: Int) ：显示numRows条 1df.show(2) show(truncate: Boolean) ：是否最多只显示20个字符，默认为true。 1df.show(false) show(numRows: Int, truncate: Boolean) ：综合前面的显示记录条数，以及对过长字符串的显示格式。 1df.show(2,false) collect：获取所有数据到数组，collect方法会将所有数据都获取到，并返回一个Array对象。 collectAsList：获取所有数据到List describe(cols: String*)：获取指定字段的统计信息，这个方法可以动态的传入一个或多个String类型的字段名，结果仍然为DataFrame对象，用于统计数值类型字段的统计值，比如count, mean, stddev, min, max等。 12345678910df.describe(&quot;name&quot;,&quot;age&quot;).show(false)+-------+---------------------------------+------------------+|summary|name |age |+-------+---------------------------------+------------------+|count |3 |2 ||mean |null |24.5 ||stddev |null |7.7781745930520225||min |Andy |19 ||max |Michaeabcdefghijklmnopqrstuvwxyxz|30 |+-------+---------------------------------+------------------+ first:获取第一行记录 12println(df.first())[null,Michaeabcdefghijklmnopqrstuvwxyxz] head:获取第一行记录，head(n: Int)获取前n行记录 123df.head(2).foreach(println)[null,Michaeabcdefghijklmnopqrstuvwxyxz][30,Andy] take(n: Int):获取前n行数据 123df.take(2).foreach(println)[null,Michaeabcdefghijklmnopqrstuvwxyxz][30,Andy] takeAsList(n: Int):获取前n行数据，并以List的形式展现 12println(df.takeAsList(2))[[null,Michaeabcdefghijklmnopqrstuvwxyxz], [30,Andy]] where(conditionExpr: String)：SQL语言中where关键字后的条件传入筛选条件表达式，可以用and和or 123456df.where(&quot;name=&apos;Andy&apos;&quot;).show(false)+---+----+|age|name|+---+----+|30 |Andy|+---+----+ filter：根据字段进行筛选 123456df.filter(&quot;name=&apos;Andy&apos;&quot;).show(false)+---+----+|age|name|+---+----+|30 |Andy|+---+----+ select：获取指定字段值,根据传入的String类型字段名，获取指定字段的值，以DataFrame类型返回 12345678df.select(&quot;name&quot;,&quot;age&quot;).show(false)+---------------------------------+----+|name |age |+---------------------------------+----+|Michaeabcdefghijklmnopqrstuvwxyxz|null||Andy |30 ||Justin |19 |+---------------------------------+----+ selectExpr：可以对指定字段进行特殊处理 ,可以直接对指定字段调用UDF函数，或者指定别名等。传入String类型参数，得到DataFrame对象。 12345678df.selectExpr(&quot;name&quot;,&quot;age as ageN&quot;,&quot;round(age)&quot;).show(false)+---------------------------------+----+-------------+|name |ageN|round(age, 0)|+---------------------------------+----+-------------+|Michaeabcdefghijklmnopqrstuvwxyxz|null|null ||Andy |30 |30 ||Justin |19 |19 |+---------------------------------+----+-------------+ col：获取指定字段 12println(df.col(&quot;name&quot;))name apply：获取指定字段 只能获取一个字段，返回对象为Column类型 12println(df.apply(&quot;name&quot;))name drop：去除指定字段，保留其他字段 ,返回一个新的DataFrame对象，其中不包含去除的字段，一次只能去除一个字段。 12345678df.drop(&quot;name&quot;).show(false)+----+|age |+----+|null||30 ||19 |+----+ limit:limit方法获取指定DataFrame的前n行记录，得到一个新的DataFrame对象。和take与head不同的是，limit方法不是Action操作。 1234567 df.limit(2).show() +----+--------------------+| age| name|+----+--------------------+|null|Michaeabcdefghijk...|| 30| Andy|+----+--------------------+ orderBy和sort：按指定字段排序，默认为升序,加个-表示降序排序 123456789101112131415161718192021222324df.orderBy(df.col(&quot;age&quot;)).show(false)+----+---------------------------------+|age |name |+----+---------------------------------+|null|Michaeabcdefghijklmnopqrstuvwxyxz||19 |Justin ||30 |Andy |+----+---------------------------------+df.orderBy(- df.col(&quot;age&quot;)).show(false)+----+---------------------------------+|age |name |+----+---------------------------------+|null|Michaeabcdefghijklmnopqrstuvwxyxz||30 |Andy ||19 |Justin |+----+---------------------------------+df.orderBy(df.col(&quot;age&quot;).desc).show(false)+----+---------------------------------+|age |name |+----+---------------------------------+|30 |Andy ||19 |Justin ||null|Michaeabcdefghijklmnopqrstuvwxyxz|+----+---------------------------------+ sortWithinPartitions :和上面的sort方法功能类似，区别在于sortWithinPartitions方法返回的是按Partition排好序的DataFrame对象。 group by:根据字段进行group by操作 ,groupBy方法有两种调用方式，可以传入String类型的字段名，也可传入Column类型的对象。 1df.groupBy(&quot;name&quot;) cube和rollup：group by的扩展 GroupedData对象 :该方法得到的是GroupedData类型对象，在GroupedData的API中提供了group by之后的操作,max(colNames: String)方法，获取分组中指定字段或者所有的数字类型字段的最大值，只能作用于数字型字段。min(colNames: String)方法，获取分组中指定字段或者所有的数字类型字段的最小值，只能作用于数字型字段。mean(colNames: String)方法，获取分组中指定字段或者所有的数字类型字段的平均值，只能作用于数字型字段。sum(colNames: String)方法，获取分组中指定字段或者所有的数字类型字段的和值，只能作用于数字型字段。count()方法，获取分组中的元素个数 distinct：返回一个不包含重复记录的DataFrame ,与dropDuplicates()方法不传入指定字段时的结果相同。 dropDuplicates：根据指定字段去重 ，类似于select distinct a, b操作 12df.distinct().show(false)df.dropDuplicates().show(false) 聚合：聚合操作调用的是agg方法，该方法有多种调用方式。一般与groupBy方法配合使用。如：对id字段求最大值，对c4字段求和。 unionALL：对两个DataFrame进行组合类似于SQL中的UNION ALL操作。 12345678910111213df.union(df1).show(false)+----+---------------------------------+|age |name |+----+---------------------------------+|null|Michaeabcdefghijklmnopqrstuvwxyxz||30 |Andy ||19 |Justin ||19 |Justin ||null|Michaeabcdefghijklmnopqrstuvwxyxz||30 |Andy ||19 |Justin ||19 |Justin |+----+---------------------------------+ join(笛卡尔积):DF1.join(DF2) join(using一个字段形式 ):下面这种join类似于a join b using column1的形式，需要两个DataFrame中有相同的一个列名， join(using多个字段形式):除了上面这种using一个字段的情况外，还可以using多个字段，如下 12345678910111213141516171819202122df.join(df1).show(false)df.join(df1,&quot;name&quot;).show(false)+---------------------------------+----+----+|name |age |age |+---------------------------------+----+----+|Michaeabcdefghijklmnopqrstuvwxyxz|null|null||Andy |30 |30 ||Justin |19 |19 ||Justin |19 |19 ||Justin |19 |19 ||Justin |19 |19 |+---------------------------------+----+----+df.join(df1,Seq(&quot;name&quot;,&quot;age&quot;)).show(false)+------+---+|name |age|+------+---+|Andy |30 ||Justin|19 ||Justin|19 ||Justin|19 ||Justin|19 |+------+---+ 指定join类型 :两个DataFrame的join操作有inner, outer, left_outer, right_outer, leftsemi类型。在上面的using多个字段的join情况下，可以写第三个String类型参数，指定join的类型 12345678910df.join(df1,Seq(&quot;name&quot;,&quot;age&quot;),&quot;inner&quot;).show(false)+------+---+|name |age|+------+---+|Andy |30 ||Justin|19 ||Justin|19 ||Justin|19 ||Justin|19 |+------+---+ 使用Column类型来join:如果不用using模式，灵活指定join字段的话，可以使用如下形式 1DF1.join(DF2 , DF1(&quot;id&quot; ) === DF2( &quot;t1_id&quot;)) 在指定join字段同时指定join类型: 1DF1.join(DF2 , DF1(&quot;id&quot; ) === DF2( &quot;t1_id&quot;), &quot;inner&quot;) stat:获取指定字段统计信息,stat方法可以用于计算指定字段或指定字段之间的统计信息，比如方差，协方差等。这个方法返回一个DataFramesStatFunctions类型对象。下面代码演示根据c4字段，统计该字段值出现频率在30%以上的内容。在jdbcDF中字段c1的内容为”a, b, a, c, d, b”。其中a和b出现的频率为2 / 6，大于0.3 1DF.stat.freqItems(Seq (&quot;c1&quot;) , 0.3).show() intersect:计算出两个DataFrame中相同的记录， except:获取一个DataFrame中有另一个DataFrame中没有的记录 withColumnRenamed:重命名DataFrame中的指定字段名 ,如果指定的字段名不存在，不进行任何操作 1DF.withColumnRenamed( &quot;id&quot; , &quot;idx&quot; ) withColumn:whtiColumn(colName: String , col: Column)方法根据指定colName往DataFrame中新增一列，如果colName已存在，则会覆盖当前列。 explode:行转列,有时候需要根据某个字段内容进行分割，然后生成多行，这时可以使用explode方法 ,下面代码中，根据c3字段中的空格将字段内容进行分割，分割的内容存储在新的字段c3_中，如下所示1DF.explode( &quot;c3&quot; , &quot;c3_&quot; )&#123;time: String =&gt; time.split( &quot; &quot; )&#125; 3.实例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.spark.sqlimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContext/** * 创建dataframe */object DataFrameCreate extends App&#123; val conf = new SparkConf() .setMaster(&quot;local&quot;) .setAppName(&quot;DataFrameCreate&quot;) //create a sc val sc = new SparkContext(conf) //create sqlcontext val sqlContext=new SQLContext(sc) //create a dataframe val fileName=&quot;E:\\\\spark\\\\src\\\\main\\\\resources\\\\people.json&quot; val df = sqlContext.read.json(fileName) val df1 = sqlContext.read.json(fileName) //print all the record ,including the head /*df.show() //print the schema info df.printSchema() //print name df.select(&quot;name&quot;, &quot;age&quot;).show() //print age &gt;20 df.filter(df.col(&quot;age&quot;)&gt;20).show //print name and age +1 df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;)+1).show() //group by age,and count df.groupBy(&quot;age&quot;).count().show()*/ //df.show() //df.show(2,false) //df.show(false) //df.describe(&quot;name&quot;,&quot;age&quot;).show(false) //println(df.first()) //df.head(2).foreach(println) //println(df.takeAsList(2)) //df.where(&quot;name=&apos;Andy&apos;&quot;).show(false) //df.filter(&quot;name=&apos;Andy&apos;&quot;).show(false) //df.select(&quot;name&quot;,&quot;age&quot;).show(false) //df.selectExpr(&quot;name&quot;,&quot;age as ageN&quot;,&quot;round(age)&quot;).show(false) //println(df.col(&quot;name&quot;)) //println(df.apply(&quot;name&quot;)) //df.drop(&quot;name&quot;).show(false) //df.limit(2).show() //df.orderBy(df.col(&quot;age&quot;)).show(false) //df.orderBy(- df.col(&quot;age&quot;)).show(false) //df.orderBy(df.col(&quot;age&quot;).desc).show(false) // println(df.groupBy(&quot;name&quot;)) //df.distinct().show(false) //df.dropDuplicates().show(false) //df.union(df1).show(false) //df.join(df1).show(false) //df.join(df1,&quot;name&quot;).show(false) df.join(df1,Seq(&quot;name&quot;,&quot;age&quot;),&quot;inner&quot;).show(false)&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"MySQL-ZIP包安装","slug":"Mysql-ZIP包安装20190524","date":"2017-03-11T06:52:31.000Z","updated":"2019-05-24T04:59:13.674Z","comments":true,"path":"2017/03/11/Mysql-ZIP包安装20190524/","link":"","permalink":"http://qioinglong.top/2017/03/11/Mysql-ZIP包安装20190524/","excerpt":"","text":"1.先将dos目录改成mysql的bin目录1cd D:\\Program Files\\mysql-5.7.17\\bin&gt; 2.修改mysql目录下的my-default.ini文件找到下面两行改成你的mysql目录12basedir=D:\\Program Files\\mysql-5.7.17\\ （mysql所在目录）datadir=D:\\Program Files\\mysql-5.7.17\\data （mysql所在目录\\data） 3.命令行：mysqld install1D:\\Program Files\\mysql-5.7.17\\bin&gt;mysqld.exe install 4.直接启动看看——net start mysql（失败了）123D:\\Program Files\\mysql-5.7.17\\net start mysqlMySQL 服务正在启动 .MySQL 服务无法启动。 5.执行一下命令1D:\\Program Files\\mysql-5.7.17\\bin&gt;mysqld --initialize-insecure 6.设置密码123456789101112131415161718D:\\Program Files\\mysql-5.7.17\\bin&gt;mysql -urootWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 4Server version: 5.7.17 MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; set password=&quot;root&quot; -&gt; ;Query OK, 0 rows affected (0.00 sec)mysql&gt;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://qioinglong.top/tags/MySQL/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"spark学习笔记1-wordCount","slug":"spark学习笔记1-wordCount20190524","date":"2017-03-10T02:54:59.000Z","updated":"2019-05-24T04:59:16.520Z","comments":true,"path":"2017/03/10/spark学习笔记1-wordCount20190524/","link":"","permalink":"http://qioinglong.top/2017/03/10/spark学习笔记1-wordCount20190524/","excerpt":"1.第一步创建SparkConf对象，设置Spark应用的配置信息，使用setMaster()可以设置Spark应用程序要连接的Spark集群的master节点的url，如果设置为”local”则代表在本地运行。 2.第二步创建SparkContext，在Spark中，SparkContext是Spark所有功能的一个入口，你无论是用java、scala，甚至是python编写，都必须要有一个SparkContext，它的主要作用，包括初始化Spark应用程序所需的一些核心组件，包括调度器（DAGSchedule、TaskScheduler），还会去到Spark Master节点上进行注册等等。在Spark中，编写不同类型的Spark应用程序，使用的SparkContext是不同的，如果使用scala，使用的就是原生的SparkContext对象,如果使用Java，那么就是JavaSparkContext对象,如果是开发Spark SQL程序，那么就是SQLContext、HiveContext.","text":"1.第一步创建SparkConf对象，设置Spark应用的配置信息，使用setMaster()可以设置Spark应用程序要连接的Spark集群的master节点的url，如果设置为”local”则代表在本地运行。 2.第二步创建SparkContext，在Spark中，SparkContext是Spark所有功能的一个入口，你无论是用java、scala，甚至是python编写，都必须要有一个SparkContext，它的主要作用，包括初始化Spark应用程序所需的一些核心组件，包括调度器（DAGSchedule、TaskScheduler），还会去到Spark Master节点上进行注册等等。在Spark中，编写不同类型的Spark应用程序，使用的SparkContext是不同的，如果使用scala，使用的就是原生的SparkContext对象,如果使用Java，那么就是JavaSparkContext对象,如果是开发Spark SQL程序，那么就是SQLContext、HiveContext. 3.第三步创建RDD，要针对输入源（hdfs文件、本地文件），创建一个初始的RDD，输入源中的数据会打散，分配到RDD的每个partition中，从而形成一个初始的分布式的数据集，SparkContext中，用于根据文件类型的输入源创建RDD的方法，叫做textFile()方法，在Java中，创建的普通RDD，都叫做JavaRDD，RDD中，有元素这种概念，如果是hdfs或者本地文件呢，创建的RDD，每一个元素就相当于是文件里的一行。 4.第四步对初始RDD进行transformation操作，也就是一些计算操作，通常操作会通过创建function，并配合RDD的map、flatMap等算子来执行。如果比较简单，则创建指定Function的匿名内部类，如果function比较复杂，则会单独创建一个类，作为实现这个function接口的类 5.第五步使用action操作的，来触发程序的执行。 6.实现代码1234567891011121314151617181920212223242526272829package com.spark.coreimport org.apache.spark.SparkConfimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject WordCount &#123; def main(args: Array[String]): Unit = &#123; //1.create sparkConf val conf = new SparkConf() //.setMaster(&quot;local&quot;) .setAppName(&quot;WordCount&quot;) //2.create sparkContext val sc = new SparkContext(conf) //3.create a rdd //val fileName=&quot;E:\\\\spark-scala-study\\\\spark-scala-study\\\\resouce\\\\README.md&quot; val fileName=&quot;hdfs://hadoop01:9000/spark-study/README.md&quot; val lines=sc.textFile(fileName) //每一行拆分成单个的单词 val words = lines.flatMap(line=&gt;line.split(&quot; &quot;)) //将每一个单词，映射为(单词, 1)的这种格式 val wordPairs=words.map(word=&gt;(word,1)) //单词作为key，统计每个单词出现的次数 val wordCount=wordPairs.reduceByKey(_+_) wordCount.foreach(wordCount=&gt;println(wordCount._1 + &quot; appear &quot; + wordCount._2 + &quot; times&quot;)) &#125;&#125; 7.原理图","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Flume安装","slug":"Flume安装20190524","date":"2017-03-09T12:54:59.000Z","updated":"2019-05-24T04:59:12.282Z","comments":true,"path":"2017/03/09/Flume安装20190524/","link":"","permalink":"http://qioinglong.top/2017/03/09/Flume安装20190524/","excerpt":"","text":"1.解压重命名12tar -xvf apache-flume-1.7.0-bin.tar.gzmv apache-flume-1.7.0-bin flume-1.7.0 2.配置环境变量1234#config flumeexport FLUME_HOME=/home/yangql/app/flume-1.7.0export FLUME_HOME_CONF=/home/yangql/app/flume-1.7.0/confexport PATH=$PATH:$FLUME_HOME/bin 2.配置flume.conf1cp flume-conf.properties.template flume.conf 加入以下内容 1234567891011121314151617181920# Define a memory channel called ch1 on agent1agent1.channels.ch1.type = memory# Define an Avro source called avro-source1 on agent1 and tell it# to bind to 0.0.0.0:41414. Connect it to channel ch1.agent1.sources.avro-source1.channels = ch1agent1.sources.avro-source1.type = avroagent1.sources.avro-source1.bind = 0.0.0.0agent1.sources.avro-source1.port = 41414# Define a logger sink that simply logs all events it receives# and connect it to the other end of the same channel.agent1.sinks.log-sink1.channel = ch1agent1.sinks.log-sink1.type = logger# Finally, now that we&apos;ve defined all of our components, tell# agent1 which ones we want to activate.agent1.channels = ch1agent1.sources = avro-source1agent1.sinks = log-sink1 4.启动 1bin/flume-ng agent --conf ./conf/ -f conf/flume.conf -Dflume.root.logger=DEBUG,console -n agent1","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://qioinglong.top/tags/Flume/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Kafka安装","slug":"Kafka安装20190524","date":"2017-03-08T16:54:59.000Z","updated":"2019-05-24T04:59:13.442Z","comments":true,"path":"2017/03/09/Kafka安装20190524/","link":"","permalink":"http://qioinglong.top/2017/03/09/Kafka安装20190524/","excerpt":"1.解压重命名12tar -xvf kafka_2.12-0.10.2.0.tgzmv kafka_2.12-0.10.2.0 kafka-2.12 2.配置环境变量123#config kafka 2.12export KAFKA_HOME=/home/yangql/app/kafka-2.12export PATH=$PATH:$KAFKA_HOME/bin","text":"1.解压重命名12tar -xvf kafka_2.12-0.10.2.0.tgzmv kafka_2.12-0.10.2.0 kafka-2.12 2.配置环境变量123#config kafka 2.12export KAFKA_HOME=/home/yangql/app/kafka-2.12export PATH=$PATH:$KAFKA_HOME/bin 3.修改server.properties broker.id=0listeners = PLAINTEXT://hadoop01:9092advertised.listeners=PLAINTEXT://hadoop01:9092log.dirs=/home/yangql/app/kafka-2.12/logszookeeper.connect=hadoop01:2181,hadoop02:2181,hadoop03:2181 4.同步12scp -r kafka-2.12/ yangql@hadoop02:/home/yangql/app/scp -r kafka-2.12/ yangql@hadoop03:/home/yangql/app/ 5.修改node1123broker.id=1listeners = PLAINTEXT://hadoop02:9092advertised.listeners=PLAINTEXT://hadoop02:9092 6.修改node2123broker.id=2listeners = PLAINTEXT://hadoop03:9092advertised.listeners=PLAINTEXT://hadoop03:9092 因为Kafka集群需要保证各个Broker的id在整个集群中必须唯一，需要调整这个配置项的值。 7.启动1./kafka-server-start.sh -daemon /home/yangql/app/kafka-2.12/config/server.properties &amp; 8.测试在namenode上创建mytest主题（kafka有几个，replication-factor就填几个）1./kafka-topics.sh --create --topic mytest --replication-factor 3 --partitions 2 --zookeeper hadoop01:2181 在namenode上查看刚才创建的mytest主题1./kafka-topics.sh --list --zookeeper hadoop01:2181 在datanode1上发送消息至kafka，发送消息“this is for test”1./kafka-console-producer.sh --broker-list hadoop01:9092 --sync --topic mytest 在datanode2上开启一个消费者，模拟consumer，可以看到刚才发送的消息1./kafka-console-consumer.sh --zookeeper hadoop01:2181 --topic mytest --from-beginning","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://qioinglong.top/tags/Kafka/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark SQL on hive配置和实战","slug":"Spark-SQL-on-hive配置和实战20190524","date":"2017-03-04T00:54:59.000Z","updated":"2019-05-24T04:59:15.145Z","comments":true,"path":"2017/03/04/Spark-SQL-on-hive配置和实战20190524/","link":"","permalink":"http://qioinglong.top/2017/03/04/Spark-SQL-on-hive配置和实战20190524/","excerpt":"1.安装配置Hive ##在Spark SQL下安装配置Hive并将元数据保存到数据库Mysql中，详见文档","text":"1.安装配置Hive ##在Spark SQL下安装配置Hive并将元数据保存到数据库Mysql中，详见文档 2.在Spark SQL下配置hive-site.xml在master1上的spark-2.1.0-bin-hadoop2.7/conf目录创建hive-site.xml文件，内容如下：1234567&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://master1:9083&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 3.配置驱动将$HIVE_HOME/lib/mysql-connector-java-5.1.40-bin.jar 中的mysql驱动拷贝到$SPARK_HOME/jars/下面即可。 4.启动hive的metastore后台进程hive —service metastore &gt;&gt; /home/yangql/app/hive-2.1.1/logs/metastore.log 2&gt;&amp; 1&amp; 5.验证 ./spark-shell —master spark://hadoop01:7077 val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc) hiveContext.sql(“show databases”).collect.foreach(println)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark SQL on hive配置和实战","slug":"Spark SQL on hive配置和实战20190524","date":"2017-03-04T00:54:59.000Z","updated":"2019-05-24T04:59:14.821Z","comments":true,"path":"2017/03/04/Spark SQL on hive配置和实战20190524/","link":"","permalink":"http://qioinglong.top/2017/03/04/Spark SQL on hive配置和实战20190524/","excerpt":"1.安装配置Hive ##在Spark SQL下安装配置Hive并将元数据保存到数据库Mysql中，详见文档","text":"1.安装配置Hive ##在Spark SQL下安装配置Hive并将元数据保存到数据库Mysql中，详见文档 2.在Spark SQL下配置hive-site.xml在master1上的spark-2.1.0-bin-hadoop2.7/conf目录创建hive-site.xml文件，内容如下：1234567&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://master1:9083&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 3.配置驱动将$HIVE_HOME/lib/mysql-connector-java-5.1.40-bin.jar 中的mysql驱动拷贝到$SPARK_HOME/jars/下面即可。 4.启动hive的metastore后台进程hive —service metastore &gt;&gt; /home/yangql/app/hive-2.1.1/logs/metastore.log 2&gt;&amp; 1&amp; 5.验证 ./spark-shell —master spark://hadoop01:7077 val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc) hiveContext.sql(“show databases”).collect.foreach(println)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"class与Case class的区别","slug":"Scala class与Case class的区别20190524","date":"2017-03-03T09:54:59.000Z","updated":"2019-05-24T04:59:14.150Z","comments":true,"path":"2017/03/03/Scala class与Case class的区别20190524/","link":"","permalink":"http://qioinglong.top/2017/03/03/Scala class与Case class的区别20190524/","excerpt":"","text":"在Scala中存在case class，它其实就是一个普通的class。但是它又和普通的class略有区别，如下： 初始化的时候可以不用new，当然你也可以加上，普通类一定需要加new toString的实现更漂亮 默认实现了equals 和 hashCode 默认是可以序列化的，也就是实现了Serializable 自动从scala.Product中继承一些函数 case class构造函数的参数是public级别的，我们可以直接访问 支持模式匹配","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala集合","slug":"Scala集合20190524","date":"2017-03-02T16:54:59.000Z","updated":"2019-05-24T04:59:14.372Z","comments":true,"path":"2017/03/03/Scala集合20190524/","link":"","permalink":"http://qioinglong.top/2017/03/03/Scala集合20190524/","excerpt":"","text":"数组是一种可变的、可索引的数据集合。在Scala中用Array[T]的形式来表示Java中的数组形式 T[]。以下介绍各种集合操作方法。 1. ++合并集合，并返回一个新的数组，新数组包含左右两个集合对象的内容。1def ++[B](that: GenTraversableOnce[B]): Array[B] 123456val a = Array(1,2)val b = Array(3,4)val c = a ++ bprintln(c.mkString(&quot;,&quot;))//output1,2,3,4 2. ++:合并集合，并返回一个新的数组，新数组包含左右两个集合对象的内容。但是不同的是右边操纵数的类型决定着返回结果的类型1def ++:[B &gt;: A, That](that: collection.Traversable[B])(implicit bf: CanBuildFrom[Array[T], B, That]): That 12345678val a = List(1,2)val b = scala.collection.mutable.LinkedList(3,4)val c = a ++: bprintln(c.mkString(&quot;,&quot;))println(c.getClass().getName())//output1,2,3,4scala.collection.mutable.LinkedList 3. +:在数组前面添加一个元素，并返回新的对象1def +:(elem: A): Array[A] 123val a = List(1,2)val c = 0 +: aprintln(c.mkString(&quot;,&quot;)) 4. :+在数组末尾添加一个元素，并返回新对象1def :+(elem: A): Array[A] 12345val a = List(1,2)val c = a :+ 0println(c.mkString(&quot;,&quot;))//output1,2,0 5. foldfold函数是将一种格式的输入数据转换成另外一种格式，fold函数需要输入两个参数，初始值，以及一个函数，输入的函数也需要两个参数，累加值和当前item的索引。12345678910val numbers = Array(1, 2, 3, 4) val result =numbers.fold(0)(&#123; (z,i)=&gt;&#123; z-i &#125; &#125;) println(result)//output-10// 代码开始运行时，初始值0作为第一个参数传入到fold函数中，array中的第一个item作为第二个参数传入到fold函数中 fold函数开始对传进的参数进行计算，此处是加法计算，然后返回值 fold函数将上一步返回的值作为第一个参数，并把array中的第二个item作为参数传进去继续计算，同样返回值。 重复上一步计算，知道array中所有元素都遍历完。应用例子: 1234567891011121314151617181920212223242526package com.scala.baseclass Foo(val name:String,val age:Int,val sex:Symbol) &#123;&#125;object Foo&#123; def apply(name:String,age:Int,sex:Symbol)=new Foo(name,age,sex)&#125;var fooList=Foo(&quot;name1&quot;,10,&apos;male):: Foo(&quot;name2&quot;,10,&apos;female):: Foo(&quot;name3&quot;,10,&apos;male)::Nil var stringList=fooList.foldLeft(List[String]())( (z,f)=&gt;&#123; var title=f.sex match &#123; case &apos;male =&gt; &quot;Mr.&quot; case &apos;female =&gt; &quot;Ms.&quot; &#125; z :+ s&quot;$title $&#123;f.name&#125; $&#123;f.age&#125;&quot; &#125; ) println(stringList) //初始值 空list //output //List(Mr. name1 10, Ms. name2 10, Mr. name3 10) 6. /: foldLeft对数组中所有的元素进行相同的操作 ，foldLeft 的简写1def /:[B](z: B)(op: (B, T) ? B): B 12345678val a = List(1,2,3,4)val c = (10 /: a)(_+_) val d = (10 /: a)(_*_) println(&quot;c:&quot;+c) println(&quot;d:&quot;+d) //outputc:20d:240 7. :\\ foldRight对数组中所有的元素进行相同的操作 ,foldRight 的简写 8.fold foldLeft foldRight 区别 foldLeft从左开始计算，然后往右遍历 foldRight从右开始计算，然后往左遍历 fold遍历的顺序没有特殊的次序，但初始化参数和返回值都有限制，一：初始值的类型必须是list元素中元素类型的超类，二：初始值必须是中立的，也就是它不能改变结果，比如加法，中立值为0，乘法中立值为1，数组来说是Nil 三个函数中，初始化和返回值得参数类型必须相同&lt;=未完待续=&gt;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive-《Hive简明教程》学习","slug":"Hive-《Hive简明教程》学习20190524","date":"2017-03-02T00:54:59.000Z","updated":"2019-05-24T04:59:12.704Z","comments":true,"path":"2017/03/02/Hive-《Hive简明教程》学习20190524/","link":"","permalink":"http://qioinglong.top/2017/03/02/Hive-《Hive简明教程》学习20190524/","excerpt":"1.Hive介绍Hive是Facebook为了解决海量日志数据的分析而开发的。是一种用SQL来协助读写、管理存储在分布式存储系统上的大数据集的数据仓库软件，主要有以下几个特点: 通过类SQL来分析大数据，避免了写MapReduce来分析数据，这样使得数据分析更容易。 数据是存储在HDFS上的，Hive本身并不存储数据 Hive将数据映射成数据库和一张张表，库和表的元数据一般存储在关系数据库中。 能存储很大的数据集，并对数据的完整性，格式要求不严格。 不适用与实时计算和响应，使用于离线分析。注：后续搭建HUE平台","text":"1.Hive介绍Hive是Facebook为了解决海量日志数据的分析而开发的。是一种用SQL来协助读写、管理存储在分布式存储系统上的大数据集的数据仓库软件，主要有以下几个特点: 通过类SQL来分析大数据，避免了写MapReduce来分析数据，这样使得数据分析更容易。 数据是存储在HDFS上的，Hive本身并不存储数据 Hive将数据映射成数据库和一张张表，库和表的元数据一般存储在关系数据库中。 能存储很大的数据集，并对数据的完整性，格式要求不严格。 不适用与实时计算和响应，使用于离线分析。注：后续搭建HUE平台 2.Hive基本数据类型Hive支持大多数的数据类型： 数据类型 长度 备注 TinyInt 1字节的有符号整数 -128~127 SmallINt 2字节的有符号整数 -32768~32767 Int 4字节的有符号整数 BigInt 8字节的有符号整数 Boolean 布尔类型 true,false Float 单精度 Double 双进度 String 字符串 Timestamp Binary 字节数组 Date 日期 3.Hive DDL 数据定义3.1.创建数据库创建一个数据库就是在HDFS上创建一个目录，数据库类似命名空间来组织表，在大量Hive表的情况下，避免表名冲突，默认数据库是default创建数据库1create database if not exists dealer_db; 3.2.查看数据库定义12345hive&gt; desc database dealer_db;OKdealer_db hdfs://hadoop01:9000/user/hive/warehouse/dealer_db.dyangql USERTime taken: 0.69 seconds, Fetched: 1 row(s)hive&gt; 3.3.查看数据库列表12345678hive&gt; show databases;OKdealer_dbdefaulthellopacteraTime taken: 0.05 seconds, Fetched: 4 row(s)hive&gt; 3.4.删除数据库删除数据库时，如果库中存在表，是不能删除的，要先删除所有表，再删除数据库。添加上参数 cascase后，就可以先自动删除表，再删除数据库，删除数据库后，HDFS上数据库对应的目录就被删除了。12drop database test;drop database test cascade; 3.5.切换数据库1use dealer_db; 3.6.创建普通表row format delimited fields terminated by ‘\\t’,指定列之间的分隔符，stored as textfile指定文件的存储格式为textfile，创建表的集中方式： create table create table as select :根据查询结果创建表，并将查询结果数据插入到信件的表中。 create table like table_name :克隆表，只复制table_name的表结构。123456789create table if not exists dealerinfo(dealerid int,dealername string,cityid int,createtime date)row format delimitedfields terminated by &apos;\\t&apos;stored as textfile 3.7.创建分区表Hive查询一般是扫描整个目录，但有时我们关心的数据只是集中在某一部分数据上，比如我们查询某一天的数据时，可以使用分区表来优化，一天一个分区，Hive只扫描指定天分区的数据。普通表和分区表的区别在于，一个Hive表在HDFS上有一个对应的目录来存储数据，普通表的数据直接存储在这个目录下，而分区表数据存储时，是再划分子目录来存储的。一个分区一个子目录。主要的作用是用来查询优化性能。123456789create table dealer_log(companyid int comment &apos;commpany id&apos;,userid int comment &apos;user id&apos;,invisttime string comment &apos;invist time&apos; )partitioned by (dt string)row format delimitedfields terminated by &apos;\\t&apos;stored as textfile 在上面列子中，这个日志表是以dt字段分区，dt是个虚拟字段，dt下并未存储数据，而是用来分区的，实际数据存储时，dt字段一样的值存入同一个子目录中。所以对于分区表，尽量添加分区字段来过滤筛选。 3.8.创建桶表桶表也是一种用于优化查询而设计的表，创建普通表时，指定桶的个数，分桶的依据字段，hive就会自动将数据分桶存储，查询时只需要遍历一个桶里的数据，或者遍历部分桶，提高查询效率。1234567891011create table dealer_leads(leads_id string,dealer_id string,user_id string,user_phone string,create_time string)clustered by(dealer_id) sorted by(leads_id) into 10 Bucketsrow format delimitedfields terminated by &apos;\\t&apos;stored as textfile clusterd by 是指根据dealer_id进行hash后mo模除分桶个数，根据得到的结果，确定这行数据分入到那个桶中，这样的分发，可以确保相同的dealer_id的数据放入到同一个桶中，而dealer_id的数据，大部分根据dealer_id查询，这样大部分情况下只需要查询一个桶中的数据就行了。 sorted by是指根据桶中的那个字段进行排序，排序的好处，就是join操作的时候能获得很高的效率。 into 10 Buckets 是指一共分多少桶 在HDFS上存储时，一个桶存入一个文件中，这样根据dealer_id查询时，可以快速确定数据存在与那个桶中，遍历一个桶可以提高查询速度。 3.9.查看表 查看库中所有表 12show tables ;show tables &apos;*info&apos;;//可以用正则表达式筛选 查看表的详情 12desc dealer_log;desc formatted dealer_log; 3.10.修改表修改表包括修改表名，添加字段，修改字段。 修改表名 1alter table dealerinfo rename to dealer_info; 添加字段 1alter table dealer_info add columns(provinceid int); 修改字段 1alter table dealer_info change dealerid dealer_id int; 3.11.删除表1drop table if exists test; 4.Hive DML数据管理4.1.将数据加载到普通表可以将本地数据文件批量加载到Hive表中，要求文本中的格式要与Hive表的定义一致，包括：字段个数，顺序，列分隔符1load data local inpath &apos;/home/yangql/user_tab_comments.txt&apos; overwrite into table user_tab_comments local 关键字表示源数据文件在本地，源文件可以在HDFS上，如果在HDFS上，则去掉local，inpath后面的路径类似：hdfs://namenode:9000/user/datapath 1hive&gt; load data inpath &apos;/input/user_tab_comments.txt&apos; overwrite into table user_tab_comments; overwrite表示如果hive表中有数据，就会覆盖掉原有的数据，如果省略掉overwrite,默认是追加数据。 4.2.将数据加载到分区表1load data inpath &apos;hdfs://hadoop01:9000/input/yangql/dealer_log.txt&apos; overwrite into table dealer_log partition(dt=&apos;2017-03-03&apos;) 4.3.将数据加载到分桶表 先创建普通临时表 12345678910create table dealer_leads_tmp( leads_id string, dealer_id string, user_id string, user_phone string, create_time string ) row format delimited fields terminated by &apos;\\t&apos; stored as textfile; 载入临时表 1load data local inpath &apos;/home/yangql/dealer_leads.txt&apos; overwrite into table dealer_leads_tmp 导入分桶表 12set hive.enforce.bucketing = true;insert overwrite table dealer_leads select * from dealer_leads_tmp; 4.4.数据导出 导出数据，将Hive表中的数据导出到本地文件中 1insert overwrite local directory &apos;/home/yangql/dealer_leads-20170302.txt&apos; select * from dealer_leads; 导出到HDFS 1insert overwrite directory &apos;/input/yangql/dealer_leads-20170302.txt&apos; select * from dealer_leads; 4.5.插入数据 inset select(overwrite:覆盖，去掉后表示追加数据) 1insert overwrite table_name select * from table_name; 插入分区表 1insert overwrite table dealer_log partition(dt=&apos;2016-10-21&apos;) select * from dealer_leads_tmp; 一次遍历，多次插入 123from dealer_loginsert overwrite table log1 partition(dt=&apos;2017-03-04&apos;) select companyid,userid,invisttimeinsert overwrite table log2 partition(dt=&apos;2017-03-04&apos;) select companyid,userid,invisttime; 复制表复制表可以将表的结构和数据复制并创建为一个新表，复制过程中，可以对数据进行筛选，列可以进行删减。 123456create table dealer_logbakrow format delimitedfields terminated by &apos;\\t&apos;stored as textfileasselect * from dealer_log 克隆表(只有表结构，不含具体数据) 1create table dealer_logbak01 like dealer_log; 备份表（备份表是将表的元数据和数据都导出到HDFS） 1export table dealer_log to &apos;/input/yangql/dealer_log_bak.export&apos; 还原表 1import table dealer_log_0302 from &apos;/input/yangql/dealer_log_bak.export&apos; 5.HiveQL 数据查询语法数据查询很多跟关系数据库一样，这里只列举部分： 限制查询条数 1select * from table_name limit 10; RLike 正则表达式匹配 Hive对子查询的支持有限，只允许在select from 后面出现。 Hive join 连接只能支持等值连接（on a.id=b.id），不支持不等值连接(on a.id!=b.id)。支持inner join ,left join ,right join ,full join 连接谓词中不支持 or left semi-join(Hive sql 中不存在exists)，左半开连接，但是select后面的列只能是左边的列，不能有右边的列。 排序，order by 。order by这样操作，肯定是Map后汇总到一个reduce上执行，如果数据量打大，会造成reduce执行过程相当长，所以，Hive中尽量不用Order by，除非能确认数据量很小。 sort by：sort by是在每个reduce中进行排序，是一个局部排序，可以保证每个reduce中是进行排序好的。但从全局来看不一定是排序的。 distribute by 和sort by：distribute by是指定map输出结果怎样划分后分配到各个reduce上去。然后再指定sort by。这种也不能做到全局排序，只能保证排序字段值相同的放在一起，并且在reduce局部上是排序好的。distribute by 必须在sort by 之前。 cluster by：如果distribute by和sort by的字段是同一个，那可以用cluster by来替换。 自定义函数：Java编写，继承UDF类并实现evaluate() 7.hive-架构 (1) Hive 的核心是驱动引擎： 解释器：将Hive SQL 转化为语法树（AST） 编译器：将语法树编译为逻辑执行计划 优化器：将逻辑执行计划进行优化 执行器：调用底层的运行框架执行逻辑执行计划 (2) Hive 的底层存储Hive的数据是存储在HDFS上的，Hive中的库和表可以看作是对HDFS上的数据的一个映射，Hive运行在Hadoop集群上。(3) Hive 程序的执行过程Hive中的执行器，是将最终要执行的MapReduce程序放在Yarn上以一系列的方式去执行。(4) Hive的元数据一般是存储在Mysql这种关系数据库上的。Hive和Mysql通过metaStore服务交互。 8.Hive SQL 优化 利用分区表优化 利用桶表优化 join优化 (1)优先过滤后再join，最大限度的减少参与Join的数据量。(2)小表JOIN大表的原则，应该遵守小表JOIN大表的原则，原因是Join操作的reduce阶段，位于Join左边的表的内荣会被加载近内存，将条目少的放在左边，可以减少OOM发生的几率。JOIN中执行顺序是从左到右生成Job。(3)Hive中，当多个表进行Join时，如果join on 条件相同，那么他们会合并为一个Mapreduce，所以利用这个特性，可以将相同的join on的放入一个job来节省执行时间。(4)启用mapjoin，mapjoin是将join双方比较小的表直接分发到各个map进程的内存中。在map进程中进行map操作，这样就省掉了reduce步骤，提高了速度。(5)桶表mapjoin(6)group by 数据倾斜优化：Group By 很容易导致数据倾斜问题，因为实际业务中，通常是数据集中在某些点上，这也符合常见的 2/8 原则，这样会造成对数据分组后，某一些分组上数据量非常大，而其他的分组上数据量很小，而在 mapreduce 程序中，同一个分组的数据会分配到同一个 reduce 操作上去， 导致某一些 reduce 压力很大，其他的 reduce 压力很小，这就是数据倾斜， 整个 job 执行时间取决于那个执行最慢的那个 reduce。解决这个问题的方法是配置一个参数： set hive.groupby.skewindata=true。当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中， Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的GroupBy Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。(7)order by优化： order by 只能是在一个 reduce 进程中进行的，所以如果对一个大数据集进行 order by,会导致一个 reduce 进程中处理的数据相当大， 造成查询执行超级缓慢。 在要有进行 order by 全局排序的需求时， 用以下几个措施优化：(1) 在最终结果上进行 order by，不要在中间的大数据集上进行排序。 如果最终结果较少， 可以在一个 reduce 上进行排序时，那么就在最后的结果集上进行 order by。(2) 如果需求是取排序后前 N 条数据， 那么可以使用 distribute by 和 sort by 在各个 reduce 上进行排序后取前 N 条， 然后再对各个 reduce 的结果集合并后在一个 reduce 中全局排序， 再取前 N 条， 因为参与全局排序的 Order By 的数据量最多有 reduce 个数*N12345select a.leads_id,a.user_name from(select leads_id,user_name from dealer_leadsdistribute by length(user_name) sort by length(user_name) desc limit 10) a order by length(a.user_name) desc limit 10; (8)Group By Map 端聚合并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。 hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True。 hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目 (9)一次读取多次插入(10)Join 字段显示类型转换(11)使用 orc、 parquet 等列式存储格式创建表时，尽量使用 orc、 parquet 这些列式存储格式， 因为列式存储的表， 每一列的数据在物理上是存储在一起的， Hive 查询时会只遍历需要列数据， 大大减少处理的数据量 9.Hive文件格式Hive 中的文件格式常见的有： textfile 文件格式、 sequencefile 二进制序列化文件格式、 rcfile、orc、 parquet。 hive 表的文件格式一般是在创建表时用 stored as 语句声明，其中 textfile 和 sequencefile 是以行存储数据的， rcfile、 orc、 parquet 是列式存储的。 9.1.TextFile 格式TextFile 是 Hive 的默认文件格式， 数据不做压缩， 磁盘开销比较大， 数据解析时开销也比较大。 从本地文件向 Hive load 数据只能用 textfile 文件格式。 9.2.SequenceFile 格式SequenceFile 是 Hadoop API 提供的一种二进制文件支持， 其具有使用方便、 可分割、 可压缩的特点。 9.3.Rcfile 格式Rcfile 是一种行列存储结合的存储方式， 首先将数据按行分块， 保证同一个记录在一个块上， 避免读取一行记录需要读取多个块的情况， 然后块数据列式存储，这样有利于数据压缩和列存取。 9.4.Orc 格式Orc 格式是 Rcfile 格式的升级版， 性能有很大的提升， 并且数据可以压缩存储， 比 textfile 文件压缩比可以达到 70%，同时读取性能也非常高，推荐使用 orc 文件格式创建表。 9.5。Parquet 格式Parquet 是一种适合多种计算框架的文件格式， Parquet 是语言无关的，并且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与 parquet 配合的组件有：查询引擎： Hive、 Impala、 Pig、 Presto、 Drill、 Tajo、 HAWQ、 IBM Big SQL。计算框架： MapReduce、 Spark、 Cascading、 Crunch、 Scalding、 Kite所以如果一套数据要多种引擎使用， Parquet 是最好的选择。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive数据库创建、表创建、加载数据","slug":"Hive数据库创建、表创建、加载数据20190524","date":"2017-02-28T04:54:59.000Z","updated":"2019-05-24T04:59:13.303Z","comments":true,"path":"2017/02/28/Hive数据库创建、表创建、加载数据20190524/","link":"","permalink":"http://qioinglong.top/2017/02/28/Hive数据库创建、表创建、加载数据20190524/","excerpt":"1.数据库1.1.创建数据库在Hive数据库是一个命名空间或表的集合。dbproperties,按照键值对的方式增加文档的说明。此语法声明如下：12CREATE DATABASE|SCHEMA [IF NOT EXISTS] &lt;database name&gt;with dbproperties(key=value,key=value) 在这里，IF NOT EXISTS是一个可选子句，通知用户已经存在相同名称的数据库。执行创建一个名为pactera数据库：","text":"1.数据库1.1.创建数据库在Hive数据库是一个命名空间或表的集合。dbproperties,按照键值对的方式增加文档的说明。此语法声明如下：12CREATE DATABASE|SCHEMA [IF NOT EXISTS] &lt;database name&gt;with dbproperties(key=value,key=value) 在这里，IF NOT EXISTS是一个可选子句，通知用户已经存在相同名称的数据库。执行创建一个名为pactera数据库：1CREATE DATABASE IF NOT EXISTS pactera with dbproperties(&apos;creator&apos;=&apos;yangql&apos;,&apos;date&apos;=&apos;2017-02-28&apos;); 或者1create schema if not EXISTS pactera; 查询数据库1234567hive&gt; show databases;OKdefaulthellopacteraTime taken: 0.083 seconds, Fetched: 3 row(s)hive&gt; 1.2.删除数据库删除数据库声明语法：12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name[RESTRICT|CASCADE]; 执行删除刚创建的数据库pactera1DROP DATABASE IF EXISTS pactera; 或者级联删除数据库(当数据库还有表时，级联删除表后在删除数据库),默认是restrict1DROP DATABASE IF EXISTS pactera CASCADE; 1.3.查看数据库相关信息查看数据库的描述信息和文件目录位置路径信息1hive&gt; desc database pactera; 查看数据库的描述信息和文件目录位置路径信息,加上数据库键值对的属性信息1desc database extended pactera; 1.4.修改数据库只能修改数据库的键值对属性值。数据库名和数据库所在的目录位置不能修改1alter database pactera set dbproperties(&apos;edited-by&apos;=&apos;yangql&apos;); 2.创建表 tblproperties：按照键值对的格式为表增加额外的文档说明，也可用来表示数据库连接的必要的元数据信息 hive会自动增加二个表属性：last_modified_by(最后修改表的用户名)，last_modified_time(最后一次修改的时间)2.1.内部表 创建表实例 123create table if not exists pactera.t1(name string comment &apos;your name&apos;,salary float comment &apos;salary&apos;)comment &apos;this is table for test&apos;tblproperties(&apos;creator&apos;=&apos;yangql&apos;,&apos;created_at&apos;=&apos;2014-11-13 09:50:33&apos;); 查看和列举表的tblproperties属性信息 1show tblproperties t1; 使用like在创建表的时候，拷贝表模式(而无需拷贝数据) 1create table if not exists pactera.t2 like pactera.t1; 查看表的详细结构信息（也可以显示表是管理表，还是外部表。还有分区信息） 1desc extended pactera.t1; 使用formatted信息更多些，可读性更强 1desc formatted pactera.t1; 2.2.外部表删除外部表时，表的元数据会被删除掉，但是数据不会被删除。如果数据被多个工具共享，可以创建外部表。示例12345create external table if not exists pactera.t2(name string comment &apos;name&apos;,salary float comment &apos;salary&apos;)comment &apos;this is a test table &apos;tblproperties(&apos;creator&apos;=&apos;yangql&apos;,&apos;created_at&apos;=&apos;2017-03-13 09:50:33&apos;) 2.3.分区表1234567create table if not exists pactera.t3(name string comment &apos;name&apos;,salary float comment &apos;salary&apos;)comment &apos;this is a test table&apos;partitioned by(country string,state string)STORED AS rcfiletblproperties(&apos;creator&apos;=&apos;yangql&apos;,&apos;created_at&apos;=&apos;2014-11-13 09:50:33&apos;) —查看表中存在的所有分区1show partitions table_name; —查看表中特定分区1show partitions table_name partition(country=’US’); 3.删除，修改表 删除表 1drop table if exists table_name; 修改表-表重命名 1alter table old_table_name rename to new_table_name; 增加分区 1alter table table_name add if not exists partition(year=2011,month=1,day=1) 修改分区存储路径 12alter table table_name partition(year=2011,month=1,day=2)set location ‘/logs/2011/01/02’; 删除某个分区 1alter table table_name drop if exists partition(year=2011,month=1,day=2); 修改列信息,after:字段移到severity字段之后,first:移动到第一个位置 1234alter table table_namechange column old_name new_name intcomment &apos;this is comment&apos;after severity; 增加列 1alter table table_name add columns(app_name string comment &apos;application name&apos;); 删除或者替换列 1alter table table_name replace columns(hms int comment &apos;hhh&apos;); 修改表属性 1alter table table_name set tblproperties(&apos;notes&apos;=&apos;this is a notes&apos;); 修改存储属性 1alter table table_name partition(year=2011,month=1,day=1) set fileformat sequencefile;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"User yangql is not allowed to impersonate anonymous","slug":"Hive-User yangql is not allowed to impersonate anonymous20190524","date":"2017-02-26T07:54:59.000Z","updated":"2019-05-24T04:59:12.906Z","comments":true,"path":"2017/02/26/Hive-User yangql is not allowed to impersonate anonymous20190524/","link":"","permalink":"http://qioinglong.top/2017/02/26/Hive-User yangql is not allowed to impersonate anonymous20190524/","excerpt":"使用HiveServer2 and Beeline模式运行时，启动好HiveServer后运行1beeline -u jdbc:hive2://localhost:10000 -n root 连接server时出现错误:12java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException):User yangql is not allowed to impersonate anonymous","text":"使用HiveServer2 and Beeline模式运行时，启动好HiveServer后运行1beeline -u jdbc:hive2://localhost:10000 -n root 连接server时出现错误:12java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException):User yangql is not allowed to impersonate anonymous 修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项【其中yangql是用户名，可以是自己的用户名如zhangsan】: &lt;property&gt; &lt;name&gt;hadoop.proxyuser.yangql.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.yangql.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; 重启hadoop","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive权限控制","slug":"Hive权限控制20190524","date":"2017-02-25T04:54:59.000Z","updated":"2019-05-24T04:59:13.250Z","comments":true,"path":"2017/02/25/Hive权限控制20190524/","link":"","permalink":"http://qioinglong.top/2017/02/25/Hive权限控制20190524/","excerpt":"目前hive支持简单的权限管理，默认情况下是不开启，这样所有的用户都具有相同的权限，同时也是超级管理员，也就对hive中的所有表都有查看和改动的权利。Hive本身也不提供创建用户组和用户的命令,Hive用户组和用户即Linux用户组和用户，和hadoop一样，本身不提供用户组和用户管理，只做权限控制。","text":"目前hive支持简单的权限管理，默认情况下是不开启，这样所有的用户都具有相同的权限，同时也是超级管理员，也就对hive中的所有表都有查看和改动的权利。Hive本身也不提供创建用户组和用户的命令,Hive用户组和用户即Linux用户组和用户，和hadoop一样，本身不提供用户组和用户管理，只做权限控制。 1.启用权限控制开启启身份认证后，任何用户必须被grant privilege才能对实体进行操作。在hive-site.xml文件中进行相应的配置12345678910&lt;property&gt;&lt;name&gt;hive.security.authorization.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;enable or disable the hive clientauthorization&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.security.authorization.createtable.owner.grants&lt;/name&gt;&lt;value&gt;ALL&lt;/value&gt;&lt;description&gt;the privileges automatically granted to the ownerwhenever a table gets created. An example like &quot;select,drop&quot; willgrant select and drop privilege to the owner of the table&lt;/description&gt;&lt;/property&gt; 2.角色管理123456789101112131415161718--创建和删除角色 create role role_name; drop role role_name; --展示所有roles show roles --赋予角色权限 grant select on database db_name to role role_name; grant select on [table] t_name to role role_name; --查看角色权限 show grant role role_name on database db_name; show grant role role_name on [table] t_name; --角色赋予用户 grant role role_name to user user_name --回收角色权限 revoke select on database db_name from role role_name; revoke select on [table] t_name from role role_name; --查看某个用户所有角色 show role grant user user_name; 3.HIVE支持的权限 权限名称 含义 ALL 所有权限 ALTER 允许修改元数据（modify metadata data of object）—-表信息数据 UPDATE 允许修改物理数据（modify physical data of object）—-实际数据 CREATE 允许进行Create操作 DROP 允许进行DROP操作 INDEX 允许建索引（目前还没有实现） LOCK 当出现并发的使用允许用户进行LOCK和UNLOCK操作 SELECT 允许用户进行SELECT操作 SHOW_DATABASE 允许用户查看可用的数据库 4.与角色相关的表 表名 含义 Db_privs 记录了User/Role在DB上的权限 Tbl_privs 记录了User/Role在table上的权限 Tbl_col_privs 记录了User/Role在table column上的权限 Roles 记录了所有创建的role Role_map 记录了User与Role的对应关系 5.配置超级管理员为限制任何用户都可以进行Grant/Revoke操作，提高安全控制，需事先Hive的超级管理员。在hive-site.xml中添加hive.semantic.analyzer.hook配置，并实现自己的权限控制类HiveAdmin。1234&lt;property&gt; &lt;name&gt;hive.semantic.analyzer.hook&lt;/name&gt; &lt;value&gt;com.hive.HiveAdmin&lt;/value&gt;&lt;/property&gt; com.hive.HiveAdmin类代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.bigdata.hive; import org.apache.hadoop.hive.ql.parse.ASTNode; import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook; import org.apache.hadoop.hive.ql.parse.HiveParser; import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext; import org.apache.hadoop.hive.ql.parse.SemanticException; import org.apache.hadoop.hive.ql.session.SessionState; public class HiveAdmin extends AbstractSemanticAnalyzerHook &#123; private static String[] admin = &#123;&quot;admin&quot;, &quot;root&quot;,&quot;yangql&quot;&#125;; @Override public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast) throws SemanticException &#123; switch (ast.getToken().getType()) &#123; case HiveParser.TOK_CREATEDATABASE: case HiveParser.TOK_DROPDATABASE: case HiveParser.TOK_CREATEROLE: case HiveParser.TOK_DROPROLE: case HiveParser.TOK_GRANT: case HiveParser.TOK_REVOKE: case HiveParser.TOK_GRANT_ROLE: case HiveParser.TOK_REVOKE_ROLE: String userName = null; if (SessionState.get() != null&amp;&amp;SessionState.get().getAuthenticator() != null)&#123; userName=SessionState.get().getAuthenticator().getUserName(); &#125; if (!admin[0].equalsIgnoreCase(userName) &amp;&amp; !admin[1].equalsIgnoreCase(userName)) &#123; throw new SemanticException(userName + &quot; can&apos;t use ADMIN options, except &quot; + admin[0]+&quot;,&quot;+admin[1] +&quot;.&quot;); &#125; break; default:break; &#125; return ast; &#125; public static void main(String[] args) throws SemanticException &#123; String[] admin = &#123;&quot;admin&quot;, &quot;root&quot;&#125;; String userName = &quot;root&quot;; for(String tmp: admin)&#123; System.out.println(tmp); if (!tmp.equalsIgnoreCase(userName)) &#123; throw new SemanticException(userName + &quot; can&apos;t use ADMIN options, except &quot; + admin[0]+&quot;,&quot;+admin[1] +&quot;.&quot;); &#125; &#125; &#125; &#125; 6.Hive 权限管理流程总结 创建超级管理员 新建linux用户组和用户，也可以在既定用户组下建用户，赋予用户hive目录权限 超级管理员进入hive，授权新建用户组和用户的操作权限 客户端可以通过新建用户名和密码连接到hive执行授权内的动作","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hive安装","slug":"Hive安装20190524","date":"2017-02-25T04:54:59.000Z","updated":"2019-05-24T04:59:12.943Z","comments":true,"path":"2017/02/25/Hive安装20190524/","link":"","permalink":"http://qioinglong.top/2017/02/25/Hive安装20190524/","excerpt":"1.Hive介绍 在Hadoop生态圈中属于数据仓库的角色。Hive能够管理Hadoop中的数据，同时可以查询Hadoop中的数据。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。 Hive定义了简单的类SQL查询语言，称为HQL ，它允许熟悉SQL的用户查询数据。同时，这个语言也允许熟悉MapReduce开发者的开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。 本质上讲，Hive是一个SQL解析引擎。Hive可以把SQL查询转换为MapReduce中的job然后在Hadoop执行。Hive有一套映射工具，可以把SQL转换为MapReduce中的job，可以把SQL中的表、字段转换为HDFS中的文件(夹)以及文件中的列。这套映射工具称之为metastore，一般存放在derby、mysql中。 Hive的表其实就是HDFS的目录，按表名把文件夹分开。如果是分区表，则分区值是子文件夹，可以直接在M/R的Job里使用这些数据。","text":"1.Hive介绍 在Hadoop生态圈中属于数据仓库的角色。Hive能够管理Hadoop中的数据，同时可以查询Hadoop中的数据。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。 Hive定义了简单的类SQL查询语言，称为HQL ，它允许熟悉SQL的用户查询数据。同时，这个语言也允许熟悉MapReduce开发者的开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。 本质上讲，Hive是一个SQL解析引擎。Hive可以把SQL查询转换为MapReduce中的job然后在Hadoop执行。Hive有一套映射工具，可以把SQL转换为MapReduce中的job，可以把SQL中的表、字段转换为HDFS中的文件(夹)以及文件中的列。这套映射工具称之为metastore，一般存放在derby、mysql中。 Hive的表其实就是HDFS的目录，按表名把文件夹分开。如果是分区表，则分区值是子文件夹，可以直接在M/R的Job里使用这些数据。 2.Hive的体系架构 用户接口主要有三种：CLI，Client 和 WebUI。其中最常用的是CLI，CLI启动的时候，会同时启动一个Hive副本。Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。WebUI是通过浏览器访问Hive Hive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等 解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行 Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（包含的查询，比如select from tbl不会生成MapRedcue任务） Hive将元数据存储在RDBMS中 3.Hive的连接模式单用户模式：连接到一个In-memory 的数据库Derby，一般用于Unit Test 多用户模式：通过网络连接到一个数据库中，是最经常使用到的模式【生产模式】 远程服务器模式：用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库 4.Hive数据存储对于数据存储，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。Hive中所有的数据都存储在HDFS中，存储结构主要包括数据库、文件、表和视图。Hive中包含以下数据模型：Table内部表，External Table外部表，Partition分区，Bucket桶。Hive默认可以直接加载文本文件，还支持sequence file 、RCFile。 Hive数据库：类似传统数据库的DataBase，在第三方数据库里实际是一张表。简单示例命令行 hive &gt; create database databasetest; 内部表：Hive的内部表与数据库中的Table在概念上是类似。每一个Table在Hive中都有一个相应的目录存储数据。例如一个表test，它在HDFS中的路径为/input/test，其中input是在hive-site.xml中由${hive.metastore.warehouse.dir} 指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。删除表时，元数据与数据都会被删除。123456内部表简单示例：创建数据文件：test_table.txt创建表：create table test_table (key string)加载数据：LOAD DATA LOCAL INPATH ‘filepath’ INTO TABLE test_table查看数据：select * from test_table; select count(*) from test_table删除表：drop table test_table 外部表:外部表指向已经在HDFS中存在的数据，可以创建Partition。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。内部表的创建过程和数据加载过程这两个过程可以分别独立完成，也可以在同一个语句中完成，在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。而外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），实际数据是存储在LOCATION后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个External Table时，仅删除该链接。123456外部表简单示例：创建数据文件：external_table.txt创建表：create external table external_table (key string)加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE external_table查看数据：select * from external_table; •select count(*) from external_table删除表：drop table external_table 分区Partition对应于数据库中的Partition列的密集索引，但是Hive中Partition的组织方式和数据库中的很不相同。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition的数据都存储在对应的目录中。123456分区表简单示例：创建数据文件：test_partition_table.txt创建表：create table test_partition_table (key string) partitioned by (dt string)加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_partition_table partition (dt=‘2006’)查看数据：select * from test_partition_table; select count(*) from test_partition_table删除表：drop table test_partition_table 桶Buckets是将表的列通过Hash算法进一步分解成不同的文件存储。它对指定列计算hash，根据hash值切分数据，目的是为了并行，每一个Bucket对应一个文件。12345桶的简单示例：创建数据文件：test_bucket_table.txt创建表：create table test_bucket_table (key string) clustered by (key) into 20 buckets加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_bucket_table查看数据：select * from test_bucket_table; set hive.enforce.bucketing = true; Hive的视图视图与传统数据库的视图类似。视图是只读的，它基于的基本表，如果改变，数据增加不会影响视图的呈现；如果删除，会出现问题。•如果不指定视图的列，会根据select语句后的生成。1示例：create view test_view as select * from test 5.Hive执行原理Hive构建在Hadoop之上， HQL中对查询语句的解释、优化、生成查询计划是由Hive完成的 所有的数据都是存储在Hadoop中 查询计划被转化为MapReduce任务，在Hadoop中执行（有些查询没有MR任务，如：select * from table） Hadoop和Hive都是用UTF-8编码的 Hive编译器将一个Hive QL转换操作符。操作符Operator是Hive的最小的处理单元，每个操作符代表HDFS的一个操作或者一道MapReduce作业。Operator都是hive定义的一个处理过程. 6.下载解压Hive2.1.1安装包下载地址：http://apache.fayea.com/hive/hive-2.1.1/tar -zxvf apache-hive-2.1.1-bin.tar.gz -C /home/yangql/app 7.配置Hive的环境变量1vi /home/yangql/.bash_profile 12345# Hiveexport HIVE_HOME=/home/yangql/app/hive-2.1.1export HIVE_CONF_DIR=$HIVE_HOME/confexport CLASSPATH=$CLASSPATH:$HIVE_HOME/libexport PATH=$PATH:$HIVE_HOMW/bin 8.单用户配置Hive 配置 hive-log4j2.properties 1234cd $HIVE_HOME/conf cp hive-log4j2.properties.template hive-log4j2.properties 修改 property.hive.log.dir = /home/yangql/app/hive-2.1.1/logs 初始化 schematool -dbType derby -initSchema 1234567891011121314[yangql@hadoop01 logs]$ schematool -dbType derby -initSchemawhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/bin)SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/yangql/app/hive-2.1.1/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/yangql/app/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=trueMetastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriverMetastore connection User: APPStarting metastore schema initialization to 2.1.0Initialization script hive-schema-2.1.0.derby.sqlInitialization script completedschemaTool completed 启动Hadoop 集群 1sh /home/yangql/app/hadoop-2.7.2/sbin/start-all.sh 启动hiveserver2 1nohup hive --service hiveserver2 &amp; 执行命令hive进入到Hive Shell 12345hive&gt; show databases;OKdefaultTime taken: 0.499 seconds, Fetched: 1 row(s)hive&gt; 9.配置MySQL 相关安装文档参见《Linux下MySQL安装》 将MySQL的驱动包放置到$HIVE_HOME/lib目录下 1cp mysql-connector-java-5.1.40-bin.jar /home/yangql/app/hive-2.1.1/lib/ 10.修改配置文件：hive-site.xml123456789101112131415161718192021222324252627282930313233&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/home/yangql/app/hive-2.1.1/querylog&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/home/yangql/app/hive-2.1.1/operlog&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/home/yangql/app/hive-2.1.1/javaiotmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/home/yangql/app/hive-2.1.1/javaiotmp&lt;/value&gt;&lt;/property&gt;&lt;/property&gt; 11.执行schematool命令进行初始化123456789[yangql@hadoop01 conf]$ schematool -dbType mysql -initSchemawhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/bin)Metastore connection URL: jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=trueMetastore Connection Driver : com.mysql.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.1.0Initialization script hive-schema-2.1.0.mysql.sqlInitialization script completedschemaTool completed 12.验证 执行hive，进入到Hive Shell 12345678910[yangql@hadoop01 conf]$ hivewhich: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/bin)Logging initialized using configuration in file:/home/yangql/app/hive-2.1.1/conf/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt; show databases;OKdefaultTime taken: 1.654 seconds, Fetched: 1 row(s)hive&gt; 查看MySQL数据库是否有hive数据库 12345678910111213141516171819202122232425262728[yangql@hadoop01 ~]$ mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 44Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || hive || mysql || test || yangql || yangql01 |+--------------------+6 rows in set (0.00 sec)mysql&gt; 13.远程模式1nohup hive --service metastore&amp; 错误解决 问题1：Hive 和 hadoop jar包 slf4j-log4j12-1.7.10.jar冲突 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364which: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/jdk1.8.0_91/bin:/opt/scala-2.12.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/home/yangql/app/hive-2.1.1/bin:/home/yangql/bin:/home/yangql/app/hadoop-2.7.2/bin:/home/yangql/app/sqoop-1.4.6/bin:/home/yangql/app/spark-2.1.0-bin-hadoop2.7/bin:/home/yangql/app/zookeeper-3.4.6/bin:/bin)SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/yangql/app/hive-2.1.1/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/yangql/app/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in file:/home/yangql/app/hive-2.1.1/conf/hive-log4j2.properties Async: trueException in thread &quot;main&quot; java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:591) at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:226) at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:366) at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310) at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290) at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:558) ... 9 moreCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1654) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:80) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3367) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3406) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3386) at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3640) at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236) at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652) ... 23 moreCaused by: MetaException(message:Version information not found in metastore. ) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:7753) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:7731) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101) at com.sun.proxy.$Proxy20.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:565) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:416) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6490) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:238) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:70) ... 28 more 解决办法：删除可解决问题：rm -rf slf4j-log4j12-1.7.10.jar MySQL连接被拒绝1234mysql -u root -p rootgrant all privileges on *.* to root@&apos;hadoop01&apos; identified by &apos;root&apos;;flush privileges;exit; *.*代表全部数据库的全部表授权，也可以指定数据库授权，如test_db.*all privileges代表全部权限，也可以insert,update,delete,create,drop等；允许root用户在hadoop01（Linux系统的主机名，IP映射）进行远程登陆，并设置root用户的密码为root。flush privileges告诉服务器重新加载授权表。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://qioinglong.top/tags/Hive/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-学习笔记-1","slug":"Spark-学习笔记-120190524","date":"2017-02-24T02:54:59.000Z","updated":"2019-05-24T04:59:16.490Z","comments":true,"path":"2017/02/24/Spark-学习笔记-120190524/","link":"","permalink":"http://qioinglong.top/2017/02/24/Spark-学习笔记-120190524/","excerpt":"Spark RDD 弹性，缓存，创建RDD的几种方式，Persist持久化，广播","text":"Spark RDD 弹性，缓存，创建RDD的几种方式，Persist持久化，广播 1.Spark RDD弹性： 自动的进行内存与磁盘数据的切换 基于LineAge的高效容错 任务失败会自动进行特定次数的重试 Stage如果失败会自动进行特定次数的重试 2.什么时候使用缓存： 特别耗时 计算链很长 checkpoint之后 shuffle之后 3.创建RDD的几种方式： 使用程序中的集合创建RDD【主要用于测试】 使用本地文件系统创建RDD【测试大量数据文件】 使用HDFS创建RDD【生产环境最常用的RDD创建方式】 基于DB创建RDD 基于NOSQL创建RDD，如HBASE 基于S3创建RDD 基于数据流创建RDD 4.为什么要persist持久化： 某步骤计算特别耗时 计算链条特别长 checkPoint之前 shuffle之后(要进行网路传输) shuffle之前（框架默认把数据持久化到本地磁盘） 5.广播：默认情况下会考虑副本，更好的保持广播时由Driver发给当前Application的所有Executor内存级别的全局变量。Executor中的线程池中的线程共享全局变量，极大地减少了网络传输（否则的话每个Task要传输一次该变量。）并极大地节省了内存","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-配置HistoryServer","slug":"Spark-配置HistoryServer20190524","date":"2017-02-17T01:54:59.000Z","updated":"2019-05-24T04:59:15.904Z","comments":true,"path":"2017/02/17/Spark-配置HistoryServer20190524/","link":"","permalink":"http://qioinglong.top/2017/02/17/Spark-配置HistoryServer20190524/","excerpt":"1.为什么要有HistoryServer以standalone运行模式为例，在运行Spark Application的时候，Spark会提供一个WEBUI列出应用程序的运行时信息；但该WEBUI随着Application的完成(成功/失败)而关闭，也就是说，Spark Application运行完(成功/失败)后，将无法查看Application的历史记录； Spark history Server就是为了应对这种情况而产生的，通过配置可以在Application执行的过程中记录下了日志事件信息，那么在Application执行结束后，WEBUI就能重新渲染生成UI界面展现出该Application在执行过程中的运行时信息； Spark运行在yarn或者mesos之上，通过spark的history server仍然可以重构出一个已经完成的Application的运行时参数信息（假如Application运行的事件日志信息已经记录下来）；","text":"1.为什么要有HistoryServer以standalone运行模式为例，在运行Spark Application的时候，Spark会提供一个WEBUI列出应用程序的运行时信息；但该WEBUI随着Application的完成(成功/失败)而关闭，也就是说，Spark Application运行完(成功/失败)后，将无法查看Application的历史记录； Spark history Server就是为了应对这种情况而产生的，通过配置可以在Application执行的过程中记录下了日志事件信息，那么在Application执行结束后，WEBUI就能重新渲染生成UI界面展现出该Application在执行过程中的运行时信息； Spark运行在yarn或者mesos之上，通过spark的history server仍然可以重构出一个已经完成的Application的运行时参数信息（假如Application运行的事件日志信息已经记录下来）； 2.配置spark-defalut.conf12345park.eventLog.enabled truespark.eventLog.dir hdfs://hadoop01:9000/input/sparklogspark.yarn.historyServer.address hadoop01:18080spark.serializer org.apache.spark.serializer.KryoSerializerspark.executor.instances 4 spark.history.updateInterval默认值：10以秒为单位，更新日志相关信息的时间间隔 spark.history.retainedApplications默认值：50在内存中保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，当再次访问已被删除的应用信息时需要重新构建页面。 spark.history.ui.port默认值：18080HistoryServer的web端口 spark.history.kerberos.enabled默认值：false是否使用kerberos方式登录访问HistoryServer，对于持久层位于安全集群的HDFS上是有用的，如果设置为true，就要配置下面的两个属性 spark.history.kerberos.principal默认值：用于HistoryServer的kerberos主体名称 spark.history.kerberos.keytab用于HistoryServer的kerberos keytab文件位置 spark.history.ui.acls.enable默认值：false授权用户查看应用程序信息的时候是否检查acl。如果启用，只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息;否则，不做任何检查 spark.eventLog.enabled默认值：false是否记录Spark事件，用于应用程序在完成后重构webUI spark.eventLog.dir默认值：file:///tmp/spark-events保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建 spark.eventLog.compress默认值：false是否压缩记录Spark事件，前提spark.eventLog.enabled为true，默认使用的是snappy注：以spark.history开头的需要配置在spark-env.sh中的SPARK_HISTORY_OPTS，以spark.eventLog开头的配置在spark-defaults.conf 3.配置spark-env.sh1export SPARK_HISTORY_OPTS=&quot;-Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://hadoop01:9000/input/sparklog&quot; 4.启动Spark History Server1start-history-server.sh 5.验证登陆http://192.168.1.231:18080/ 6.HA下，Spark HistoryServer配置这里将HDFS的路径修改了，因为之前只有一个NN，HA的情况下，指定了两个，所以将hadoop01:9000替换成hadoop-cluster(hadoop/etc/hadoop/hdfs-site.xml 中dfs.nameservices配置的值),不需要指定端口 1export SPARK_HISTORY_OPTS=&quot;-Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://hadoop-cluster/spark/sparklogs&quot;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"HistoryServer","slug":"HistoryServer","permalink":"http://qioinglong.top/tags/HistoryServer/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-Yarn-cluster与Yarn-client","slug":"Spark-Yarn-cluster与Yarn-client20190524","date":"2017-02-16T14:54:59.000Z","updated":"2019-05-24T04:59:15.405Z","comments":true,"path":"2017/02/16/Spark-Yarn-cluster与Yarn-client20190524/","link":"","permalink":"http://qioinglong.top/2017/02/16/Spark-Yarn-cluster与Yarn-client20190524/","excerpt":"在Spark中，有Yarn-Client和Yarn-Cluster两种模式可以运行在Yarn上，通常情况下Yarn-cluster适用于生产环境，而Yarn-Cluster更适用于交互，调试模式，以下是它们的区别","text":"在Spark中，有Yarn-Client和Yarn-Cluster两种模式可以运行在Yarn上，通常情况下Yarn-cluster适用于生产环境，而Yarn-Cluster更适用于交互，调试模式，以下是它们的区别 1.Spark插拨式资源管理Spark支持可插拔的集群管理模式(Standalone、Mesos以及YARN )，集群管理负责启动executor进程，编写Spark application 的人根本不需要知道Spark用的是什么集群管理。Spark支持的三种集群模式，这三种集群模式都由两个组件组成:master和slave。Master服务(YARN ResourceManager,Mesos master和Spark standalone master)决定哪些application可以运行，什么时候运行以及哪里去运行。而slave服务( YARN NodeManager, Mesos slave和Spark standalone slave)实际上运行executor进程。当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器(Container)运行。Spark可以使得多个Tasks在同一个容器(container)里面运行，极大地节省了任务的启动时间。这是个很大的优点。 2.Spark On Yarn的优势 Spark支持资源动态共享，运行于Yarn的框架都共享一个集中配置好的资源池 可以很方便的利用Yarn的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 Yarn可以自由地选择executor数量 Yarn是唯一支持Spark安全的集群管理器，使用Yarn，Spark可以运行于Kerberized Hadoop之上，在它们进程之间进行安全认证 3.Appliaction Master在Yarn中，每个application都有一个Application Master进程，它是Appliaction启动的第一个容器，它负责从ResourceManager中申请资源，分配资源，同时通知NodeManager来为Application启动container，Application Master避免了需要一个活动的client来维持，启动Applicatin的client可以随时退出，而由Yarn管理的进程继续在集群中运行 4.Yarn-cluster在Yarn-cluster模式下，driver运行在Appliaction Master上，Appliaction Master进程同时负责驱动Application和从Yarn中申请资源，该进程运行在Yarn container内，所以启动Application Master的client可以立即关闭而不必持续到Application的生命周期: 客户端生成作业信息提交给ResourceManager(RM) RM在某一个NodeManager(由Yarn决定)启动container并将Application Master(AM)分配给该NodeManager(NM) NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向NM上的Application Master注册汇报并完成相应的任务5.Yarn-clientApplication Master仅仅从Yarn中申请资源给Executor，之后client会跟container通信进行作业的调度 客户端生成作业信息提交给ResourceManager(RM) RM在本地NodeManager启动container并将Application Master(AM)分配给该NodeManager(NM) NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向本地启动的Application Master注册汇报并完成相应的任务","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-Zookeeper实现HA","slug":"Spark-Zookeeper实现HA20190524","date":"2017-02-16T14:54:59.000Z","updated":"2019-05-24T04:59:15.499Z","comments":true,"path":"2017/02/16/Spark-Zookeeper实现HA20190524/","link":"","permalink":"http://qioinglong.top/2017/02/16/Spark-Zookeeper实现HA20190524/","excerpt":"在Spark集群上安装Zookeeper,实现集群的高可用性（HA），Hadoop及Spark集群的搭建请参考相关博客。","text":"在Spark集群上安装Zookeeper,实现集群的高可用性（HA），Hadoop及Spark集群的搭建请参考相关博客。 1.下载软件并解压12mv zookeeper-3.4.6.tar.gz /home/yangql/apptar -xvf zookeeper-3.4.6.tar.gz 2.设置环境变量编辑 .bash_profile文件,增加一下内容12export ZOOKEEPER_HOME=/home/yangql/app/zookeeper-3.4.6export PATH=$PATH:$ZOOKEEPER_HOME/bin 3.配置zoo.cfg文件配置文件存放在$ZOOKEEPER_HOME/conf/目录下，将zoo_sample.cfd文件名称改为zoo.cfg。配置说明： tickTime：这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。 dataDir：顾名思义就是 Zookeeper 保存数据的目录，默认情况下，Zookeeper 将写数据的日志文件也保存在这个目录里。 clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 新建两个目录 12mkdir /home/yangql/app/zookeeper-3.4.6/datamkdir /home/yangql/app/zookeeper-3.4.6/log zoo.cfg配置文件内容 1234dataDir=/home/yangql/app/zookeeper-3.4.6/datadataLogDir=/home/yangql/app/zookeeper-3.4.6/dataserver.0=hadoop01:2888:3888server.1=hadoop02:2888:3888 创建文件myid在目录/home/yangql/app/zookeeper-3.4.6/data目录下创建一个文件：myid 12touch /home/yangql/app/zookeeper-3.4.6/data/myidecho &quot;0&quot; &gt; myid 4.同步到其它机器 同步 12scp -r zookeeper-3.4.6/ yangql@hadoop02:/home/yangql/appscp .bash_profile yangql@hadoop02:/home/yangql 修改myid 1echo 1 &gt; myid 5.验证zookeeper12345678yangql@hadoop02 bin]$ zkServer.sh startJMX enabled by defaultUsing config: /home/yangql/app/zookeeper-3.4.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[yangql@hadoop02 bin]$ jps1405 QuorumPeerMain1423 Jps[yangql@hadoop02 bin]$ 6.在Spark集群上配置Zookeeper 配置 spark-env.sh12vim /home/yangql/app/spark-2.1.0-bin-hadoop2.7/conf/spark-env.shexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dsun.io.serialization.extendedDebugInfo=true -Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 7.验证 进入Spark的Web管理页面：http://192.168.1.231:8080/（Status: ALIVE）http://192.168.1.232:8080/（Status: STANDBY） 进入Spark bin目录执行 spark-shell,停止hadoop01 master 后可以看到如下信息scala&gt; 17/02/16 15:29:22 WARN client.StandaloneAppClient$ClientEndpoint: Connection to hadoop01:7077 failed; waiting for master to reconnect…17/02/16 15:29:22 WARN cluster.StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection…17/02/16 15:29:22 WARN client.StandaloneAppClient$ClientEndpoint: Connection to hadoop01:7077 failed; waiting for master to reconnect…","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Python实现文本中提取关键字","slug":"Python-Python实现文本中提取关键字20190524","date":"2017-02-16T02:54:59.000Z","updated":"2019-05-24T04:59:13.895Z","comments":true,"path":"2017/02/16/Python-Python实现文本中提取关键字20190524/","link":"","permalink":"http://qioinglong.top/2017/02/16/Python-Python实现文本中提取关键字20190524/","excerpt":"需求描述：1.提供需要统计的表清单2.将存储过程中使用的表统计出来3.将结果插入到数据库中","text":"需求描述：1.提供需要统计的表清单2.将存储过程中使用的表统计出来3.将结果插入到数据库中关键代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#_*_ coding:utf-8 _*_#!/usr/bin/env python#==========================================================# ScriptName : findTables# Author : Yangql# Created : 2015年3月12日 上午11:16:34# CopyRight : (C) Ron Yang 2017# Licence :# Purpose :#==========================================================import osimport cx_Oracleimport reprint(&quot;cx_Oracle.version:&quot;, cx_Oracle.version)host = &quot;127.0.0.1&quot;port = &quot;1521&quot;sid = &quot;orcl&quot;dsn = cx_Oracle.makedsn(host, port, sid)connection = cx_Oracle.connect(&quot;pds&quot;, &quot;pds&quot;, dsn)cursor_tbs = cx_Oracle.Cursor(connection) # 返回连接的游标对象cursor_src = cx_Oracle.Cursor(connection) # 返回连接的游标对象cursor_inser = cx_Oracle.Cursor(connection)#sql_tbs = &quot;select t.table_name from pds_tables t &quot;#sql_src = &quot;select t.name,t.TEXT from user_source t where t.TYPE=&apos;PROCEDURE&apos; &quot;sql_tbs = &quot;select t.table_name from pds_tables_b t &quot;sql_src = &quot;select t.name,t.TEXT from user_source t where t.TYPE=&apos;PROCEDURE&apos; and (t.name like &apos;UP_B%&apos; or t.name like &apos;P_B%&apos; or t.name like &apos;B%&apos;)&quot;str_tbs = cursor_tbs.execute(sql_tbs).fetchall()str_src = cursor_src.execute(sql_src).fetchall()#print(len(cursor_src.fetchall()))for row_tbs in str_tbs: #print(str(row_tbs[0]).strip().upper()) # get the first element of tuple for row_src in str_src: #print(row_src) #print(row_src[0].strip().upper()) #print(row_src[1].strip().upper()) table_names=re.findall(row_tbs[0].strip().upper(),row_src[1].strip().upper()) table_name=&quot;&quot; if table_names: #print(table_names[0]) table_name=table_names[0] proc_name=row_src[0].strip().upper() #将结果插入到数据库 #print(&quot; insert into pds_tables_procs(PROC_NAMES,TABLE_NAME) VALUES (&apos;&quot; +proc_name +&quot;&apos;,&apos;&quot; +table_name+ &quot;&apos;)&quot;) if table_name and proc_name: insert_sql = &quot; insert into pds_tables_procs_b(PROC_NAMES,TABLE_NAME) VALUES (&apos;&quot; + proc_name + &quot;&apos;,&apos;&quot; + table_name + &quot;&apos;)&quot; cursor_inser.execute(insert_sql) connection.commit()#关闭 cursor connectioncursor_tbs.close()cursor_src.close()cursor_inser.close()connection.close()print(&quot;statistics finished!!!&quot;)","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://qioinglong.top/tags/Python/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-第一个Spark程序","slug":"Spark-第一个Spark程序20190524","date":"2017-02-15T02:54:59.000Z","updated":"2019-05-24T04:59:15.551Z","comments":true,"path":"2017/02/15/Spark-第一个Spark程序20190524/","link":"","permalink":"http://qioinglong.top/2017/02/15/Spark-第一个Spark程序20190524/","excerpt":"利用Scala语言编写第一个Spark程序，并分别在本地和Spark集群正常运行，主要有以下几个步骤。1 创建Spark的配置对象SparkConf，设置Spark的运行时信息，例如说通过色图Master来设置程序要链接到的集群的Master的URL，如果设置为local，则为本地运行。2 创建SparkContext对象，SparkContext是Spark程序所有功能的入口。其核心作用是：初始化Spark应用程序的运行所需要的核心组件。包括DAGSchedule，TaskSchedule，同时还负责Spark程序往Master注册等功能.3 根据数据的不同来源，通过SparkCont来创建RDD，数据被RDD划分为一系列的Partitions，分配到每一个Partition的数据属于一个Task处理的范围4 对初始的RDD进行Transformation级别的处理。例如map,filter等高阶函数的编程","text":"利用Scala语言编写第一个Spark程序，并分别在本地和Spark集群正常运行，主要有以下几个步骤。1 创建Spark的配置对象SparkConf，设置Spark的运行时信息，例如说通过色图Master来设置程序要链接到的集群的Master的URL，如果设置为local，则为本地运行。2 创建SparkContext对象，SparkContext是Spark程序所有功能的入口。其核心作用是：初始化Spark应用程序的运行所需要的核心组件。包括DAGSchedule，TaskSchedule，同时还负责Spark程序往Master注册等功能.3 根据数据的不同来源，通过SparkCont来创建RDD，数据被RDD划分为一系列的Partitions，分配到每一个Partition的数据属于一个Task处理的范围4 对初始的RDD进行Transformation级别的处理。例如map,filter等高阶函数的编程 本地运行程序 1234567891011121314151617181920212223242526package com.yangql.sparkimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf()//创建SparkConf对象 conf.setAppName(&quot;My Fist Spark App&quot;)//设置应用程序的名称 conf.setMaster(&quot;local&quot;)//程序在本地运行 //创建SparkContext对象 val sc = new SparkContext(conf) //读取本地的数据文件,并设置任务为1 val lines=sc.textFile(&quot;D:/Program Files/spark-2.1.0-bin-hadoop2/spark-2.1.0-bin-hadoop2.7/README.md&quot;, 1) //处理数据 val words=lines.flatMap(line=&gt;line.split(&quot; &quot;)) //println(words) val pairs=words.map(word=&gt;(word,1)) val wordCounts=pairs.reduceByKey(_+_) println(wordCounts) wordCounts.foreach(wordCountsPair=&gt;println(wordCountsPair._1 +&quot;:&quot;+ wordCountsPair._2)) //关闭上下文 sc.stop() &#125;&#125;``` &lt;font color=&quot;red&quot;&gt;集群运行程序&lt;/font&gt; package com.yangql.spark import org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject WordCount_Cluster { def main(args: Array[String]): Unit = { val conf = new SparkConf()//创建SparkConf对象 conf.setAppName(“My Fist Spark App”)//设置应用程序的名称 //conf.setMaster(“local”)//程序在本地运行 //创建SparkContext对象 val sc = new SparkContext(conf) //读取本地的数据文件,并设置任务为1 val lines=sc.textFile(“/input/spark-test.txt”, 1) //处理数据 val words=lines.flatMap(line=&gt;line.split(“ “)) //println(words) val pairs=words.map(word=&gt;(word,1)) val wordCounts=pairs.reduceByKey(_+_) println(wordCounts) wordCounts.collect.foreach(wordCountsPair=&gt;println(wordCountsPair._1 +”:”+ wordCountsPair._2)) //关闭上下文 sc.stop() }}```","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Spark-CentOS6下搭建spark-2.1.0-bin-hadoop2.7.md","slug":"Spark-CentOS6下搭建spark-2.1.0-bin-hadoop2.720190524","date":"2017-02-13T02:54:59.000Z","updated":"2019-05-24T04:59:14.970Z","comments":true,"path":"2017/02/13/Spark-CentOS6下搭建spark-2.1.0-bin-hadoop2.720190524/","link":"","permalink":"http://qioinglong.top/2017/02/13/Spark-CentOS6下搭建spark-2.1.0-bin-hadoop2.720190524/","excerpt":"1.环境信息 hadoop01:192.168.1.231 hadoop01:192.168.1.232 hadoop01:192.168.1.233","text":"1.环境信息 hadoop01:192.168.1.231 hadoop01:192.168.1.232 hadoop01:192.168.1.233 2.软件信息 spark-2.1.0-bin-hadoop2.7 hadoop-2.7.2 scala-2.12.1 jdk1.8.0_91 3.hadoop2.7.2安装参见文档《Hadoop2.5.1完全分布式集群部》 4.安装JDK1.8 下载JDK包 jdk-8u91-linux-x64.tar.gz 将jdk-8u91-linux-x64.tar.gz包拷贝到 /opt目录并解压 12mv /home/yangql/jdk-8u91-linux-x64.tar.gz /opttar -xvf jdk-8u91-linux-x64.tar.gz 将JDK环境配置到/etc/profile文件中 12export JAVA_HOME=/opt/jdk1.8.0_91export PATH=$PATH:$JAVA_HOME/bin source /etc/profile生效配置 检验配置是否正确12345[root@hadoop01 etc]# java -versionjava version &quot;1.8.0_91&quot;Java(TM) SE Runtime Environment (build 1.8.0_91-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)[root@hadoop01 etc]# 5.安装Scala 下载安装包 scala-2.12.1.tgz 将scala-2.12.1.tgz包拷贝到/opt目录并解压 12mv /home/yangql/scala-2.12.1.tgz /opttar -xvf scala-2.12.1.tgz 将Scala环境配置到/etc/profile文件中 12export SCALA_HOME=/opt/scala-2.12.1export PATH=$PATH:$SCALA_HOME/bin 环境验证 123[root@hadoop01 etc]# scala -versionScala code runner version 2.12.1 -- Copyright 2002-2016, LAMP/EPFL and Lightbend, Inc.[root@hadoop01 etc]# 6.安装Spark 下载Spark安装包spark-2.1.0-bin-hadoop2.7.tgz 将spark-2.1.0-bin-hadoop2.7.tgz包拷贝到 /opt中并解压 12mv /home/yangql/spark-2.1.0-bin-hadoop2.7.tgz /opttar -xvf spark-2.1.0-bin-hadoop2.7.tgz 修改配置文件spark-env.sh 1234567cp spark-env.sh.template spark-env.shvim spark-env.shexport SCALA_HOME=/opt/scala-2.12.1export JAVA_HOME=/opt/jdk1.8.0_91export SPARK_MASTER_IP=hadoop01export SPARK_WORKER_MEMORY=512mexport HADOOP_CONF_DIR=/home/yangql/app/hadoop-2.7.2/etc/hadoop 修改配置文件slaves 1234cp slaves.template slavesvim slaveshadoop02hadoop03 7.scp到其它节点将在hadoop01上安装的jdk，scala，spark，hadoop等scp到节点hadoop02,hadoop03,如12345678scp /opt/jdk1.8.0_91 root@hadoop02:/optscp /opt/jdk1.8.0_91 root@hadoop03:/optscp /etc/profile root@hadoop02:/etcscp /etc/profile root@hadoop03:/etcscp /opt/scala-2.12.1 root@hadoop02:/optscp /opt/scala-2.12.1 root@hadoop03:/optscp /home/yangql/app/spark-2.1.0-bin-hadoop2.7 yangql@hadoop02:/home/yangql/appscp /home/yangql/app/spark-2.1.0-bin-hadoop2.7 yangql@hadoop03:/home/yangql/app 8.启动并验证123sh /home/yangql/app/hadoop-2.7.2/sbin/start-all.shsh /home/yangql/app/spark-2.1.0-bin-hadoop2.7/sbin/start-all.shjps 123456[yangql@hadoop01 ~]$ jps2593 ResourceManager2437 SecondaryNameNode2679 Master2247 NameNode3215 Jps 123456./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://hadoop01:7077 \\ --executor-memory 512m \\ /home/yangql/app/spark-2.1.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.0.jar \\ 1000","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://qioinglong.top/tags/Spark/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala学习之偏函数-异常-lazy","slug":"Scala学习之偏函数-异常-lazy20190524","date":"2017-02-11T08:54:59.000Z","updated":"2019-05-24T04:59:14.774Z","comments":true,"path":"2017/02/11/Scala学习之偏函数-异常-lazy20190524/","link":"","permalink":"http://qioinglong.top/2017/02/11/Scala学习之偏函数-异常-lazy20190524/","excerpt":"偏函数：当函数有多个参数，而在使用该函数时不想提供所有参数（比如函数有3个参数），只提供0~2个参数，此时得到的函数便是偏函数。 Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。 惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。","text":"偏函数：当函数有多个参数，而在使用该函数时不想提供所有参数（比如函数有3个参数），只提供0~2个参数，此时得到的函数便是偏函数。 Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。 惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。1.偏函数123456789101112131415161718192021222324252627package com.spark/** * 偏函数：当函数有多个参数，而在使用该函数时不想提供所有参数（比如函数有3个参数），只提供0~2个参数，此时得到的函数便是偏函数。 * Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。 惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。 */object HelloPartitionFunc &#123; def main(args: Array[String]): Unit = &#123; val sample = 1 to 8 val isEven:PartialFunction[Int,Unit]=&#123; case x if x % 2 ==0 =&gt; println(x) &#125; val isEven1:PartialFunction[Int,String]=&#123; case x if x % 2 ==0 =&gt; x + &quot; is even&quot; &#125; isEven(4) //val evenNumber = sample collect isEven1 val evenNumber = sample.collect(isEven1) evenNumber.foreach(x=&gt;println(x)) val isOdd:PartialFunction[Int,String]=&#123; case x if x % 2 != 0 =&gt; x + &quot; is odd&quot; &#125; val numbers = sample map (isEven1 orElse isOdd) numbers.foreach(println) &#125;&#125; 2.异常,lazy1234567891011121314151617package com.sparkimport java.io.IOExceptionobject HelloExceptionAndLazy &#123; def main(args: Array[String]): Unit = &#123; try&#123; lazy val score=100 println(score) 1/0 &#125;catch&#123; case ioexception:IOException =&gt; println(&quot;IOException &quot; + ioexception.toString()) case illegalArgument:IllegalArgumentException =&gt; println(&quot;IllegalArgumentException &quot;+ illegalArgument.toString()) case arithmeticException:ArithmeticException =&gt; println(&quot;ArithmeticException &quot;+arithmeticException.toString()) &#125; &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala学习之泛型-上边界-下边界","slug":"Scala学习之泛型-上边界-下边界20190524","date":"2017-02-10T08:54:59.000Z","updated":"2019-05-24T04:59:14.618Z","comments":true,"path":"2017/02/10/Scala学习之泛型-上边界-下边界20190524/","link":"","permalink":"http://qioinglong.top/2017/02/10/Scala学习之泛型-上边界-下边界20190524/","excerpt":"1.Scala的类，函数，方法都可以是泛型2.上边界 “&lt;:”表达了泛型的类型必须是某种类型或者其子类，是对类型的限定3.下边界 “&gt;:”表达了泛型的类型必须是某种类型或者其父类，是对类型的限定4.View bound:把类型转换为目标类型，是上边界和下边界的加强版本 T &lt;% ,这个代码所表达的T必须是writable类型的，但是T没有直接继承Writable接口，此时需要通过“implicit”的方式来实现这个功能。implicit def dog2Person(dog:Dog)=new Person(dog.name)5.T：ClassTag：这也是一种类型转换系统，只是在编译时类型信息不够，需要借助JVM的runtime来通过运行时获得完整的类型信息。6.逆变协变：-T +T7.covariant 协变:使你能够使用比原始指定的类型的子类,当我们定义一个协变类型 List[A+] 时，List[Child]可以是List[Parent]的子类型。8.Contravariance 逆变:使你能够使用比原始指定的类型的父类。当我们定义一个逆变类型 List[-A] 时，List[Child]可以是List[Parent]的父类型。9.context bound,T:Ordering这种语法必须能够变成Ordering T格式10.Invariance 不变。你只能使用原始指定的类型，不能协变和逆变","text":"1.Scala的类，函数，方法都可以是泛型2.上边界 “&lt;:”表达了泛型的类型必须是某种类型或者其子类，是对类型的限定3.下边界 “&gt;:”表达了泛型的类型必须是某种类型或者其父类，是对类型的限定4.View bound:把类型转换为目标类型，是上边界和下边界的加强版本 T &lt;% ,这个代码所表达的T必须是writable类型的，但是T没有直接继承Writable接口，此时需要通过“implicit”的方式来实现这个功能。implicit def dog2Person(dog:Dog)=new Person(dog.name)5.T：ClassTag：这也是一种类型转换系统，只是在编译时类型信息不够，需要借助JVM的runtime来通过运行时获得完整的类型信息。6.逆变协变：-T +T7.covariant 协变:使你能够使用比原始指定的类型的子类,当我们定义一个协变类型 List[A+] 时，List[Child]可以是List[Parent]的子类型。8.Contravariance 逆变:使你能够使用比原始指定的类型的父类。当我们定义一个逆变类型 List[-A] 时，List[Child]可以是List[Parent]的父类型。9.context bound,T:Ordering这种语法必须能够变成Ordering T格式10.Invariance 不变。你只能使用原始指定的类型，不能协变和逆变123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Animal[T](val species:T)&#123; def getAnimal(specie:T):T=species&#125;//定义一个Person类class Person(val name:String)&#123; def talk(person:Person)&#123; println(this.name + &quot; &quot; + person.name) &#125;&#125;//Worker类class Worker(name:String) extends Person(name)//Club类 上边界限定只能是Person类class Club[T &lt;: Person](p1:T,p2:T)&#123; def communication = p1.talk(p2)&#125;//Club类 view bound &lt;%class Club1[T &lt;% Person](p1:T,p2:T)&#123; def communication = p1.talk(p2)&#125;class Dog(val name:String) extends Animalclass Enginerclass Expert extends Enginer//逆变class Meeting[-T]//协变class Meeting1[+T]//context boundclass Maximum[T:Ordering](val x:T,val y:T)&#123; def bigger(implicit ord:Ordering[T])=&#123; if(ord.compare(x, y) &gt; 0) x else y &#125;&#125;object HelloTypeSystem &#123; def main(args: Array[String]): Unit = &#123; //将Dog类型转换为Person类型 implicit def dog2Person(dog:Dog)=new Person(dog.name) val p = new Person(&quot;Tom&quot;) val w=new Worker(&quot;Mary&quot;) val dog=new Dog(&quot;daHuang&quot;) new Club(p,w).communication //view bound new Club1[Person](w,dog).communication val e = new Meeting[Enginer] val export=new Meeting[Expert] participateMeet(e) participateMeet(export) println(new Maximum(3,5).bigger) &#125; def participateMeet(meeting:Meeting[Expert])&#123; println(&quot;welcome&quot;) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala学习之集合函数式编程","slug":"Scala学习之集合函数式编程20190524","date":"2017-02-10T05:54:59.000Z","updated":"2019-05-24T04:59:14.669Z","comments":true,"path":"2017/02/10/Scala学习之集合函数式编程20190524/","link":"","permalink":"http://qioinglong.top/2017/02/10/Scala学习之集合函数式编程20190524/","excerpt":"1.在scala中，Iterable是公共的方法 ，Iterable要求继承者实现一些公共的方法。2.Array是一个非常基础的数据结构，不属于scala集合体系。3.集合分为可变与不可变之分，不可变的集合在scala.collection.immutable保中，可变集合在scala.collection.mutable中4.List是元素的列表集合，是不可变的。5.List中head是第一个元素，tail是指剩下元素的集合的集合6.操作符”::”把List和其它的元素进行组拼来构建新的List7.如果集合中没有元素，返回Nil8.LinkedList是元素可变的列表9.Set是元素不可重复的，且元素是无序的。HashSet元素不可变，也不保证顺序。SortedSet会排序10.map与flatMap的区别：Spark 中 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象；而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象11.只有一个原始时，可以用”_“占位符替代，简化函数编程。如：.reduce(_ +_); //reduce((x,y) =&gt; x + y)","text":"1.在scala中，Iterable是公共的方法 ，Iterable要求继承者实现一些公共的方法。2.Array是一个非常基础的数据结构，不属于scala集合体系。3.集合分为可变与不可变之分，不可变的集合在scala.collection.immutable保中，可变集合在scala.collection.mutable中4.List是元素的列表集合，是不可变的。5.List中head是第一个元素，tail是指剩下元素的集合的集合6.操作符”::”把List和其它的元素进行组拼来构建新的List7.如果集合中没有元素，返回Nil8.LinkedList是元素可变的列表9.Set是元素不可重复的，且元素是无序的。HashSet元素不可变，也不保证顺序。SortedSet会排序10.map与flatMap的区别：Spark 中 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象；而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象11.只有一个原始时，可以用”_“占位符替代，简化函数编程。如：.reduce(_ +_); //reduce((x,y) =&gt; x + y) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647object HelloFunctional_Iterable &#123; def main(args: Array[String]): Unit = &#123; val names = List(1,2,3,4,5) //println(names.head) //println(names.tail) //println(5::names) var linkedList=scala.collection.mutable.LinkedList(2,3,4,5) //println(linkedList.head) while(linkedList != Nil) &#123; //println(linkedList.elem) linkedList=linkedList.tail &#125; val set=Set(1,2,3,5) //println(set) set + 1 set + 7 //println(set) val hashSet = scala.collection.mutable.HashSet(1,2,3) val linkedHashSet = scala.collection.mutable.LinkedHashSet(1,2,3) val sortedSet=scala.collection.mutable.SortedSet(1,2,4,3) //println(sortedSet) println(&quot;=====================================================&quot;) val list=List(&quot;I am so pround of spark&quot;, &quot;spark is very interested&quot;) .flatMap(x =&gt; x.split(&quot; &quot;)) .map(x =&gt; (x,1)) .map(x =&gt; (x._2)) .reduce(_ + _); //reduce((x,y) =&gt; x + y) val list5=List(&quot;I am so pround of spark&quot;, &quot;spark is very interested&quot;) .flatMap(_.split(&quot; &quot;)) .map((_,1)) .map((_._2)) .reduce(_ + _); //reduce((x,y) =&gt; x + y) println(list) val list1=List(&quot;I am so pround of spark&quot;, &quot;spark is very interested&quot;) .flatMap(x =&gt; x.split(&quot; &quot;)) val list2=List(&quot;I am so pround of spark&quot;, &quot;spark is very interested&quot;) .map(x =&gt; x.split(&quot; &quot;)) println(list1) println(list2) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala学习之模式匹配","slug":"Scala学习之模式匹配20190524","date":"2017-02-09T16:54:59.000Z","updated":"2019-05-24T04:59:14.721Z","comments":true,"path":"2017/02/10/Scala学习之模式匹配20190524/","link":"","permalink":"http://qioinglong.top/2017/02/10/Scala学习之模式匹配20190524/","excerpt":"1.样本类：添加了case的类便是样本类。这种修饰符可以让Scala编译器自动为这个类添加一些语法上的便捷设定。如下：2.添加与类名一致的工厂方法。也就是说，可以写成Var(“x”)来构造Var对象。3.样本类参数列表中的所有参数隐式获得了val前缀，因此它被当作字段维护。4.编译器为这个类添加了方法toString,hashCode和equals等方法。5..match对应Java里的switch，但是写在选择器表达式之后。即： 选择器 match {备选项}。6.一个模式匹配包含了一系列备选项，每个都开始于关键字case。每个备选项都包含了一个模式及一到多个表达式。箭头符号 =&gt; 隔开了模式和表达式。7.match表达式通过以代码编写的先后次序尝试每个模式来完成计算。类似于UnOp(“-“ , UnOp(“-“ , e))这种形式的，是构造器模式匹配。8.Scala的备选项表达式永远不会“掉到”下一个case；9.如果没有模式匹配，MatchError异常会被抛出。这意味着必须始终确信所有的情况都考虑到了，或者至少添加一个默认情况什么都不做。如 case _ =&gt;10.通配模式：case _ =&gt; 。表示默认的全匹配备选项。通配模式还可以用来忽略对象中不关心的部分。如：case BinOp(_,_,_) =&gt; XXX，则表示不关心二元操作符的元素是什么，只是检查是否为二元操作符11.常量模式 ：仅匹配自身。任何字面量都可以用作常量。包括String类型。另外，任何的val或单例对象也可以被用作常量。如，单例对象Nil是只匹配空列表的模式。12.变量模式 ：变量模式类似于通配符，可以匹配任何对象。不同点在于，Scala把变量绑定在匹配的对象上。之后就可以使用这个变量操作对象。","text":"1.样本类：添加了case的类便是样本类。这种修饰符可以让Scala编译器自动为这个类添加一些语法上的便捷设定。如下：2.添加与类名一致的工厂方法。也就是说，可以写成Var(“x”)来构造Var对象。3.样本类参数列表中的所有参数隐式获得了val前缀，因此它被当作字段维护。4.编译器为这个类添加了方法toString,hashCode和equals等方法。5..match对应Java里的switch，但是写在选择器表达式之后。即： 选择器 match {备选项}。6.一个模式匹配包含了一系列备选项，每个都开始于关键字case。每个备选项都包含了一个模式及一到多个表达式。箭头符号 =&gt; 隔开了模式和表达式。7.match表达式通过以代码编写的先后次序尝试每个模式来完成计算。类似于UnOp(“-“ , UnOp(“-“ , e))这种形式的，是构造器模式匹配。8.Scala的备选项表达式永远不会“掉到”下一个case；9.如果没有模式匹配，MatchError异常会被抛出。这意味着必须始终确信所有的情况都考虑到了，或者至少添加一个默认情况什么都不做。如 case _ =&gt;10.通配模式：case _ =&gt; 。表示默认的全匹配备选项。通配模式还可以用来忽略对象中不关心的部分。如：case BinOp(_,_,_) =&gt; XXX，则表示不关心二元操作符的元素是什么，只是检查是否为二元操作符11.常量模式 ：仅匹配自身。任何字面量都可以用作常量。包括String类型。另外，任何的val或单例对象也可以被用作常量。如，单例对象Nil是只匹配空列表的模式。12.变量模式 ：变量模式类似于通配符，可以匹配任何对象。不同点在于，Scala把变量绑定在匹配的对象上。之后就可以使用这个变量操作对象。 class DataFramwork //class class must have arguments case class ComputationFramework(name:String,popular:Boolean) extends DataFramwork case class SparkFramework(name:String,popular:Boolean) extends DataFramwork object HelloPatternMatch { def main(args: Array[String]): Unit = { //getSary(&quot;Hadoop&quot;) getSary(&quot;Test&quot;, 6) getMatchType(Array(3)) getCollectionsType(Array(&quot;Spark&quot;)) getCollectionsType(Array(&quot;Scala&quot;)) getCollectionsType(Array(&quot;Scala&quot;,&quot;Java&quot;)) getBigDataType(ComputationFramework(&quot;Mapreduce&quot;,true)) getBigDataType(SparkFramework(&quot;Spark&quot;,true)) getValue(&quot;Java&quot;,Map(&quot;Java&quot;-&gt;&quot;The best&quot;,&quot;Scala&quot;-&gt;&quot;The second&quot;,&quot;Spark&quot;-&gt;&quot;The most&quot;)) } //模式匹配 def getSary(name:String,age:Int){ name match{ case &quot;Spark&quot; =&gt; println(&quot;Spark&quot;) case &quot;Scala&quot; =&gt; println(&quot;Scala&quot;) case _ if name == &quot;Hadoop&quot; =&gt; println(&quot;Hadoop&quot;) //将传入参数name值传给变量_name case _name if age&gt;5 =&gt; println(_name + &quot; age is &quot; + 5 ) case _ =&gt; println(&quot;others&quot;) } } //模式匹配 匹配数据类型 def getMatchType(msg:Any){ msg match{ case i:Int =&gt; println(&quot;Integer&quot;) case s:String =&gt; println(&quot;String&quot;) case d:Double =&gt; println(&quot;Double&quot;) //只能匹配Array不能具体匹配内容 case array:Array[Int] =&gt; println(&quot;Array&quot;) case _ =&gt; println(&quot;Unknow&quot;) } } //匹配集合中具体的值 def getCollectionsType(msg:Array[String]){ msg match{ case Array(&quot;Scala&quot;) =&gt; println(&quot;one element&quot;) case Array(&quot;Scala&quot;,&quot;Java&quot;) =&gt; println(&quot;two elements&quot;) case Array(&quot;Spark&quot;,_*) =&gt; println(&quot;many elements&quot;) case _ =&gt; println(&quot;unknow&quot;) } } //匹配模式类 def getBigDataType(data:DataFramwork){ data match{ case ComputationFramework(name,popular) =&gt; println(&quot;ComputationFramwork name: &quot; + name + &quot; popular :&quot; + popular) case SparkFramework(name,popular) =&gt; println(&quot;SparkFramework name: &quot; + name + &quot; popular :&quot; + popular) case _ =&gt; println(&quot;Unkown class&quot;) } } //值匹配 def getValue(key:String,context: Map[String,String]){ context.get(key) match{ case Some(value) =&gt; println(value) case None =&gt; println(&quot;Not Found&quot;) } } }","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Scala基础学习-函数式编程","slug":"Scala基础学习-函数式编程20190524","date":"2017-02-09T15:54:59.000Z","updated":"2019-05-24T04:59:14.314Z","comments":true,"path":"2017/02/09/Scala基础学习-函数式编程20190524/","link":"","permalink":"http://qioinglong.top/2017/02/09/Scala基础学习-函数式编程20190524/","excerpt":"1.函数可以直接赋值给变量2.函数更常用的方式是匿名函数，定义的时候只需要说明输入参数的类型和函数体，如果要使用可以赋值给一个变量(其实是val常量)3.函数可以作为参数直接传递给函数，简化了编程的语法4.函数的返回值也可以是函数,当函数的返回类型是函数时，这个时候就表明了scala的函数实现了闭包5.scala闭包的内幕，scala函数的背后是类和对象，所以scala的参数都作为对象的成员，所以后续可以继续访问。6.Curring，复杂的函数编程中经常用到，可以维护变量在内存中的状态，且可以实现返回函数的链式功能，可以实现非常复杂的算法，","text":"1.函数可以直接赋值给变量2.函数更常用的方式是匿名函数，定义的时候只需要说明输入参数的类型和函数体，如果要使用可以赋值给一个变量(其实是val常量)3.函数可以作为参数直接传递给函数，简化了编程的语法4.函数的返回值也可以是函数,当函数的返回类型是函数时，这个时候就表明了scala的函数实现了闭包5.scala闭包的内幕，scala函数的背后是类和对象，所以scala的参数都作为对象的成员，所以后续可以继续访问。6.Curring，复杂的函数编程中经常用到，可以维护变量在内存中的状态，且可以实现返回函数的链式功能，可以实现非常复杂的算法，123456789101112131415161718192021222324252627object HelloFunctionProgramming &#123; def main(args: Array[String]): Unit = &#123; //函数赋值给变量 val hiData=bigData _ hiData(&quot;Spark&quot;) //_* 每一个元素 Array(1 to 10 : _*).map((item:Int)=&gt;2*item).foreach(x =&gt; println(x)) def result=(name:String) =&gt; println(&quot;Hi &quot; + name) result(&quot;Java&quot;) def result1(message:String)=(name:String) =&gt; println(message+&quot; &quot; + name) result1(&quot;hello&quot;)(&quot;Java&quot;)//currying 函数写法 &#125; //匿名函数 val f=(name:String) =&gt; println(&quot;Hi &quot; + name) f(&quot;kafaka&quot;) getName(f,&quot;scala&quot;) def bigData(name:String)&#123; println(&quot;Hi &quot; + name) &#125; def getName(func:(String) =&gt; Unit,name : String)&#123; func(name) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"读书笔记-你从未真正拼过","slug":"读书笔记-你从未真正拼过20190524","date":"2017-02-09T08:05:59.000Z","updated":"2019-05-24T04:39:13.369Z","comments":true,"path":"2017/02/09/读书笔记-你从未真正拼过20190524/","link":"","permalink":"http://qioinglong.top/2017/02/09/读书笔记-你从未真正拼过20190524/","excerpt":"","text":"断断续续读了一个月，把《你从未真正拼过》读完了，感触很多。没读到这本书之前，做事没有恒心，虎头蛇尾，每一年的都在计划，但真正能完成的很少。这本书，是各个领域的牛人分享的职业规划，时间管理，个人管理等很多方面。文章中有很多的观点，这里不一一涉及，只记录那些自己感触最深的。 纠结A还是B最重要的是你的目的，你真正想要的是什么，能力有了，目标有了，很多事情就变得非常明朗。所有选择A还是B并不需要那么究竟，想清楚自己的目标是什么，自己的目的是什么。 学好英语大学毕业五年，一直在学习英语，但是却没有坚持下来。每次都是兴致来了学一下，兴致去了就丢下。美其名曰，我在学习英语，但是收获很少。英语是一门全球化的语言，英语带给你的，是全世界的思想精髓，是浩如烟海的信息量。","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"Scala学习之Map and Tuple","slug":"Scala学习之Map and Tuple20190524","date":"2017-02-01T16:54:59.000Z","updated":"2019-05-24T04:59:14.581Z","comments":true,"path":"2017/02/02/Scala学习之Map and Tuple20190524/","link":"","permalink":"http://qioinglong.top/2017/02/02/Scala学习之Map and Tuple20190524/","excerpt":"1.默认情况下Map构造的是不可变集合，里面的内容不能修改，一旦修改就变成新的Map，原有的Map保持不变。2.Map的实例调用工厂方法apply来构造Map实例，3.可变Map需要调用类：scala.collection.mutable.Map4.如果想直接New出Map实例，需要使用HashMap等具体实现子类。5.查询一个Map中的值用方法getOrElse，一方面如果值不存在会抛出异常。6.scala.collection.immutable.SortedMap 可以得到排序的Map7.交换key与value的值：val result = for((name,age) &lt;- persInfo) yield(age,name)8.scala.collection.mutable.LinkedHashMap 可以记住插入的顺序9.Tuple中可以有很多不同类型的数据。Tuple可以作为函数的返回值","text":"1.默认情况下Map构造的是不可变集合，里面的内容不能修改，一旦修改就变成新的Map，原有的Map保持不变。2.Map的实例调用工厂方法apply来构造Map实例，3.可变Map需要调用类：scala.collection.mutable.Map4.如果想直接New出Map实例，需要使用HashMap等具体实现子类。5.查询一个Map中的值用方法getOrElse，一方面如果值不存在会抛出异常。6.scala.collection.immutable.SortedMap 可以得到排序的Map7.交换key与value的值：val result = for((name,age) &lt;- persInfo) yield(age,name)8.scala.collection.mutable.LinkedHashMap 可以记住插入的顺序9.Tuple中可以有很多不同类型的数据。Tuple可以作为函数的返回值12345678910111213141516171819202122232425262728293031323334353637383940414243444546object HelloMapTuple &#123; def main(args: Array[String]): Unit = &#123; val bigData=Map(&quot;spark&quot;-&gt;6,&quot;hadoop&quot;-&gt;5) val language = scala.collection.mutable.Map(&quot;Java&quot;-&gt;10,&quot;C&quot;-&gt;0) language(&quot;Java&quot;)=11 for((name,age)&lt;-language)&#123; println(name) println(age) &#125; //检索 println(language.getOrElse(&quot;Python&quot;, &quot;&quot;)) val persInfo = new scala.collection.mutable.HashMap[String,Int] //add and reduce persInfo += (&quot;Java&quot;-&gt;10,&quot;C&quot;-&gt;0) persInfo -= (&quot;Java&quot;) for((name,age)&lt;-persInfo)&#123; println(name + &quot; &quot; + age) &#125; for(key &lt;- persInfo.keySet) println(key) for(value &lt;- persInfo.values) println(value) val result = for((name,age) &lt;- persInfo) yield(age,name) for((name,age)&lt;-result)&#123; println(name + &quot; &quot; + age) &#125; val sortedMap=scala.collection.immutable.SortedMap((&quot;Tom&quot;,&quot;10&quot;),(&quot;Aim&quot;,&quot;6&quot;),(&quot;Xary&quot;,&quot;8&quot;)) for((name,age)&lt;-sortedMap)&#123; println(name + &quot; &quot; + age) &#125; val linkedMap=scala.collection.mutable.LinkedHashMap((&quot;Tom&quot;,&quot;10&quot;),(&quot;Aim&quot;,&quot;6&quot;),(&quot;Xary&quot;,&quot;8&quot;)) for((name,age)&lt;-linkedMap)&#123; println(name + &quot; &quot; + age) &#125; val bigTuple=(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;f&quot;) println(bigTuple._5) println(bigTuple._1) &#125;&#125;","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://qioinglong.top/tags/Scala/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"sqoop实战","slug":"Sqoop实战20190524","date":"2017-01-22T16:54:59.000Z","updated":"2019-05-24T04:59:16.601Z","comments":true,"path":"2017/01/23/Sqoop实战20190524/","link":"","permalink":"http://qioinglong.top/2017/01/23/Sqoop实战20190524/","excerpt":"sqoop连接mysql数据库，讲数据库表中数据导出到HDFS。","text":"sqoop连接mysql数据库，讲数据库表中数据导出到HDFS。 1. MySQL数据库准备 （1）创建数据库 1234567891011121314151617181920 [yangql@hadoop01 ~]$ mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; CREATE DATABASE yangql DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;ERROR 1007 (HY000): Can&apos;t create database &apos;yangql&apos;; database existsmysql&gt; CREATE DATABASE yangql01 DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;Query OK, 1 row affected (0.02 sec)mysql&gt; （2）创建表 1234567 CREATE TABLE `persinfo` ( `id` int(4) NOT NULL AUTO_INCREMENT, `name` char(20) NOT NULL, `sex` int(4) NOT NULL DEFAULT &apos;0&apos;, `last_ts` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`)); （3）插入数据 1234567891011121314INSERT INTO `persinfo` VALUES (1, &apos;fsafsaf&apos;, 1, &apos;2017-1-21 02:46:24&apos;);INSERT INTO `persinfo` VALUES (2, &apos;111&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (3, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (4, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (5, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (6, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (7, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (8, &apos;Tom&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (9, &apos;mark&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (10, &apos;safada&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (11, &apos;fsafdsa&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (12, &apos;44&apos;, 0, &apos;2017-1-19 10:27:15&apos;);INSERT INTO `persinfo` VALUES (13, &apos;fsfffff&apos;, 0, &apos;2017-1-20 10:35:44&apos;);INSERT INTO `persinfo` VALUES (14, &apos;4444&apos;, 1, &apos;2017-1-22 02:46:08&apos;); 2 导入 （1）通用参数—connect，数据库连接字符串—username，数据库访问用户名—password ，指定数据库连接密码（明文）—P 交互式的指定数据库密码—password-file，使用密码文件制定数据库密码（2）导入控制参数——选择部分数据导入—query，要导入的数据用SQL查询控制示例：import --connect jdbc:mysql://hadoop01:3306/yangql --username root --password root --query \"select * from persinfo where id","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://qioinglong.top/tags/Sqoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Linux下MySQL安装","slug":"Linux下Mysql安装20190524","date":"2017-01-19T04:04:31.000Z","updated":"2019-05-24T04:59:13.607Z","comments":true,"path":"2017/01/19/Linux下Mysql安装20190524/","link":"","permalink":"http://qioinglong.top/2017/01/19/Linux下Mysql安装20190524/","excerpt":"1.MySQL相关介绍MySQL是一个关系型数据库管理系统，由瑞典MySQL AB公司开发，目前属于Oracle公司。MySQL是一种关联数据库管理系统，关联数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。MySQL的SQL语言是用于访问数据库的最常用标准化语言。MySQL软件采用了双授权政策，它分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择MySQL作为网站数据库。在Linux上安装mysql数据库，我们可以去其官网上下载mysql数据库的rpm包，http://dev.mysql.com/downloads/mysql/5.6.html#downloads。","text":"1.MySQL相关介绍MySQL是一个关系型数据库管理系统，由瑞典MySQL AB公司开发，目前属于Oracle公司。MySQL是一种关联数据库管理系统，关联数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。MySQL的SQL语言是用于访问数据库的最常用标准化语言。MySQL软件采用了双授权政策，它分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择MySQL作为网站数据库。在Linux上安装mysql数据库，我们可以去其官网上下载mysql数据库的rpm包，http://dev.mysql.com/downloads/mysql/5.6.html#downloads。本教程通过yum来进行MySQL数据库的安装的，通过这种方式进行安装，可以将跟MySQL相关的一些服务、jar包都给我们安装好，省去了很多不必要的麻烦。 2.卸载掉原有mysql 查看该操作系统上是否已经安装了mysql数据库 12345[root@hadoop01 ~]# rpm -qa | grep mysqlmysql-server-5.1.73-7.el6.x86_64mysql-libs-5.1.73-7.el6.x86_64mysql-5.1.73-7.el6.x86_64mysql-devel-5.1.73-7.el6.x86_64 通过 rpm -e 命令 或者 rpm -e —nodeps 命令来卸载掉 123456[root@hadoop01 ~]# rpm -e mysql // 普通删除模式[root@hadoop01 ~]# rpm -e --nodeps mysql // 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除``` ## 3.通过yum来进行MySQL的安装 ## - 输入 yum list | grep mysql 命令来查看yum上提供的mysql数据库可下载的版本： [root@hadoop01 ~]# yum list | grep mysqlmysql.x86_64 5.1.73-7.el6 @basemysql-devel.x86_64 5.1.73-7.el6 @basemysql-libs.x86_64 5.1.73-7.el6 @basemysql-server.x86_64 5.1.73-7.el6 @baseapr-util-mysql.x86_64 1.3.9-3.el6_0.1 basebacula-director-mysql.x86_64 5.0.0-13.el6 basebacula-storage-mysql.x86_64 5.0.0-13.el6 basedovecot-mysql.x86_64 1:2.0.9-22.el6 basefreeradius-mysql.x86_64 2.2.6-6.el6_7 baselibdbi-dbd-mysql.x86_64 0.8.3-5.1.el6 basemod_auth_mysql.x86_64 1:3.0.0-11.el6_0.1 basemysql-bench.x86_64 5.1.73-7.el6 basemysql-connector-java.noarch 1:5.1.17-6.el6 basemysql-connector-odbc.x86_64 5.1.5r1144-7.el6 basemysql-devel.i686 5.1.73-7.el6 basemysql-embedded.i686 5.1.73-7.el6 basemysql-embedded.x86_64 5.1.73-7.el6 basemysql-embedded-devel.i686 5.1.73-7.el6 basemysql-embedded-devel.x86_64 5.1.73-7.el6 basemysql-libs.i686 5.1.73-7.el6 basemysql-test.x86_64 5.1.73-7.el6 basepcp-pmda-mysql.x86_64 3.10.9-6.el6 basephp-mysql.x86_64 5.3.3-48.el6_8 updatesqt-mysql.i686 1:4.6.2-28.el6_5 baseqt-mysql.x86_64 1:4.6.2-28.el6_5 basersyslog-mysql.x86_64 5.8.10-10.el6_6 basersyslog7-mysql.x86_64 7.4.10-5.el6 base1- 输入 yum install -y mysql-server mysql mysql-devel 命令将mysql mysql-server mysql-devel都安装好 [root@hadoop01 ~]# yum install -y mysql-server mysql mysql-deve1- 查看刚安装好的mysql-server的版本 [root@hadoop01 ~]# rpm -qi mysql-server123## 4.MySQL数据库的初始化及相关配置 ##我们在安装完mysql数据库以后，会发现会多出一个mysqld的服务，这个就是咱们的数据库服务，我们通过输入 `service mysqld start` 命令就可以启动我们的mysql服务。注意：如果我们是第一次启动mysql服务，mysql服务器首先会进行初始化的配置，如： [root@hadoop01 ~]# service mysqld start 初始化 MySQL 数据库： WARNING: The host ‘xiaoluo’ could not be looked up with resolveip.This probably means that your libc libraries are not 100 % compatiblewith this binary MySQL version. The MySQL daemon, mysqld, should worknormally with the exception that host name resolving will not work.This means that you should use IP addresses instead of hostnameswhen specifying MySQL privileges !Installing MySQL system tables…OKFilling help tables…OK To start mysqld at boot time you have to copysupport-files/mysql.server to the right place for your system PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !To do so, start the server, then issue the following commands: /usr/bin/mysqladmin -u root password ‘new-password’/usr/bin/mysqladmin -u root -h xiaoluo password ‘new-password’ Alternatively you can run:/usr/bin/mysql_secure_installation which will also give you the option of removing the testdatabases and anonymous user created by default. This isstrongly recommended for production servers. See the manual for more instructions. You can start the MySQL daemon with:cd /usr ; /usr/bin/mysqld_safe &amp; You can test the MySQL daemon with mysql-test-run.plcd /usr/mysql-test ; perl mysql-test-run.pl Please report any problems with the /usr/bin/mysqlbug script! 正在启动 mysqld： 12这时我们会看到第一次启动MySQL服务器以后会提示非常多的信息，目的就是对MySQL数据库进行初始化操作，当我们再次重新启动MySQL服务时，就不会提示这么多信息了 [root@hadoop01 ~]# service mysqld restartStopping mysqld: [ OK ]Starting mysqld: [ OK ][root@hadoop01 ~]#12我们在使用MySQL数据库时，都得首先启动mysqld服务，我们可以 通过 `chkconfig --list | grep mysqld` 命令来查看mysql服务是不是开机自动启动，如： [root@hadoop01 ~]# chkconfig —list | grep mysqldmysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@hadoop01 ~]#12我们发现mysqld服务并没有开机自动启动，我们当然可以通过 `chkconfig mysqld on` 命令来将其设置成开机启动，这样就不用每次都去手动启动了 [root@hadoop01 ~]# chkconfig mysqld on[root@hadoop01 ~]# chkconfig —list | grep mysqlmysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@hadoop01 ~]#12为root用户设置密码 [root@hadoop01 ~]# mysqladmin -u root password ‘root123’ // 通过该命令给root账号设置密码为 root123 1登录 [root@hadoop01 ~]# mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.1.73 Source distribution Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type ‘help;’ or ‘\\h’ for help. Type ‘\\c’ to clear the current input statement. mysql&gt;123## 5.MySQL数据库的主要配置文件 ## - /etc/my.cnf 这是mysql的主配置文件 [root@hadoop01 ~]# cat /etc/my.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysql Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0 [mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid[root@hadoop01 ~]#1- /var/lib/mysql mysql数据库的数据库文件存放位置 [root@hadoop01 ~]# ls -lrt /var/lib/mysqltotal 20488drwx——— 2 mysql mysql 4096 Jan 19 04:59 testdrwx——— 2 mysql mysql 4096 Jan 19 04:59 mysql-rw-rw—— 1 mysql mysql 5242880 Jan 19 04:59 ib_logfile1-rw-rw—— 1 mysql mysql 10485760 Jan 19 07:21 ibdata1-rw-rw—— 1 mysql mysql 5242880 Jan 19 07:21 ib_logfile0srwxrwxrwx 1 mysql mysql 0 Jan 19 07:21 mysql.sock[root@hadoop01 ~]#1- 创建数据库 mysql&gt; create database yangql -&gt; ;Query OK, 1 row affected (0.00 sec) 12- /var/log MySQL数据库的日志输出存放位置 因为我们的mysql数据库是可以通过网络访问的，并不是一个单机版数据库，其中使用的协议是 tcp/ip 协议，我们都知道mysql数据库绑定的端口号是 3306 ，所以我们可以通过 netstat -anp 命令来查看一下，Linux系统是否在监听 3306 这个端口号","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://qioinglong.top/tags/MySQL/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Linux下mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz安装","slug":"Linux下mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz安装20190524","date":"2017-01-19T04:04:31.000Z","updated":"2019-05-24T04:59:13.558Z","comments":true,"path":"2017/01/19/Linux下mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz安装20190524/","link":"","permalink":"http://qioinglong.top/2017/01/19/Linux下mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz安装20190524/","excerpt":"1.从官网下载 mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz12官网： http://dev.mysql.com/downloads/mysql/wget -c http://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz 2.创建mysql的用户组/用户, data目录及其用户目录123# groupadd mysql# useradd -g mysql -d /home/mysql mysql# mkdir /home/mysql/data","text":"1.从官网下载 mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz12官网： http://dev.mysql.com/downloads/mysql/wget -c http://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz 2.创建mysql的用户组/用户, data目录及其用户目录123# groupadd mysql# useradd -g mysql -d /home/mysql mysql# mkdir /home/mysql/data 3.解压安装包并将解压包里的内容拷贝到mysql的安装目录/home/mysql123# tar -xzvf mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz# cd mysql-5.7.9-linux-glibc2.5-x86_64# mv * /home/mysql 4.初始化mysql数据库123456789# cd /home/mysql/data# rm -fr *# ./bin/mysqld --user=mysql --basedir=/home/mysql --datadir=/home/mysql/data --initialize2016-04-08T01:47:57.556677Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2016-04-08T01:47:59.945537Z 0 [Warning] InnoDB: New log files created, LSN=457902016-04-08T01:48:00.333528Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2016-04-08T01:48:00.434908Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: ece26421-fd2b-11e5-a1e3-00163e001e5c.2016-04-08T01:48:00.440125Z 0 [Warning] Gtid table is not ready to be used. Table &apos;mysql.gtid_executed&apos; cannot be opened.2016-04-08T01:48:00.440904Z 1 [Note] A temporary password is generated for root@localhost: **mjT,#x_5sW 牢记上面的随机密码， 如上dkdsfd*dmode, 下面我们修改密码时需要用到。 5.检测下是否能启动mysql服务123# cd /home/mysql# ./support-files/mysql.server startStarting MySQL.. SUCCESS! 若改用了/home/mysql为mysql的安装目录basedir， 则在启动服务时会出现如下错误：123# ./support-files/mysql.server start./support-files/mysql.server: line 276: cd: /usr/local/mysql: No such file or directoryStarting MySQL ERROR! Couldn&apos;t find MySQL server (/usr/local/mysql/bin/mysqld_safe) 由上面可知mysql的tar.gz安装包的默认安装目录为/usr/local/mysql， 这时候我们需要修改/support-files/mysql.server文件的basedir和datadir目录路径为我们环境所在的mysql的basedir和datadir路径， 如下：123456789# vim support-files/mysql.server--------------------------...basedir=/home/mysqldatadir=/home/mysql/data...--------------------------# ./support-files/mysql.server startStarting MySQL.. SUCCESS! 6.创建软链接1# ln -s /home/mysql/bin/mysql /usr/bin/mysql 7.创建配置文件 将默认生成的my.cnf备份1# mv /etc/my.cnf /etc/my.cnf.bak 进入mysql的安装目录支持文件目录1# cd /home/mysql/support-files 拷贝配置文件模板为新的mysql配置文件,1# cp my-default.cnf /etc/my.cnf 可按需修改新的配置文件选项， 不修改配置选项， mysql则按默认配置参数运行.如下是我修改配置文件/etc/my.cnf， 设置编码为utf8以防乱码12345678910111213# vim /etc/my.cnf[mysqld]basedir = /home/mysqldatadir = /home/mysql/datacharacter_set_server=utf8init_connect=&apos;SET NAMES utf8&apos;[client]default-character-set=utf8 8.配置mysql服务开机自动启动 拷贝启动文件到/etc/init.d/下并重命令为mysqld1# cp /home/mysql/support-files/mysql.server /etc/init.d/mysqld 增加执行权限1# chmod 755 /etc/init.d/mysqld 检查自启动项列表中没有mysqld这个，如果没有就添加mysqld：12# chkconfig --list mysqld# chkconfig --add mysqld 设置MySQL在345等级自动启动1# chkconfig --level 345 mysqld on 或用这个命令设置开机启动：1# chkconfig mysqld on 9.mysql服务的启动/重启/停止 启动mysql服务1# service mysqld start 重启mysql服务1# service mysqld restart 停止mysql服务1# service mysqld stop 10.初始化mysql用户root的密码 先将mysql服务停止1# service mysqld stop 进入mysql安装目录， 执行：12345# cd /home/mysql# ./bin/mysqld_safe --skip-grant-tables --skip-networking&amp;[1] 6225[root@localhost mysql]# 151110 02:46:08 mysqld_safe Logging to &apos;/home/mysql/data/localhost.localdomain.err&apos;.151110 02:46:08 mysqld_safe Starting mysqld daemon with databases from /home/mysql/data 另外打开一个终端(p.s. 如果是ssh连接登录的, 另外创建一个ssh连接即可）， 执行操作如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# mysql -u root mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -AWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.7.9 MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; use mysql;Database changedmysql&gt; UPDATE user SET password=PASSWORD(&apos;123456&apos;) WHERE user=&apos;root&apos;;ERROR 1054 (42S22): Unknown column &apos;password&apos; in &apos;field list&apos;mysql&gt; update user set authentication_string = PASSWORD(&apos;123456&apos;) where user = &apos;root&apos;;Query OK, 1 row affected, 1 warning (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 1mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; \\s--------------mysql Ver 14.14 Distrib 5.7.9, for linux-glibc2.5 (x86_64) using EditLine wrapperConnection id: 2Current database: mysqlCurrent user: root@SSL: Not in useCurrent pager: stdoutUsing outfile: &apos;&apos;Using delimiter: ;Server version: 5.7.9 MySQL Community Server (GPL)Protocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8Db characterset: utf8Client characterset: utf8Conn. characterset: utf8UNIX socket: /tmp/mysql.sockUptime: 4 min 47 secThreads: 1 Questions: 43 Slow queries: 0 Opens: 127 Flush tables: 1 Open tables: 122 Queries per second avg: 0.149--------------mysql&gt; exit;Bye 到此， 设置完mysql用户root的密码且确保mysql编码集是utf8, 注意上面， 新版本的mysql.user表里的密码字段是authentication_string 快捷键ctrl + c停止# ./bin/mysqld_safe …命令， 重新启动mysql服务， 用新密码连接mysql：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# service mysqld startStarting MySQL SUCCESS![root@localhost bin]# mysql -uroot -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.9Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; use mysql;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.mysql &gt; exit;Bye咦？又要我改密码， 我们通过mysqladmin来修改密码， 先输入原密码， 再设置新密码， 总算可以了吧！！！# cd /home/mysql# ./bin/mysqladmin -u root -p passwordEnter password:New password:Confirm new password:Warning: Since password will be sent to server in plain text, use ssl connection to ensure password safety.# mysql -uroot -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 6Server version: 5.7.9 MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt;或直接：# ./bin/mysqladmin -uroot -p&apos;**mjT,#x_5sW&apos; password &apos;123456&apos;mysqladmin: [Warning] Using a password on the command line interface can be insecure.Warning: Since password will be sent to server in plain text, use ssl connection to ensure password safety.其中， **mjT,#x_5sW就是我们在使用mysqld --initialize时牢记下的随机密码 11.mysql远程授权 格式如下：``mysql&gt; grant all [privileges] on db_name.table_name to ‘username’@’host’ identified by ‘password’;示例如下： mysql&gt; grant all privileges on . to ‘root’@’%’ identified by ‘123456’;Query OK, 0 rows affected, 1 warning (0.04 sec) mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) mysql&gt;或用 mysql&gt; grant all on . to ‘root’@’%’ identified by ‘123456’;```到此， 完成了mysql的安装 及配置！！！","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://qioinglong.top/tags/MySQL/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Sqoop安装","slug":"Sqoop安装20190524","date":"2017-01-17T16:54:59.000Z","updated":"2019-05-24T04:59:16.576Z","comments":true,"path":"2017/01/18/Sqoop安装20190524/","link":"","permalink":"http://qioinglong.top/2017/01/18/Sqoop安装20190524/","excerpt":"Sqoop是SQL to Hadoop的缩写，主要作用在于在结构的数据存储(关系型数据库)与Hadoop之间进行数据双向交换。也就是说，Sqoop可以将关系数据库(如MySQL,Oracel等)的数据导入Hadoop的HDFS、Hive中，也可以将HDFS、Hive的数据导出到关系数据库中。Sqoop充分利用了Hadoop的优点，整个导入都是由MapReduce计算框架实现并行化，非常高效。","text":"Sqoop是SQL to Hadoop的缩写，主要作用在于在结构的数据存储(关系型数据库)与Hadoop之间进行数据双向交换。也就是说，Sqoop可以将关系数据库(如MySQL,Oracel等)的数据导入Hadoop的HDFS、Hive中，也可以将HDFS、Hive的数据导出到关系数据库中。Sqoop充分利用了Hadoop的优点，整个导入都是由MapReduce计算框架实现并行化，非常高效。(1) 解压文件[yangql@hadoop01 opt]$ tar -xvf sqoop-1.4.6.binhadoop-2.0.4-alpha.tar.gz(2) 将文件重命名[yangql@hadoop01 opt]$ mv sqoop-1.4.6.binhadoop-2.0.4-alpha sqoop-1.4.6(3) 配置环境变量export SQOOP_HOME=/home/yangql/app/sqoop-1.4.6export PATH=PATH:PATH:SQOOP_HOME/bin(4) 验证(忽略警告信息)12345678910[yangql@hadoop01 bin]$ sqoop versionWarning: /home/yangql/app/sqoop-1.4.6/../hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: /home/yangql/app/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/yangql/app/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /home/yangql/app/sqoop-1.4.6/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.Try &apos;sqoop help&apos; for usage.","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://qioinglong.top/tags/Sqoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hadoop学习笔记-基础知识-20170118","slug":"Hadoop学习笔记-基础知识-2017011820190524","date":"2017-01-16T19:20:00.000Z","updated":"2019-05-24T04:59:12.492Z","comments":true,"path":"2017/01/17/Hadoop学习笔记-基础知识-2017011820190524/","link":"","permalink":"http://qioinglong.top/2017/01/17/Hadoop学习笔记-基础知识-2017011820190524/","excerpt":"1.Hadoop历史Hadoop最早起源于Nutch，，Nutch是一个开源的搜索引擎,由Doug Cutting在2002年的时候开发完成，原本Nutch的目标是为了构建一个大型的搜索引擎，涵盖网页爬取、索引、查询等一些功能，但随着网页爬取的数量不断增加，此时对于存储和索引就形成了比较严重的瓶颈问题。此时，Google发表了三篇论文:GFS，MapReduce,BigTable，这三篇论文堪称大数据领域的开篇巨作，也由此拉开了至今为止都非常火热的大数据应用。","text":"1.Hadoop历史Hadoop最早起源于Nutch，，Nutch是一个开源的搜索引擎,由Doug Cutting在2002年的时候开发完成，原本Nutch的目标是为了构建一个大型的搜索引擎，涵盖网页爬取、索引、查询等一些功能，但随着网页爬取的数量不断增加，此时对于存储和索引就形成了比较严重的瓶颈问题。此时，Google发表了三篇论文:GFS，MapReduce,BigTable，这三篇论文堪称大数据领域的开篇巨作，也由此拉开了至今为止都非常火热的大数据应用。2003年google首先发表了GFS（Google File System），这篇文章描述了Google网页爬取相关数据的存储架构，在随后的2004年又发表了分布式计算框架的论文MapReduce（分布式计算框架的核心思想）。基于这三篇论文，Doug Cutting 带领的Nutch团队自己动手开发了一套存储架构叫Nutch Distributed File System(NDFS)和分布式计算框架MapReduce，2006年时，由于这两个项目不仅可以应用于搜索领域，所以Nutch团队将这两个项目从Nutch中移出Nutch，成为Lucene的一个子项目，此时，这个子项目被命名为Hadoop，同时NDFS被更名为HDFS（Hadoop Distributed File System）。与此同时，Doug Cutting加入Yahoo!，组建了一个团队专门开发Hadoop，在2006年的2月，Apache Hadoop项目正式启动使得HDFS和MapReduce独立发展，2年之后，也就是2008年，Hadoop成为了Apache的顶级项目。 2.Hadoop架构变化Hadoop的版本更迭非常快，从0.x到1.x再到现在的2.7.3，主要经历了比较重大的2个MapReduce（后续简称为MR）架构，分别成为MRv1和MRv2。第一个是在Hadoop 2.x之前的版本MR v1，此时的MR主要有2个服务进程：JobTracker和TaskTracker，Hadoop集群的资源调度和作业调度都是由JobTracker来控制的，这就造成了JobTracker这个服务进程以及运行JobTracker节点的压力过大，容易造成单点故障等问题。从Hadoop 2.x之后，也就是MR v2开始之后，整个架构对于资源调度和作业调度进行了分离，将原来由JobTracker负责的资源调度和管理分成对于每一个作业运行所需资源的资源协调器来负责，这个资源协调器被称为YARN（Yet Another Resource Negotiator），对于整个集群来说，将资源调度和作业调度分开，也更为合理。在YARN中，由RM负责资源的协调和调度，而运行于从节点上的NM则负责对应Task所需资源的申请和该节点资源使用情况的汇报。 3.主要模块 Hadoop Common，支持Hadoop其他模块的通用工具 HDFS，Hadoop的分布式文件系统，提供了高吞吐量的数据访问 YARN，作业调度和资源管理的一个框架 MapReduce，基于YARN的大数据分布式计算框架4.Hadoop相关概念 Master：主节点，运行着NameNode、ResourceManager服务进程 Slave：从节点，运行着DataNode、NodeManager服务进程 NameNode： NameNode运行于主节点之上，就是被称为Master的节点，负责记录各个数据块的存储空间（NameSpace），与DataNode进行通信，获取其状态信息，并根据决策算法将数据分发到某些节点。 DataNode：DataNode运行于从节点之上，也就是被称为Slave的节点，负责实际数据的存储，与NameNode进行通信，将其所在节点的存储状态汇报给NameNode，以供其决策。 HDFS集群：一个HDFS集群包含一个单独的Master节点和多个Slave节点，由一台Master服务器所架设的NameNode，称之为单NameNode集群，两台Master服务器架设成的NameNode，称之为双NameNode集群。 单机模式：服务进程运行在一个台机器上，并且文件系统也直接采用本地文件系统，没有分布式文件系统，而是直接读写本地文件，无论是在存储上，还是在计算时，都是由单机完成。所以严格意义上来讲，这样的Hadoop运行模式不能称之为集群，而且单机模式主要用于实验。 伪分布式模式：伪分布式模式由一台机器实现的，它通过一个机器上不同的Java进程来模拟分布式中不同节点上的服务进程，这种情况下将文件系统设置成分布式的文件系统，这样即使是一台机器，也可以看成是逻辑上的分布式。 分布式模式：伪分布式当中运行的一些服务进程放到另外的一些机器上去运行，例如我们把DataNode和NodeManager放到多个从节点（从服务器）上去运行。就实现了一个真正的分布式模式的Hadoop集群。5.Yarn框架5.1 MRv1 与 MRv2 比较 MRv1：编程模型：Map阶段和Reduce阶段，数据处理引擎：MapTask和ReduceTask，运行时环境：JobTracker（资源管理和作业控制）和TaskTracker（接受JobTracker命令并具体执行） MRv2：编程模型、数据处理引擎，与MRv1是一样的唯一不同的是运行时环境。MRv2是运行于YARN之上的MapReduce计算框架。YARN（资源管理与调度）和ApplicationMaster（作业控制）。YARN支持多种计算框架的资源管理器。5.2 MRv1局限性 可扩展性差，同时具备资源管理和作业控制两个功能，集群中的瓶颈。 可靠性差，主从架构（Master/Slave架构），其中的主节点存在单点故障，一旦主节点出现问题，将导致集群不可用。 资源利用率低，MRv1采用Slot资源分配模式，粗粒度的资源划分单位，通常任务不会用完一个槽的对应资源，且其他任务也无法使用这些空闲资源。 无法支持多种计算框架5.3 YARN 优势 资源利用率高， 运维成为降低 数据共享5.4 YARN的基本组成结构 ResourceManager：全局的资源管理器，负责整个集群的资源管理、分配与调度。 Scheduler（调度器），纯调度器，默认下是Fair Scheduler NodeManager：对每一个slave上的资源和任务做管理 ，定时的向ResourceManager汇报HearBeat（资源的使用情况和Container的运行状态），接受来自ApplicationMaster的启动/停止的请求 Container：资源分配单位（MRv1中Slot），动态分配。 ApplicationMaster：每个APP都会包含一个 ApplicationMaster,ApplicationMaster 的功能包括：1） 向ResourceManager申请资源（用Container资源抽象）2） 将任务做进一步的分配3） 与NodeManager通信启动/停止任务4） 跟踪每一个Task的运行状态（包括Failed后的操作）5.5 YARN的通信协议 Client与RM通信的协议，作业的提交、应用程序的状态等。 AM与RM通信协议，向RM注册AM，申请资源。 AM与NM通信协议，启动/停止Container。 RM与NM通信协议,汇报slave节点的资源信息包括Container的状态（运行状况）5.6 YARN的工作流程 短作业，作业运行几秒、几分、几小时或几天……会正常结束的作业； 长作业，如无意外，永远不停止的作业（服务部署）向YARN提交一个作业（或应用程序）：YARN的运行过程:1，客户向YARN提交Application2，RM为这个应用程序分配一个Container（某个NM上），RM要求该NM在分配的Container上启动应用程序的ApplicationMaster（AM）3，AM向RM注册自己4，AM向RM申请资源5，与RM分配资源的NM通信，要求其启动Task6，NM设置好运行环境之后启动任务7，各Task向AM汇报进度和状态8，程序运行完毕，AM向RM注销并关闭自己。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://qioinglong.top/tags/Hadoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"2017年计划","slug":"2017年计划20190524","date":"2017-01-13T06:52:31.000Z","updated":"2019-05-24T04:39:13.178Z","comments":true,"path":"2017/01/13/2017年计划20190524/","link":"","permalink":"http://qioinglong.top/2017/01/13/2017年计划20190524/","excerpt":"","text":"&lt;=空=&gt;","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"hadoop-RedHat5.4-64bit编译安装hadoop2.5.1.md","slug":"RedHat5.4-64bit编译安装hadoop2.5.120190524","date":"2017-01-08T16:54:59.000Z","updated":"2019-05-24T04:59:14.071Z","comments":true,"path":"2017/01/09/RedHat5.4-64bit编译安装hadoop2.5.120190524/","link":"","permalink":"http://qioinglong.top/2017/01/09/RedHat5.4-64bit编译安装hadoop2.5.120190524/","excerpt":"1 环境 系统：redhat 5.4 软件: jdk 1.7,hadoop- 2.5.1- src.tar.gz,Maven 3.1.1,protobuf2.5.0 IP:192.168.1.231 主机名：hadoop01 需要连接互联网(重要)","text":"1 环境 系统：redhat 5.4 软件: jdk 1.7,hadoop- 2.5.1- src.tar.gz,Maven 3.1.1,protobuf2.5.0 IP:192.168.1.231 主机名：hadoop01 需要连接互联网(重要) 2 安装包123456[root@hadoop01 yangql]# yum install gcc[root@hadoop01 yangql]# yum intall gcc-c++ [root@hadoop01 yangql]# yum install make[root@hadoop01 yangql]# yum install cmake[root@hadoop01 yangql]# yum install openssl-devel[root@hadoop01 yangql]# yum install ncurses-devel 3 安装protochadoop2.5.1编译需要protoc2.5.0的支持，所以还要安装下载protoc2.5.0 官方网址：https://code.google.com/p/protobuf/downloads/list 百度云盘：http://pan.baidu.com/s/1slEnwT3 配置命令： 12345[root@hadoop01 yangql]# tar -xvf protobuf-2.5.0.tar.gz[root@hadoop01 yangql]# cd protobuf-2.5.0[root@hadoop01 yangql]# ./configure --prefix=/usr/local/protobuf[root@hadoop01 yangql]# make[root@hadoop01 yangql]# make install /usr/local/protobuf:自定义安装目录，可根据自己情况设定 修改.bash_profile,protobuf加入到环境变量中 1export PATH=/usr/local/protobuf/bin:$PATH 执行protoc —version命令，显示如下信息时，安装成功 12[root@hadoop01 etc]# protoc --versionlibprotoc 2.5.0 4 maven 安装准备maven作为编译hadoop的工具 下载mavn 12[root@hadoop01 yangql]# wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.1.1/binaries/apache-maven-3.1.1-bin.tar.gz[root@hadoop01 yangql]# tar -zxvf apache-maven-3.1.1-bin.tar.gz 配置maven的环境变量,修改.bash_profile文件，加入： 12export MAVEN_HOME=/home/yangql/apache-maven-3.1.1export PATH=$PATH:$&#123;MAVEN_HOME&#125;/bin 执行命令source .bash_profile,使环境变量生效 执行命令 mvn -version，显示如下信息时，maven安装成功 1234567root@hadoop01 yangql]# mvn -versionApache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 08:22:22-0700)Maven home: /home/yangql/mavenJava version: 1.7.0_79, vendor: Oracle CorporationJava home: /usr/jdk1.7.0_79/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.18-164.el5&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; 由于maven国外服务器可能连不上，先给maven配置一下国内镜像在maven目录下，conf/settings.xml,在&lt;/mirros&gt;里添加如下内容 123456&lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexusosc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; 1234567891011121314151617181920212223242526272829303132&lt;profile&gt; &lt;id&gt;jdk-1.7&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.7&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; 5 编译hadoop源码 解压hadoop源文件，并执行编译命令 123[root@hadoop01 yangql]# tar -zxvf hadoop-2.5.1-src.tar.gz[root@hadoop01 yangql]# cd /home/yangql/hadoop-2.5.1-src[root@hadoop01 hadoop-2.5.1-src]# mvn package -Pdist,native -DskipTests -Dtar 编译耗时比较长，成功后结果类似于： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778main: [exec] $ tar cf hadoop-2.5.1.tar hadoop-2.5.1 [exec] $ gzip -f hadoop-2.5.1.tar [exec] [exec] Hadoop dist tar available at: /home/yangql/hadoop-2.5.1-src/hadoop-dist/target/hadoop-2.5.1.tar.gz [exec][INFO] Executed tasks[INFO][INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---[INFO] Building jar: /home/yangql/hadoop-2.5.1-src/hadoop-dist/target/hadoop-dist-2.5.1-javadoc.jar[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO][INFO] Apache Hadoop Main ................................ SUCCESS [15.164s][INFO] Apache Hadoop Project POM ......................... SUCCESS [8.006s][INFO] Apache Hadoop Annotations ......................... SUCCESS [14.515s][INFO] Apache Hadoop Assemblies .......................... SUCCESS [1.927s][INFO] Apache Hadoop Project Dist POM .................... SUCCESS [7.819s][INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [18.135s][INFO] Apache Hadoop MiniKDC ............................. SUCCESS [12.993s][INFO] Apache Hadoop Auth ................................ SUCCESS [17.715s][INFO] Apache Hadoop Auth Examples ....................... SUCCESS [11.649s][INFO] Apache Hadoop Common .............................. SUCCESS [6:07.558s][INFO] Apache Hadoop NFS ................................. SUCCESS [34.173s][INFO] Apache Hadoop Common Project ...................... SUCCESS [0.259s][INFO] Apache Hadoop HDFS ................................ SUCCESS [12:47.239s][INFO] Apache Hadoop HttpFS .............................. SUCCESS [1:37.363s][INFO] Apache Hadoop HDFS BookKeeper Journal ............. SUCCESS [37.636s][INFO] Apache Hadoop HDFS-NFS ............................ SUCCESS [20.800s][INFO] Apache Hadoop HDFS Project ........................ SUCCESS [0.432s][INFO] hadoop-yarn ....................................... SUCCESS [0.207s][INFO] hadoop-yarn-api ................................... SUCCESS [4:55.064s][INFO] hadoop-yarn-common ................................ SUCCESS [2:25.453s][INFO] hadoop-yarn-server ................................ SUCCESS [0.342s][INFO] hadoop-yarn-server-common ......................... SUCCESS [1:02.345s][INFO] hadoop-yarn-server-nodemanager .................... SUCCESS [1:16.102s][INFO] hadoop-yarn-server-web-proxy ...................... SUCCESS [14.198s][INFO] hadoop-yarn-server-applicationhistoryservice ...... SUCCESS [35.040s][INFO] hadoop-yarn-server-resourcemanager ................ SUCCESS [1:00.546s][INFO] hadoop-yarn-server-tests .......................... SUCCESS [3.670s][INFO] hadoop-yarn-client ................................ SUCCESS [28.510s][INFO] hadoop-yarn-applications .......................... SUCCESS [0.156s][INFO] hadoop-yarn-applications-distributedshell ......... SUCCESS [11.962s][INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SUCCESS [8.555s][INFO] hadoop-yarn-site .................................. SUCCESS [0.356s][INFO] hadoop-yarn-project ............................... SUCCESS [15.177s][INFO] hadoop-mapreduce-client ........................... SUCCESS [0.347s][INFO] hadoop-mapreduce-client-core ...................... SUCCESS [1:35.106s][INFO] hadoop-mapreduce-client-common .................... SUCCESS [1:18.736s][INFO] hadoop-mapreduce-client-shuffle ................... SUCCESS [23.857s][INFO] hadoop-mapreduce-client-app ....................... SUCCESS [55.494s][INFO] hadoop-mapreduce-client-hs ........................ SUCCESS [48.159s][INFO] hadoop-mapreduce-client-jobclient ................. SUCCESS [1:43.626s][INFO] hadoop-mapreduce-client-hs-plugins ................ SUCCESS [9.165s][INFO] Apache Hadoop MapReduce Examples .................. SUCCESS [37.976s][INFO] hadoop-mapreduce .................................. SUCCESS [15.514s][INFO] Apache Hadoop MapReduce Streaming ................. SUCCESS [1:31.967s][INFO] Apache Hadoop Distributed Copy .................... SUCCESS [1:17.597s][INFO] Apache Hadoop Archives ............................ SUCCESS [10.781s][INFO] Apache Hadoop Rumen ............................... SUCCESS [31.381s][INFO] Apache Hadoop Gridmix ............................. SUCCESS [22.927s][INFO] Apache Hadoop Data Join ........................... SUCCESS [13.934s][INFO] Apache Hadoop Extras .............................. SUCCESS [15.672s][INFO] Apache Hadoop Pipes ............................... SUCCESS [31.610s][INFO] Apache Hadoop OpenStack support ................... SUCCESS [30.244s][INFO] Apache Hadoop Client .............................. SUCCESS [37.309s][INFO] Apache Hadoop Mini-Cluster ........................ SUCCESS [0.690s][INFO] Apache Hadoop Scheduler Load Simulator ............ SUCCESS [1:38.066s][INFO] Apache Hadoop Tools Dist .......................... SUCCESS [17.296s][INFO] Apache Hadoop Tools ............................... SUCCESS [0.204s][INFO] Apache Hadoop Distribution ........................ SUCCESS [1:46.145s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 54:21.938s[INFO] Finished at: Sun Jan 08 21:52:17 PST 2017[INFO] Final Memory: 83M/243M[INFO] ------------------------------------------------------------------------ 编译后路径在译后的路径在:hadoop-2.5.1-src/hadoop-dist/target/hadoop-2.5.1，进入hadoop-2.5.1目录，测试安装是否成功 1234567[root@hadoop01 bin]# ./hadoop versionHadoop 2.5.1Subversion Unknown -r UnknownCompiled by root on 2017-01-09T05:00ZCompiled with protoc 2.5.0From source with checksum 6424fcab95bfff8337780a181ad7c78This command was run using /home/yangql/hadoop-2.5.1-src/hadoop-dist/target/hadoop-2.5.1/share/hadoop/common/hadoop-common-2.5.1.jar 6 编译中遇到的问题 问题描述1：Could not resolve dependencies(Could not resolve dependencies for project org.apache.hadoop:hadoop-minikdc:jar:2.7.0:) 问题解决1：解决办法：这种情况很常见，而且很多都碰到了，他们也是完全按照文档来配置的，但是就不成功，这就是因为插件没有下载完毕造成的。所以尽量多执行几次下面命令1mvn package -Pdist,native -DskipTests -Dtar","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://qioinglong.top/tags/Hadoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hadoop2.5.1完全分布式集群部署","slug":"Hadoop2.5.1完全分布式集群部署20190524","date":"2017-01-02T16:00:00.000Z","updated":"2019-05-24T04:59:12.329Z","comments":true,"path":"2017/01/03/Hadoop2.5.1完全分布式集群部署20190524/","link":"","permalink":"http://qioinglong.top/2017/01/03/Hadoop2.5.1完全分布式集群部署20190524/","excerpt":"1 部署所需软件 操作系统：rhel-server-5.4-x86_64-dvd.iso JDK：jdk-7u79-linux-x64.tar.gz Hadoop：hadoop-2.5.1.tar.gz 远程连接工具：Xmanager Enterprise 虚拟机：vmware 开发工具：eclipse","text":"1 部署所需软件 操作系统：rhel-server-5.4-x86_64-dvd.iso JDK：jdk-7u79-linux-x64.tar.gz Hadoop：hadoop-2.5.1.tar.gz 远程连接工具：Xmanager Enterprise 虚拟机：vmware 开发工具：eclipse 2 Hadoop 2.x集群网络拓扑 一个master节点，即namenode节点 二个slave节点，即二个datanode节点 3 安装VMware及Linux操作系统此处省略，可参照网上相关教程虚拟机网络配置可参考《虚拟机NetworkAdapter三种方式的区别》 4 操作系统环境配置4.1 添加用户yangql1234567[root@hadoop01 ~]# useradd yangql //yangql为用户名[root@hadoop01 ~]# passwd yangqlChanging password for user yangql.New UNIX password:BAD PASSWORD: it is based on a dictionary wordRetype new UNIX password:passwd: all authentication tokens updated successfully. 4.2 关闭防火墙1[root@hadoop01 ~]# vim /etc/selinux/config 123[root@hadoop01 ~]# service iptables stop #停止[root@hadoop01 ~]# chkconfig iptables off #关闭[root@hadoop01 ~]# service iptables status #查看状态 4.3 虚拟机IP地址配置 修改文件network1[root@hadoop01 sysconfig]# vim /etc/sysconfig/network 修改文件ifcfg-eth01[root@hadoop01 network-scripts]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改hosts文件1[root@hadoop01 ~]# vim /etc/hosts 4.4 Linux免密设置详见文档《Linux添加SFTP公钥步骤》http://yangql.cn/2017/01/01/Linux%E6%B7%BB%E5%8A%A0SFTP%E5%85%AC%E9%92%A5%E6%AD%A5%E9%AA%A4/#more 4.5 JDK安装 将jdk安装包在宿主机中解压然后上传至虚拟机中，也可以上传压缩包到虚拟机中解压。本教程采用解压后，用ssh工具上传 上传解压命令 1tar -xvf jdk-7u79-linux-x64.tar.gz 新解压的文件复制到/usr/目录下 配置JDK环境1[yangql@hadoop01 ~]$ vim .bash_profile 123export JAVA_HOME=/usr/jdk1.7.0_79export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 配置生效 1source .bash_profile 验证JDK是否成功 1234[yangql@hadoop01 ~]$ java -versionjava version &quot;1.7.0_79&quot;Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 5 Hadoop2.5.1安装5.1 下载并上传下载最新版本的HadoopHadoop2.5.1，并上传至/home/yangql目录。解压。1tar -zxf hadoop-2.5.1.tar.gz 解压完成后，需要配置各种文件，其中包括:12345678/hadoop-2.5.1/etc/hadoop/hadoop-env.sh/hadoop-2.5.1/etc/hadoop/yarn-env.sh/hadoop-2.5.1/etc/hadoop/slaves/hadoop-2.5.1/etc/hadoop/core-site.xml/hadoop-2.5.1/etc/hadoop/hdfs-site.xml/hadoop-2.5.1/etc/hadoop/mapred-site.xml/hadoop-2.5.1/etc/hadoop/yarn-site.xml/etc/.bash_profile 5.2 hadoop-env.sh文件，修改jdk路径配置参数：export JAVA_HOME=/usr/jdk1.7.0_79 5.3 配置yarn-env.sh文件，修改jdk路径配置参数：export JAVA_HOME=/usr/jdk1.7.0_79 5.4 配置slaves文件12hadoop02hadoop03 5.5 配置core-site.xml文件12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/yangql/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注：临时数据存放文件夹的权限要设置读写权限，否则初始hadoop会报错 5.6 配置hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.7 配置mapred-site.xmlmapred-site.xml文件在hadoop中由mapred-site.xml.template替代，将后缀去掉即可。1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;hadoop01:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;20&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reduce.tasks&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop01:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.8 配置yarn-site.xml123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop01:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop01:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resourcetracker.address&lt;/name&gt; &lt;value&gt;hadoop01:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop01:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.9 配置.bash_profile，加入hadoop环境1export HADOOP_HOME=/home/yangql/hadoop-2.5.1 6 拷贝到其他节点12scp –r /home/yangql/hadoop-2.5.1 yangql@hadoop002:/home/yangqlscp –r /home/yangql/hadoop-2.5.1 yangql@hadoop003:/home/yangql 7 格式化namenode1格式化命令：./bin/hdfs namenode –format 8 启动hadoop1234567启动hdfs: ./sbin/start-dfs.sh启动yarn: ./sbin/start-yarn.sh全部启动 ./sbin/start-all.sh启动成功后jps看进程7854 Jps7594 ResourceManager7357 NameNode 9 通过浏览器访问12http://192.168.1.231:50070/http://192.168.1.231:8088/ 10 遇到的问题10.1 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable配置完Hadoop启动的时候出现如下警告信息：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable问题在哪里？有人说这是hadoop的预编译包是32bit的，运行在64bit上就会有问题。但是这个答案大多数时候都是错的。123``` 方案2：直接在log4j日志中去除告警信息。在/usr/local/hadoop-2.5.1/etc/hadoop/log4j.properties文件中添加 log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://qioinglong.top/tags/Hadoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Linux添加SFTP公钥步骤","slug":"Linux添加SFTP公钥步骤20190524","date":"2016-12-31T16:00:00.000Z","updated":"2019-05-24T04:59:13.507Z","comments":true,"path":"2017/01/01/Linux添加SFTP公钥步骤20190524/","link":"","permalink":"http://qioinglong.top/2017/01/01/Linux添加SFTP公钥步骤20190524/","excerpt":"1.首先需要在目录(/home/yangql)创建.ssh文件夹 1[yangql@hadoop01 ~]$ mkdir .ssh 2.在客户端生成公钥和私钥12[yangql@hadoop01 ~]$ cd .ssh[yangql@hadoop01 .ssh]$ ssh-keygen -t rsa 执行创建密钥对命令(一路回车)，直至完成公钥与私钥生成。","text":"1.首先需要在目录(/home/yangql)创建.ssh文件夹 1[yangql@hadoop01 ~]$ mkdir .ssh 2.在客户端生成公钥和私钥12[yangql@hadoop01 ~]$ cd .ssh[yangql@hadoop01 .ssh]$ ssh-keygen -t rsa 执行创建密钥对命令(一路回车)，直至完成公钥与私钥生成。3.把.ssh目录下的公钥文件：/home/yangql/.ssh/id_rsa.pub文件传输到服务器上1[yangql@hadoop01 .ssh]$ scp /home/yangql/id_rsa.pub yangql@hadoop02 注: hadoop01（对应IP192.168.1.231） hadoop02（对应IP192.168.1.232） 4、检查服务器（hadoop02）权限 /home/yangql/ 必须是755 5.公钥拷贝到authorized_keys(第一次添加时将公钥重命名为authorized_keys)1[yangql@hadoop01 .ssh]$ cp id_rsa.pub authorized_keys 如果已经存在authorized_keys文件，执行命令：1[yangql@hadoop01 .ssh]$ cat id_rsa.pub &gt;&gt; /home/dxpt/.ssh/authorized_keys 6.公钥文件的权限必须是6441[yangql@hadoop01 .ssh]$ chmod 644 authorized_keys 7.客户端ssh验证(如果不需要输入密码则公钥设置成功,只有第一次连接需要输入YES确认：)123[yangql@hadoop02 home]$ sftp yangql@hadoop02Connecting to hadoop02...sftp&gt; 8.验证通过后，则可通过免密传输文件 注意事项： 权限：如果配置完对等信任公钥，仍提示输入密码或者访问拒绝，则需要查看服务器的目录权限是否正确，家目录权限755，.ssh目录权限是755,authorized_keys文件权限是644 备份：authorized_keys不能出现空格等不是公钥的信息，否则公钥文件就会失效，每次附加新公钥时，养成变更前备份的好习惯","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://qioinglong.top/tags/Linux/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"虚拟机NetworkAdapter三种方式的区别","slug":"虚拟机NetworkAdapter三种方式的区别20190524","date":"2016-12-29T22:52:31.000Z","updated":"2019-05-24T04:59:16.811Z","comments":true,"path":"2016/12/30/虚拟机NetworkAdapter三种方式的区别20190524/","link":"","permalink":"http://qioinglong.top/2016/12/30/虚拟机NetworkAdapter三种方式的区别20190524/","excerpt":"Vmware在安装时默认安装了两块虚拟网卡，VMnet1和VMnet8，另外还有VMnet0。这些虚拟网卡的配置都是由Vmware虚拟机自动生成的，一般来说不需要用户自行设置。Vmware提供了三种网络连接模式，分别为","text":"Vmware在安装时默认安装了两块虚拟网卡，VMnet1和VMnet8，另外还有VMnet0。这些虚拟网卡的配置都是由Vmware虚拟机自动生成的，一般来说不需要用户自行设置。Vmware提供了三种网络连接模式，分别为1.bridged(桥接模式):默认使用VMnet0，不提供DHCP服务 在桥接模式下，虚拟机和宿主计算机处于同等地位，虚拟机就像是一台真实主机一样存在于局域网中。因此在桥接模式下，我们就要像对待其他真实计算机一样为其配置IP、网关、子网掩码等等。当我们可以自由分配局域网IP时，使用桥接模式就可以虚拟出一台真实存在的主机。 2.NAT(网络地址转换模式):默认使用VMnet8，提供DHCP服务 在NAT模式下，宿主计算机相当于一台开启了DHCP功能的路由器，而虚拟机则是内网中的一台真实主机，通过路由器(宿主计算机)DHCP动态获得网络参数。因此在NAT模式下，虚拟机可以访问外部网络，反之则不行，因为虚拟机属于内网。使用NAT模式的方便之处在于，我们不需要做任何网络设置，只要宿主计算机可以连接到外部网络，虚拟机也可以。NAT模式通常也是大学校园网Vmware最普遍采用的连接模式，因为我们一般只能拥有一个外部IP。很显然，在这种情况下，非常适合使用NAT模式。 3.Host-only(主机模式):默认使用VMnet1，提供DHCP服务 在Host-only模式下，相当于虚拟机通过双绞线和宿主计算机直连，而宿主计算机不提供任何路由服务。因此在Host-only模式下，虚拟机可以和宿主计算机互相访问，但是虚拟机无法访问外部网络。当我们要组成一个与物理网络相隔离的虚拟网络时，无疑非常适合使用Host-only模式。","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"VMare","slug":"VMare","permalink":"http://qioinglong.top/tags/VMare/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"2016年终总结","slug":"2016-年终总结120190524","date":"2016-12-24T22:52:31.000Z","updated":"2019-05-24T04:39:13.142Z","comments":true,"path":"2016/12/25/2016-年终总结120190524/","link":"","permalink":"http://qioinglong.top/2016/12/25/2016-年终总结120190524/","excerpt":"","text":"","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"2016年终总结","slug":"2016-年终总结20190524","date":"2016-12-24T22:52:31.000Z","updated":"2019-05-24T04:39:13.121Z","comments":true,"path":"2016/12/25/2016-年终总结20190524/","link":"","permalink":"http://qioinglong.top/2016/12/25/2016-年终总结20190524/","excerpt":"","text":"","categories":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}],"tags":[{"name":"个人管理","slug":"个人管理","permalink":"http://qioinglong.top/tags/个人管理/"}],"keywords":[{"name":"读书","slug":"读书","permalink":"http://qioinglong.top/categories/读书/"}]},{"title":"Hadoop-windows下搭建hadoop,Hive,HBase","slug":"Hadoop-windows下搭建hadoop,Hive,HBase20190524","date":"2016-11-13T01:09:59.000Z","updated":"2019-05-24T04:59:12.431Z","comments":true,"path":"2016/11/13/Hadoop-windows下搭建hadoop,Hive,HBase20190524/","link":"","permalink":"http://qioinglong.top/2016/11/13/Hadoop-windows下搭建hadoop,Hive,HBase20190524/","excerpt":"1.搭建Hadoop 安装JDK1.8并设置环境变量 JAVA_HOME。 下载hadoop2.7.2 ,解压到D盘，路径为D:\\winbigdata\\hadoop2.7.2(注：如何不是根目录，不要带空格) 添加环境变量HADOOP_HOME=D:\\winbigdata\\hadoop2.7.2\\ ,将D:\\winbigdata\\hadoop2.7.2\\bin和D:\\winbigdata\\hadoop2.7.2\\sbin添加到path中。 下载hadooponwindows，下载地址https://github.com/sardetushar/hadooponwindows 删除hadoop下的etc和bin。将hadooponwindows里的etc和bin拷贝到D:\\winbigdata\\hadoop2.7.2\\下。 修改etc/hadoop/core-site.xml","text":"1.搭建Hadoop 安装JDK1.8并设置环境变量 JAVA_HOME。 下载hadoop2.7.2 ,解压到D盘，路径为D:\\winbigdata\\hadoop2.7.2(注：如何不是根目录，不要带空格) 添加环境变量HADOOP_HOME=D:\\winbigdata\\hadoop2.7.2\\ ,将D:\\winbigdata\\hadoop2.7.2\\bin和D:\\winbigdata\\hadoop2.7.2\\sbin添加到path中。 下载hadooponwindows，下载地址https://github.com/sardetushar/hadooponwindows 删除hadoop下的etc和bin。将hadooponwindows里的etc和bin拷贝到D:\\winbigdata\\hadoop2.7.2\\下。 修改etc/hadoop/core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/hdfs-site.xml,data/namenode,data/datanode自己创建的目录 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/winbigdata/hadoop-2.7.2/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/winbigdata/hadoop-2.7.2/data/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc\\hadoop\\yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/hadoop-env.cmd,修改JAVA_HOME(这里使用PROGRA~1,不要用program files,有空格会报错) 12@rem set JAVA_HOME=%JAVA_HOME%set JAVA_HOME=C:\\PROGRA~1\\Java\\jdk1.8.0_05 格式化namenode 1hdfs namenode -format 启动Hadoop,sbin目录下执行 1start-all 启动了4个窗口，namenode,datanode,yarn resourcemanager,yarn nodemanager1- 停止Hadoop,sbin下执行 stop-all1- web页面 Resourcemanager address:http://localhost:8088/Namenode address:http://localhost:50070/1234## 2.安装MYSQL ##- 下载mysql-5.7.17-winx64.zip，解压到D:\\winbigdata，将文件夹重命名为mysql- 配置环境变量MYSQL_HOME=D:\\winbigdata\\mysql并将 %MYSQL_HOME%\\bin 加到Path中- 创建my.ini文件，内容如下： [mysqld]basedir = D:\\\\winbigdata\\\\mysqldatadir = D:\\\\winbigdata\\\\mysql\\\\dataport = 3306server_id = MYSQL1- 以管理员身份运行cmd，进入mysql的bin目录。 mysqld —initialize1234初始化成功后，会在datadir目录下生成一些文件，其中，xxx.err文件里说明了root账户的临时密码。那行大概长这样：2017-06-15T01:20:27.235211Z 1 [Note] A temporary password is generated for root@localhost: (r=ol.mQK7kD即密码是：(r=ol.mQK7kD- 进入到bin目录注册mysql服务(一定要进入到bin目录，否则启动时会报错，无法找到指定文件) mysqld -install MySQL1- 启动mysql服务 net start MySQL1- 修改密码 mysql -uroot -pSET PASSWORD = PASSWORD(‘mysql123’);1234## 2.安装HIVE ##- 下载Hive，我安装的版本是`apache-hive-2.1.1-bin`。安装路径是`D:\\winbigdata\\hive-2.1.1`- 设置环境变量 `HIVE_HOME=D:\\winbigdata\\hive-2.1.1`，并将`D:\\winbigdata\\hive-2.1.1\\bin`加入到Path中- 修改hive-site.xm（复制一个hive-default.xml.template，重命名为hive-site.xml） javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true JDBC connect string for a JDBC metastore &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;mysql123&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; 12修改类似于 “$&#123;system:” 的配置了。在hive-site.xml中有很多地方引用了这种形式的变量，但是在实际运行时，在windows环境下这个变量存在问题。因此统一修改成 相对路径 `/winbigdata/hive-2.1.1/hive_home`.使相关的文件尽量保存在同一个目录下。 hive.exec.local.scratchdir /winbigdata/hive-2.1.1/hive_home/scratch_dir Local scratch space for Hive jobs hive.downloaded.resources.dir /winbigdata/hive-2.1.1/hive_home/resources_dir/${hive.session.id}_resources Temporary local directory for added resources in the remote file system. hive.querylog.location /winbigdata/hive-2.1.1/hive_home/querylog_dir Location of Hive run time structured log file hive.server2.logging.operation.log.location /winbigdata/hive-2.1.1/hive_home/operation_dir Top level directory where operation logs are stored if logging functionality is enabled 12- 修改hive-log4j2.properties 将hive-log4j2.properties.template这个文件复制，重命名为hive-log4j2.properties. status = INFOname = HiveLog4j2packages = org.apache.hadoop.hive.ql.log list of propertiesproperty.hive.log.level = INFOproperty.hive.root.logger = DRFAproperty.hive.log.dir = hive_logproperty.hive.log.file = hive.logproperty.hive.perflogger.log.level = INFO list of all appendersappenders = console, DRFA console appenderappender.console.type = Consoleappender.console.name = consoleappender.console.target = SYSTEM_ERRappender.console.layout.type = PatternLayoutappender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n daily rolling file appenderappender.DRFA.type = RollingRandomAccessFileappender.DRFA.name = DRFAappender.DRFA.fileName = ${hive.log.dir}/${hive.log.file} Use %pid in the filePattern to append @ to the filename if you want separate log files for different CLI sessionappender.DRFA.filePattern = ${hive.log.dir}/${hive.log.file}.%d{yyyy-MM-dd}appender.DRFA.layout.type = PatternLayoutappender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%nappender.DRFA.policies.type = Policiesappender.DRFA.policies.time.type = TimeBasedTriggeringPolicyappender.DRFA.policies.time.interval = 1appender.DRFA.policies.time.modulate = trueappender.DRFA.strategy.type = DefaultRolloverStrategyappender.DRFA.strategy.max = 30 list of all loggersloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, PerfLogger logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxnlogger.NIOServerCnxn.level = WARN logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIOlogger.ClientCnxnSocketNIO.level = WARN logger.DataNucleus.name = DataNucleuslogger.DataNucleus.level = ERROR logger.Datastore.name = Datastorelogger.Datastore.level = ERROR logger.JPOX.name = JPOXlogger.JPOX.level = ERROR logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLoggerlogger.PerfLogger.level = ${hive.perflogger.log.level} root loggerrootLogger.level = ${hive.log.level}rootLogger.appenderRefs = rootrootLogger.appenderRef.root.ref = ${hive.root.logger}12- 将元数据导入到mysql切换到目录D:\\winbigdata\\hive-2.1.1\\scripts\\metastore\\upgrade\\mysql mysql -uroot -pmysql&gt;SOURCE D:\\winbigdata\\hive-2.1.1\\scripts\\metastore\\upgrade\\mysql\\hive-schema-2.1.0.mysql.sql1- 启动Hadoop start-all1- 启动hive-启动metastore hive —service metastore -hiveconf hive.root.logger=DEBUG1- 启动hiveserver hive —service hiveserver21- 启动客户端 hive —service cli1- 验证 create table test_table(id INT);show tables;123## 3.搭建HBase ##- 下载HBase1.2.5,解压到目录D:\\winbigdata\\hbase-1.2.5- 修改配置hbase-site.xml &lt;?xml version=”1.0”?&gt;&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;&lt;!—/* Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at* http://www.apache.org/licenses/LICENSE-2.0* Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.*/—&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;localhost:6000&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/hbase&lt;/value&gt; &lt;/property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;123- 修改配置hbase-env.cmdset JAVA_HOME=C:\\PROGRA~1\\Java\\jdk1.8.0_05- 停止hadoop stop-all.cmd1- 格式化Hadoop命名节点 hdfs namenode -format1- 启动hadoop start-all.cmd1- 启动hbase start-hbase.cmd1- 启动 HBase的rest服务 hbase rest start -p 600012`http://localhost:8085/`验证是否启动成功- 启动HBase Shell hbase shell1- 测试验证 ```","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://qioinglong.top/tags/Hadoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"Hadoop2-hdfs-垃圾箱简介","slug":"Hadoop2-hdfs-垃圾箱简介20190524","date":"2016-11-13T01:09:59.000Z","updated":"2019-05-24T04:59:12.384Z","comments":true,"path":"2016/11/13/Hadoop2-hdfs-垃圾箱简介20190524/","link":"","permalink":"http://qioinglong.top/2016/11/13/Hadoop2-hdfs-垃圾箱简介20190524/","excerpt":"hdfs为每一个用户创建一个回收站：目录： /user/用户名/.Trash/ 每一个被用户通过shell删除的文件/目录在系统回收站中都有一个周期，周期过后hdfs会自动将这些数据彻底删除周期内 可以被用户恢复。","text":"hdfs为每一个用户创建一个回收站：目录： /user/用户名/.Trash/ 每一个被用户通过shell删除的文件/目录在系统回收站中都有一个周期，周期过后hdfs会自动将这些数据彻底删除周期内 可以被用户恢复。 回收站目录如下： 1234567[yangql@hadoop01 ~]$ hadoop fs -ls /user/yangql/.Trash/Found 1 itemsdrwx------ - yangql supergroup 0 2017-01-18 03:58 /user/yangql/.Trash/Current[yangql@hadoop01 ~]$ hadoop fs -ls /user/yangql/.Trash/CurrentFound 1 items-rw-r--r-- 2 yangql supergroup 5 2017-01-18 03:58 /user/yangql/.Trash/Current/my.txt[yangql@hadoop01 ~]$ 配置垃圾箱(修改core-site.xml文件) 1234&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 将回收站数据返回hdfs中： 1[yangql@hadoop01 ~]$ hdfs dfs -mv /user/yangql/.Trash/Current/my.txt / Java端删除文件 1234//删除 将文件放在回收站中 final Trash trash = new Trash(fileSystem, fileSystem.getConf()); //创建回收站 trash.moveToTrash(new Path(&quot;/dir1/file1&quot;));// 将文件放在回收站中 fileSystem.delete(new Path(&quot;/dir1/file1&quot;), true);// 删除文件 hdfs删除文件是直接删掉 不会进入回收站 上面操作是将文件先拷贝到回收站在去删除该文件","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://qioinglong.top/tags/Hadoop/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"用Hexo-nexT-GitHub搭建个人博客","slug":"用Hexo-nexT-GitHub搭建个人博客20190524","date":"2016-11-11T01:09:59.000Z","updated":"2019-05-24T04:59:16.841Z","comments":true,"path":"2016/11/11/用Hexo-nexT-GitHub搭建个人博客20190524/","link":"","permalink":"http://qioinglong.top/2016/11/11/用Hexo-nexT-GitHub搭建个人博客20190524/","excerpt":"第一章 安装Git工具下载GitHub for Windows,直接点击安装，安装完成后，可以看到“Git Shell”和“GitHub”，”Git Shell”用于命令行模式，“GitHub”桌面版。下面介绍一些常用命令： 1. 本地使用","text":"第一章 安装Git工具下载GitHub for Windows,直接点击安装，安装完成后，可以看到“Git Shell”和“GitHub”，”Git Shell”用于命令行模式，“GitHub”桌面版。下面介绍一些常用命令： 1. 本地使用 行为 命令 备注 初始化 init 在本地的当前目录里初始化git仓库 初始化 clone 地址 从网络上某个地址拷贝仓库(repository)到本地 查看当前状态 status 查看当前仓库的状态。碰到问题不知道怎么办的时候，可以通过看它给出的提示来解决问题 查看不同 diff 查看当前状态和最新的commit之间不同的地方 diff 版本号1 版本号2 查看两个指定的版本之间不同的地方。这里的版本号指的是commit的hash值 添加文件 add -A 这算是相当通用的了。在commit之前要先add 撤回stage的东西 checkout —. 这里用小数点表示撤回所有修改，在—的前后都有空格 提交 commit -m “提交信息” 提交信息最好能体现更改了什么 删除未tracked clean -xf 删除当前目录下所有没有track过的文件。不管它是否是.gitignore文件里面指定的文件夹和文件 查看提交记录 log 查看当前版本及之前的commit记录 reflog HEAD的变更记录 版本回退 reset —hard 版本号 回退到指定版本号的版本，该版本之后的修改都被删除。同时也是通过这个命令回到最新版本。需要reflog配合 2. 远程使用 行为 命令 备注 设置用户名 config —global user.name “你的用户名” 设置邮箱 config —global user.email “你的邮箱” 生成ssh key ssh-keygen -t rsa -C “你的邮箱” 这条命令前面不用加git 添加远程仓库 remote add origin 你复制的地址 设置origin 上传并指定默认 push -u origin master 指定origin为默认主机，以后push默认上传到origin上 提交到远程仓库 push 将当前分支增加的commit提交到远程仓库 从远程仓库同步 pull 在本地版本低于远程仓库版本的时候，获取远程仓库的commit 可以用一张图直观地看出以上主要的命令对仓库的影响(图片引用自：Git introduction for CVS/SVN/TFS users) workspace 即工作区，逻辑上是本地计算机，还没添加到repository的状态； staging 即版本库中的stage，是暂存区。修改已经添加进repository，但还没有作为commit提交，类似于缓存； Local repository 即版本库中master那个地方。到这一步才算是成功生成一个新版本； Remote repository 则是远程仓库。用来将本地仓库上传到网络，可以用于备份、共享、合作。本文将使用Github作为远程仓库的例子。 3. Git for Windows软件安装官网上并没有找到离线的安装包，在百度云上找到一个离线安装包，下载地址 密码： ，安装的时候一路点击Next就行了,安装完成后在桌面会看到两个图标，“GitHub” , “Git Shell” 。GitHub是桌面端工具，此处不做介绍。用Git Shell介绍相关命令的使用。 运行 git init来初始化仓库,它会创建一个隐藏的文件夹 .git 在当前仓库创建一个文件 readme.txt git status 命令查看变化信息(它告诉我有一个还未追踪的文件，并提示我可以使用 git add &lt;file&gt;... 把它加进去。) git add -A 命令将未追踪的文件加进去 再次使用 git status 查看变化信息（状态变了，说明添加成功。再看看它的提示 Changes to be committed ，也就是说现在可以执行commit了。） 执行命令git commit -m &quot;提交信息&quot;将文件提交到repository里。提交信息用英文的双引号括起来。 执行命令git log 就可以看到提交记录了 文件修改，修改readme.txt 文件内容，并使用命令git status 查看状态(比较一下就会看到，之前的是添加新文件，当时文件还没被追踪（untracked），而这次是更改已经追踪（tracked）的文件。) 执行命令git diff查看文件做了那些变化,它默认跟最新的一个commit进行比较。红色（前面有减号-）表示删除，绿色（前面有加号+）表示添加。因此，在git看来，我们是删除了原来那一行，并添加了新的两行。这在文件内容特别多的时候效果比较明显。这个命令在以下情况可以使用：你忘记改了什么，又想知道别人发给你新版本，你想知道更改了什么) 执行命令git checkout -- .撤销这些更改,并用git status 查看状态 再次修改，并提交使用git add -A 与 git commit -m &quot;新增内容&quot;，并用git log 查看日志（现在有两个提交，我们看到两行黄色部分是以 commit 开头的，后面接着一串字符。这一串字符是16进制的数，是一串哈希值，这就是版本号） 版本回退，执行 git reset --hard 04976dd （取版本号前7位就可以了）： 清除未追踪的文件通常在reset或者pull之前要做两件事,1.将新添加且未追踪的文件删除掉，2.已追踪的文件已有修改，但又不需要这些修改，则将它们还原。执行命令git clean -xf 清除untracked文件 4.Github与Git的关联上面的操作都是在本地计算机上产生影响的，一般也够用了。如果你想和其他人分享你的代码，或者合作开发，可以用Github。 到Github注册账号。 本地配置用户名和邮箱，使用命令git config --global user.name &quot;你的用户名&quot;,`git config —global user.email “你的邮箱” 生成ssh key,执行命令ssh-keygen -t rsa -C &quot;你的邮箱&quot;,它会有三次等待你输入，直接回车即可。 将生成的ssh key复制到剪贴板，执行 clip &lt; ~/.ssh/id_rsa.pub （或者到上图提示的路径里去打开文件并复制）： 打开Github，进入Settings： 点击左边的 SSH and GPG keys ，将ssh key粘贴到右边的Key里面。Title随便命名即可 点击下面的 Add SSH key 就添加成功了。 执行 ssh -T git@github.com ：提示如下信息时，成功 创建远程仓库 首先是在右上角点击进入创建界面： 接着输入远程仓库名：yangql881012.github.io点击 Create repository 就创建好了 将远程仓库和本地仓库关联起来 先到Github上复制远程仓库的SSH地址执行命令git remote add origin 你复制的地址 执行 git push -u origin master 将本地仓库上传至Github的仓库并进行关联 以后想在commit后同步到Github上，只要直接执行 git push 就行 第二章 Hexo安装与配置这篇教程是针对与Windows的。 1.安装Node.jshexo是一款基于Node.js的静态博客框架,所以在安装Hexo之前得安装Node.js。官网上下载Node.js最新版本，我下载的版本是node-v7.1.0-x64.msi,一路下一步，安装完成后，重启电脑。 2.安装hexo 进入Git Bash,使用以下命令安装hexo($是提示符)1234$ npm install hexo-cli -g$ npm install hexo-deployer-git --save$ hexo g$ hexo d 如果网络很慢，可以执行以下命令12npm install -g cnpm --registry=https://registry.npm.taobao.orgccnpm install hexo-cli -g 3.创建博客目录接下来创建放置博客文件的文件夹：yangql881012.github.io文件夹。我hexo文件夹的位置为E:\\学习资料\\97_github\\yangql881012.github.io，名字和地方可以自由选择。之后进入文件夹，即E:\\学习资料\\97_github\\yangql881012.github.io内，点击鼠标右键，选择Git Bash，执行以下命令，Hexo会自动在该文件夹下下载搭建网站所需的所有文件。12$ hexo init$ npm install 注：npm install 安装依赖包执行以下命令，相看相应效果12$ hexo g$ hexo s 然后用浏览器访问http://localhost:4000/，此时，你应该看到了一个漂亮的博客了。 4.部署本地文件到github既然Repository已经创建了，当然是先把博客放到Github上去看看效果。编辑E:\\学习资料\\97_github\\yangql881012.github.io下的_config.yml文件，建议使用Notepad++。在_config.yml最下方，添加如下配置(命令中的第一个yangql881012为Github的用户名,第二个yangql881012.github.io为之前New的Repository的名字,记得改成自己的。另外记得一点，hexo的配置文件中任何’:’后面都是带一个空格的),如果配置以下命令出现ERROR Deployer not found : github，则参考上文的解决方法。1234deploy: type: git repository: http://github.com/yangql881012/yangql881012.github.io.git branch: master 配置好_config.yml并保存后，执行以下命令部署到Github上。 12$ hexo g$ hexo d 此时，我们的博客已经搭建起来，并发布到Github上了，在浏览器访问yangql881012.github.io就能看到自己的博客了。 5.hexo的配置文件hexo里面有两个常用到的配置文件，分别是整个博客的配置文件E:\\学习资料\\97_github\\yangql881012.github.io\\_config.yml和主题的配置文件E:\\学习资料\\97_github\\yangql881012.github.io\\themes\\next\\_config.yml，此地址是对于我来说，hexo3.0使用的默认主题是landscape，因此你们的地址应该是E:\\学习资料\\97_github\\yangql881012.github.io\\themes\\landscape\\_config.yml，可在这两个文件里修改相应的配置信息。 博客配置文件 _config.yml Hexo Configuration12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: 不圆的珠子subtitle:description:author: 不圆的珠子language: zh-Hans #设置语言timezone:# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://www.yangql.cnroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/#theme: landscapetheme: nextfeed: #之后配置rss会用，使用如下配置即可 type: atom path: atom.xml limit: 20 # Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: http://github.com/yangql881012/yangql881012.github.io.git branch: masteravatar: /images/avatar.jpgduoshuo_shortname: yangql881012#添加搜索search: path: search.xml field: post format: html limit: 10000 主题配置文件 _config.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418# ---------------------------------------------------------------# Site Information Settings# ---------------------------------------------------------------# Put your favicon.ico into `hexo-site/source/` directory.favicon: /favicon.ico# Set default keywords (Use a comma to separate)keywords: &quot;Hexo, NexT&quot;# Set rss to false to disable feed link.# Leave rss as empty to use site&apos;s feed link.# Set rss to specific value if you have burned your feed already.rss:# Specify the date when the site was setupsince: 2016 #网站时间 从xx开始 类似 1990-2016# Canonical, set a canonical link tag in your hexo, you could use it for your SEO of blog.# See: https://support.google.com/webmasters/answer/139066# Tips: Before you open this tag, remeber set up your URL in hexo _config.yml ( ex. url: http://yourdomain.com )canonical: true# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash (/archives -&gt; archives)menu: home: / categories: /categories archives: /archives tags: /tags #schedule: /schedule about: /about #commonweal: /404.html# Enable/Disable menu icons.# Icon Mapping:# Map a menu item to a specific FontAwesome icon name.# Key is the name of menu item and value is the name of FontAwsome icon. Key is case-senstive.# When an question mask icon presenting up means that the item has no mapping icon.menu_icons: enable: true #KeyMapsToMenuItemKey: NameOfTheIconFromFontAwesome home: home about: user categories: th schedule: calendar tags: tags archives: archive commonweal: heartbeat# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse#scheme: Mistscheme: Pisces# ---------------------------------------------------------------# Font Settings# - Find fonts on Google Fonts (https://www.google.com/fonts)# - All fonts set here will have the following styles:# light, light italic, normal, normal intalic, bold, bold italic# - Be aware that setting too much fonts will cause site running slowly# - Introduce in 5.0.1# ---------------------------------------------------------------font: enable: true # Uri of fonts host. E.g. //fonts.googleapis.com (Default) host: # Global font settings used on &lt;body&gt; element. global: # external: true will load this font family from host. external: true family: Lato # Font settings for Headlines (h1, h2, h3, h4, h5, h6) # Fallback to `global` font settings. headings: external: true family: # Font settings for posts # Fallback to `global` font settings. posts: external: true family: # Font settings for Logo # Fallback to `global` font settings. # The `size` option use `px` as unit logo: external: true family: size: # Font settings for &lt;code&gt; and code blocks. codes: external: true family: size:# ---------------------------------------------------------------# Sidebar Settings# ---------------------------------------------------------------# Social Links# Key is the link label showing to end users.# Value is the target link (E.g. GitHub: https://github.com/iissnan)#social: #LinkLabel: Link# Social Links Icons# Icon Mapping:# Map a menu item to a specific FontAwesome icon name.# Key is the name of the item and value is the name of FontAwsome icon. Key is case-senstive.# When an globe mask icon presenting up means that the item has no mapping icon.social_icons: enable: true # Icon Mappings. # KeyMapsToSocalItemKey: NameOfTheIconFromFontAwesome GitHub: github Twitter: twitter Weibo: weibo# Sidebar Avatar# in theme directory(source/images): /images/avatar.jpg# in site directory(source/uploads): /uploads/avatar.jpg#avatar:# Table Of Contents in the Sidebartoc: enable: true # Automatically add list number to toc. number: true# Creative Commons 4.0 International License.# http://creativecommons.org/# Available: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero#creative_commons: by-nc-sa#creative_commons:sidebar: # Sidebar Position, available value: left | right #position: left position: right # Sidebar Display, available value: # - post expand on posts automatically. Default. # - always expand for all pages automatically # - hide expand only when click on the sidebar toggle icon. # - remove Totally remove sidebar including sidebar toggler. display: post #display: always #display: hide #display: remove# Blogrolls#links_title: Links#links_layout: block#links_layout: inline#links: #Title: http://example.com/# ---------------------------------------------------------------# Misc Theme Settings# ---------------------------------------------------------------# Custom Logo.# !!Only available for Default Scheme currently.# Options:# enabled: [true/false] - Replace with specific image# image: url-of-image - Images&apos;s urlcustom_logo: enabled: false image:# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal# Automatically scroll page to section which is under &lt;!-- more --&gt; mark.scroll_to_more: true# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150# Wechat Subscriber#wechat_subscriber: #enabled: true #qcode: /path/to/your/wechatqcode ex. /uploads/wechat-qcode.jpg #description: ex. subscribe to my blog by scanning my public wechat account# ---------------------------------------------------------------# Third Party Services Settings# ---------------------------------------------------------------# MathJax Supportmathjax: enable: true cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML# Swiftype Search API Key#swiftype_key:# Baidu Analytics ID#baidu_analytics:# Duoshuo ShortName#duoshuo_shortname:# Disqus#disqus_shortname:# Baidu Share# Available value:# button | slide# Warning: Baidu Share does not support https.#baidushare:## type: button# Share#jiathis:# Warning: JiaThis does not support https.#add_this_id:# Share#duoshuo_share: true# Google Webmaster tools verification setting# See: https://www.google.com/webmasters/#google_site_verification:# Google Analytics#google_analytics:# CNZZ count#cnzz_siteid:# Make duoshuo show UA# user_id must NOT be null when admin_enable is true!# you can visit http://dev.duoshuo.com get duoshuo user id.duoshuo_info: ua_enable: true admin_enable: false user_id: 0 #admin_nickname: Author# Facebook SDK Support.# https://github.com/iissnan/hexo-theme-next/pull/410facebook_sdk: enable: false app_id: #&lt;app_id&gt; fb_admin: #&lt;user_id&gt; like_button: #true webmaster: #true# Facebook comments plugin# This plugin depends on Facebook SDK.# If facebook_sdk.enable is false, Facebook comments plugin is unavailable.facebook_comments_plugin: enable: false num_of_posts: 10 # min posts num is 1 width: 100% # default width is 550px scheme: light # default scheme is light (light or dark)# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: #&lt;app_id&gt; app_key: #&lt;app_key&gt;# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: false # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer:# Tencent analytics ID# tencent_analytics:# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: false# Google Calendar# Share your recent schedule to others via calendar page## API Documentation:# https://developers.google.com/google-apps/calendar/v3/reference/events/listcalendar: enable: false calendar_id: &lt;required&gt; api_key: &lt;required&gt; orderBy: startTime offsetMax: 24 offsetMin: 4 timeZone: showDeleted: false singleEvents: true maxResults: 250#! ---------------------------------------------------------------#! DO NOT EDIT THE FOLLOWING SETTINGS#! UNLESS YOU KNOW WHAT YOU ARE DOING#! ---------------------------------------------------------------# Motionuse_motion: true# Fancyboxfancybox: true# Script Vendors.# Set a CDN address for the vendor you want to customize.# For example# jquery: https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js# Be aware that you should use the same version as internal ones to avoid potential problems.# Please use the https protocol of CDN files when you enable https on your site.vendors: # Internal path prefix. Please do not edit it. _internal: lib # Internal version: 2.1.3 jquery: # Internal version: 2.1.5 # See: http://fancyapps.com/fancybox/ fancybox: fancybox_css: # Internal version: 1.0.6 # See: https://github.com/ftlabs/fastclick fastclick: # Internal version: 1.9.7 # See: https://github.com/tuupola/jquery_lazyload lazyload: # Internal version: 1.2.1 # See: http://VelocityJS.org velocity: # Internal version: 1.2.1 # See: http://VelocityJS.org velocity_ui: # Internal version: 0.7.9 # See: https://faisalman.github.io/ua-parser-js/ ua_parser: # Internal version: 4.4.0 # See: http://fontawesome.io/ fontawesome:# Assetscss: cssjs: jsimages: images# Theme versionversion: 5.0.2 6.发布一篇文章 在Git Bash执行命令：$ hexo new “my new post” 在E:\\yangql881012.github.io\\source_post中打开my-new-post.md，打开方式使用记事本或notepad++。1234567title: my new post #文章标题date: 2016-11-12 22:56:29 #发表日期，一般不改动categories: blog #文章文类tags: [博客，文章] #文章标签，多于一项时用这种格式，只有一项时使用tags: blog---#这里是正文，用markdown写，你可以选择写一段显示在首页的简介后，加上&lt;!--more--&gt;#在&lt;!--more--&gt;之前的内容会显示在首页，之后的内容会被隐藏，当游客点击Read more才能看到。 写完文章后，你可以使用以下命令： $ hexo g生成静态文件。 $ hexo s在本地预览效果。 hexo d同步到github 使用http://huangjunhui.github.io进行访问。 第3章 安装nexTHexo 安装主题的方式非常简单，只需要将主题文件拷贝至站点目录的 themes 目录下， 然后修改下配置文件即可。具体到 NexT 来说，安装步骤如下。 1.下载主题 打开 Git Bash cd E:\\学习资料\\97_github\\yangql881012.github.io git clone https://github.com/iissnan/hexo-theme-next themes/next2.启用主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。1theme: next 到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 我们最好使用 hexo clean 来清除 Hexo 的缓存。 3.验证主题首先启动 Hexo 本地站点，并开启调试模式（即加上 —debug），整个命令是 hexo s —debug。 在服务启动的过程，注意观察命令行输出是否有任何异常信息，如果你碰到问题，这些信息将帮助他人更好的定位错误。 当命令行输出中提示出：INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop.此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 4.主题设定Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新Scheme 的切换通过更改 主题配置文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 即可。123#scheme: Muse#scheme: Mistscheme: Pisces 更多信息请见nexT官方文档 第四章 图片使用1.首先确认_config.yml中有：1post_asset_folder: true 2.然后在yangql881012.github.io/下执行1npm install https://github.com/CodeFalling/hexo-asset-image --save 3.确保在yangql881012.github.io/source/_posts下创建和md文件同名的目录，在里面放该md需要的图片，然后在md中插入1![](目录名/文件名.png) 即可在hexo generate时正确生成插入图片。比如： 1234|- post1.md|_ post1 |- pic1.png 在md文件中插入图片时只需写 1![](post1/pic1.png) 首次配置完了需要执行一次清除操作，再生成页面：123$ hexo clean$ hexo generate$ hexo server","categories":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://qioinglong.top/tags/Hexo/"}],"keywords":[{"name":"技术","slug":"技术","permalink":"http://qioinglong.top/categories/技术/"}]},{"title":"银行业务概览","slug":"银行_银行业务概览","date":"2015-05-25T00:55:50.000Z","updated":"2019-05-26T07:19:03.619Z","comments":true,"path":"2015/05/25/银行_银行业务概览/","link":"","permalink":"http://qioinglong.top/2015/05/25/银行_银行业务概览/","excerpt":"&nbsp;&nbsp;&nbsp;&nbsp;商业银行经营业务种类繁多，主要分为负债业务(商业银行的资金来源)，资产业务(商业银行运用资金的业务)，中间业务(银行不运用自身的资金，利用自己信用关系，代客承办支付和其它委托事项而收取的手续费)","text":"&nbsp;&nbsp;&nbsp;&nbsp;商业银行经营业务种类繁多，主要分为负债业务(商业银行的资金来源)，资产业务(商业银行运用资金的业务)，中间业务(银行不运用自身的资金，利用自己信用关系，代客承办支付和其它委托事项而收取的手续费) 1.资产业务资产业务，是商业银行的主要收入来源 1.1.贷款业务贷款业务是商业银行最主要的资产业务，是实现收入的主要来源，主要有信用贷款，抵押贷款，质押贷款、担保贷款以及贷款证券化。 1.1.1.信用贷款信用贷款，指凭借款人的信誉，而不需提供任何抵押品的贷款。主要包含普通借款限额、透支贷款、贷款承诺、消费贷款、贴现 普通贷款限额客户与银行订立一种非正式协议，以确定贷款额度，在限定额度内，客户可随时得到银行的贷款支持，限额的有效期一般不超过９０天。普通贷款限额内的贷款，利率是浮动的，与银行的优惠利率挂钩。 透支贷款：银行允许客户在其帐户上以透支的方式向客户提供贷款。 贷款承诺：贷款承诺，是一种比较正式和具有法律约束的协议。银行与客户签订正式合同，在合同中银行承诺在指定期限和限额内向客户提供相应贷款，客户要为银行的承诺提供费用。 消费贷款：消费贷款是向消费者个人发放的用于购买耐用消费品或支付其他费用的贷款。 贴现：也称票据贴现贷款，是顾客将未到期的票据提交银行，由银行扣除自贴现日起至到期日止的利息而取得现款。 1.1.2.抵押贷款抵押贷款，又称“抵押放款”。是指某些国家银行采用的一种贷款方式。要求借款方提供一定的抵押品作为贷款的担保，以保证贷款的到期偿还。抵押品一般为易于保存，不易损耗，容易变卖的物品，如有价证券、票据、股票、房地产等。贷款期满后，如果借款方不按期偿还贷款，银行有权将抵押品拍卖，用拍卖所得款偿还贷款。拍卖款清偿贷款的余额归还借款人。如果拍卖款不足以清偿贷款，由借款人继续清偿。主要分为以下几类： 存货抵押：又称商品抵押，指用客户掌握的各种货物，包括商品、原材料，在制品和制成品抵押，向银行申请贷款。 客账抵押：客户把应收账款作为担保取得短期贷款。 证券抵押：以各种有价证券如：股票、汇票、期票、存单、债券等作为抵押，取得短期贷款。 设备抵押：以机械设备、车辆、船舶等作为担保向银行取得定期贷款 不动产抵押：借款人提供如：土地、房屋等不动产抵押，取得贷款 人寿保险单抵押：是指在保险金请求权上设立抵押权，它以人寿保险合同的退保金为限额，以保险单为抵押，对被保险人发放贷款。 1.1.3.担保贷款担保贷款，是指由经第三者出具保证书担保的贷款。保证书是保证为借款人作贷款担保，与银行的契约性文件，其中规定了银行和保证人的权利和义务。银行只要取得经保证人签字的银行拟定的标准格式保证书，即可向借款人发放贷款。所以，保证书是银行可以接受的最简单的担保形式。 1.1.贷款证券化贷款证券化是指商业银行通过一定程序将贷款转化为证券发行的过程。具体做法是：商业银行将所持有的各种流动性较差的贷款，组合成若干个资产库（Assets Pool），出售给专业性的融资公司，再由融资公司以这些资产库为担保，发行资产抵押证券。这种资产抵押证券同样可以通过证券发行市场发行或私募的方式推销给投资者。出售证券所收回的资金则可做为商业银行新的资金来源再用于发放其它贷款。 1.2.投资业务商业银行的投资业务是指银行购买有价证券的活动。投资是商业银行一项重要的资产业务，是银行收入的主要来源之一。商业银行的投资业务，按照对象的不同，可分为国内证券投资和国际证券投资。 2.负债业务负债是银行由于授信而承担的将以资产或资本偿付的能以货币计量的债务。存款、派生存款是银行的主要负债，约占资金来源的80％以上，另外联行存款、同业存款、借入或拆入款项或发行债券等，也构成银行的负债。 2.1.活期存款活期存款是相对于定期存款而言的，是不需预先通知可随时提取或支付的存款。活期存款构成了商业银行的重要资金来源，也是商业银行创造信用的重要条件。但成本较高。商业银行只向客户免费或低费提供服务，一般不支付或较少支付利息。 2.2.定期存款定期存款是相对于活期存款而言的，是一种由存户预先约定期限的存款。定期存款占银行存款比重较高。因为定期存款固定而且比较长，从而为商业银行提供了稳定的资金来源，对商业银行长期贷款与投资具有重要意义。 2.3.储蓄存款储蓄存款，是指个人或公司将属于其所有的人民币或者外币存入储蓄机构，储蓄机构开具存折或者存单作为凭证，个人凭存折或存单可以支取存款的本金和利息，储蓄机构依照规定支付存款本金和利息的活动。储蓄存款是社会公众将当期暂时不用的收入存入银行而形成的存款。储蓄存款的存户一般限于个人。传统的储蓄存款不能开支票进行支付，可以获得利息。这种存款通常由银行给存款人发一张存折，作为存款和提取存款的凭证。储蓄存款的存折不具有流通性，不能转让和贴现。 2.4.可转让定期存单（CDs）可转让定期存单存款是定期存款的一种主要形式，但与前述定期存款又有所区别。可转让存单存款的明显特点是：存单面额固定，不记姓名，利率有固定也有浮动，存期为３个月、６个月、９个月和１２个月不等。存单能够流通转让，以能够满足流动性和盈利性的双重要求。 2.5.可转让支付命令存款帐户它实际上是一种不使用支票的支票帐户。它以支付命令书取代了支票。通过此帐户，商业银行既可以提供支付上的便利，又可以支付利息，从而吸引储户，扩大存款。 2.6.自动转帐服务存款帐户这一帐户与可转让支付命令存款帐户类似，是在电话转帐服务基础上发展而来。发展到自动转帐服务时，存户可以同时在银行开立两个帐户：储蓄帐户和活期存款帐户。银行收到存户所开出的支票需要付款时，可随即将支付款项从储蓄帐户上转到活期存款帐户上，自动转帐，即时支付支票上的款项。 2.7.掉期存款掉期存款指的是顾客在存款时把手上的代币兑换成其所选择的外币，作为外币定期存款存入银行。到期满时顾客先将外币存款连本带息兑回本币后才提取。存款期限由一个月至一年不等。 3.中间业务中间业务又称表外业务，其收入不列入银行资产负债表。主要有结算业务、信用证业务、信托业务、租赁业务、代理业务、银行卡业务、咨询业务等。 3.1.结算业务结算业务是由商业银行的存款业务衍生出来的一种业务。使用包括银行汇票、商业汇票、银行本票和支票等结算工具开展相关业务。 银行汇票由企业单位或个人将款项交存开户银行，由银行签发给其持往异地采购商品时办理结算或支配现金的票据。 商业汇票由企业签发的一种票据，适用于企业单位先发货后收款或双方约定延期付款的商品交易。 银行本票申请人将款项交存银行，由银行签发给其凭以办理转帐或支取现金的票据。可分为不定额本票和定额本票。 支票由企业单位或个人签发的，委托其开户银行付款的票据，是我国传统的票据结算工具，可用于支取现金和转帐。 3.2.信用证业务信用证（Letter of Credit），作为商业贸易的手段之一，银行信用证是进口商的代理银行为进口商提供自身的信用，保证在一定的条件下承付出口商开给进口商的票据，即将所开票据当作是开给本行的票据。所谓信用证即是保证承付这些票据的证书。 银行信用证银行信用证(（Bank Credit）)汇票的接受人是银行，开证行或受其委托的保兑银行承兑开给自己的汇票。 不可撤销信用证与可撤销信用证不可撤销信用证（IrrevocableCredit），是指开证行一旦开立了信用证并将之通知了受益人，在其有效期间，如若没有开证委托人、受益人或已依据此信用证贴现汇票的银行的同意，不可单方面地撤销此信用证，也不可变更其条件。 保兑信用证与不保兑信用证开证行以外的银行对卖方开出的汇票保证兑付，这种信用证称为保兑信用证（Confirmed Credit），而无此保证者则称为不保兑信用证（UnconfirmedCredit）。 一般信用证和特定信用证信用证的开证行特别指定某一银行贴现根据此信用证开出的汇票，这种信用证称为特定信用证（Special or Restricted Credit ），而不限定贴现银行者称为普通信用证（Generalor Open Credit）。 3.3.信托业务信托（Trust），可以从两方面考察，从委托人来说，信托是为自己或为第三者的利益，把自己的财产委托别人管理或处理的一种行为；从受托人来说，信托是受委托人的委托，为了受益人的利益，代为管理、营运或处理信托人托管财产的一种过程。广义的信托还包括代理业务，如受托代办有价证券的签证、发行、收回、掉换、转让、还本付息以及代客保管物品等。 信托与代理的主要区别在于财产权是否转移，如果财产权从委托人转移到受托人，则是信托关系，而代理则不涉及财产权转移。 信托关系（FiduciaryRelationship）是一种包括委托人、受托人和受益人在内的多边关系。 信托种类按信托方式划分主要有投资信托、融资信托、公益信托、职工福利信托。 3.4.租赁业务银行的租赁业务主要包括融资性租赁（Financial Lease ）、操作性租赁（Operating Lease）、出售与返租式租赁（Sale and Lease back）、转租赁（Sublease ） 融资性租赁（Financial Lease ）是以融通资金为目的租赁。一般先由承租人自行从供货处选好所需设备，并谈妥交易条件，然后找出租人（金融机构或其附属的专业子公司），要求后者按谈妥的条件向供货商购买设备，并签订租赁合同，取得设备使用权，并按期交纳租金。这时出租人支付了全部资金，等于提供了百分之百的信贷，因此又叫融资性租赁或资本性租赁。 操作性租赁（Operating Lease）又叫服务性租赁，是由出租人向承租人提供一种特殊服务的租赁。这种特殊服务主要是指设备的短期使用或利用服务，如出租人买下库房、车船、电子计算机等，然后出租给承租人。操作性租赁通常适用于一些需要专门技术进行保养、技术更新、使用频度不高的设备。 出售与返租式租赁（Sale and Lease back）是财产所有人将其财产出售以后又租回使用的一种租赁方式。这种租赁的后半段与一般租赁完全相同，只是增加了前半段的出售过程，财产所有者又变成了财产使用者。 转租赁（Sublease ） 是将设备或财产进行两次重复租赁的方式。国际租赁中通常采用这种租赁方式。 3.5.代理业务代理融通（Factoring）又叫代收帐款或收买应收帐款，是由商业银行或专业代理融通公司代顾客收取应收款项，并向顾客提供资金融通的一种业务方式。代理融通业务一般涉及三方面当事人，一是商业银行或经营代理融通业务的公司，二是出售应收帐款、取得资金融通的工商企业，三是取得商业信用及赊欠工商企业贷款的顾客。三者的关系是，工商企业对顾客赊销货物或劳务，然后把应收的赊销帐款转让给银行或代理融通公司，由后者向企业提供资金并到期向顾客收帐。 3.6.银行卡业务商业银行的银行卡业务主要包括借记卡和信用卡 借记卡借记卡是指发卡银行向持卡人签发的，没有信用额度，持卡人先存款、后使用的银行卡。 信用卡信用卡是消费信贷的一种工具和形式，具有“先消费”、方便消费者的特点。信用卡的种类很多，除银行发行的信用卡外，还有商业和其他服务业发行的零信用卡、旅游娱乐卡等。 3.7.咨询业务在现代社会，信息已成为社会发展的主要支柱之一。商业银行通过资金运动的记录，以及与资金运动相关资料的收集整理，可以为企业提供丰富实用的经济信息。其主要内容有：企业财务资料资信评价；商品市场供需结构变化趋势介绍；金融市场动态分析。","categories":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}],"tags":[{"name":"银行","slug":"银行","permalink":"http://qioinglong.top/tags/银行/"}],"keywords":[{"name":"金融业务","slug":"金融业务","permalink":"http://qioinglong.top/categories/金融业务/"}]}]}