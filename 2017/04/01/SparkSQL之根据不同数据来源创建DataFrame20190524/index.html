<!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="不圆的石头">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://qioinglong.top">
    <!--SEO-->





<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->


<title>SparkSQL之根据不同数据来源创建DataFrame | 不圆的石头</title>


    <link rel="alternate" href="/atom.xml" title="不圆的石头" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    

    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="main-header" style="background-image:url(http://snippet.shenliyang.com/img/banner.jpg)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="不圆的石头">
         <!--   <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block"> -->
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                 <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
    	</div>
    </div>
</header>

    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://qioinglong.top">不圆的石头</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/金融业务/"><i class="fa "></i>金融业务</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/数据分析/"><i class="fa "></i>数据分析</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/读书/"><i class="fa "></i>读书</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/技术/"><i class="fa "></i>技术</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="SparkSQL之根据不同数据来源创建DataFrame">
            
	            SparkSQL之根据不同数据来源创建DataFrame
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/技术">
            技术
        </a>
    </span>
    

    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/Spark" title="Spark">
                        Spark
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2017/04/01</span>
        </span>
        
    
</div>

            
            
            <p class="fa fa-exclamation-triangle warning">
                本文于<strong>802</strong>天之前发表。
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>本文主要介绍根据不同数据来源创建DataFrame，主要有JDBC数据源，Hive数据源，JSON数据源，Parquet数据源。<br><a id="more"></a></p>
<h2 id="2-JDBC数据源创建DataFrame"><a href="#2-JDBC数据源创建DataFrame" class="headerlink" title="2. JDBC数据源创建DataFrame"></a>2. JDBC数据源创建DataFrame</h2><p>关键点：</p>
<ul>
<li>连接数据库</li>
<li>使用SQLContext.read.format(“jdbc”)</li>
<li><p>options 参数配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">options(Map(</span><br><span class="line">    &quot;url&quot;-&gt;url,</span><br><span class="line">    &quot;dbtable&quot;-&gt;&quot;student_score&quot;,</span><br><span class="line">    &quot;user&quot;-&gt;userName,</span><br><span class="line">    &quot;password&quot;-&gt;password</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure>
</li>
<li><p>将DataFrame数据保存到关系数据库中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">studentStructDF.write.saveAsTable(&quot;good_student&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实例代码:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">package com.spark.sql</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Administrator on 2017/3/12.</span><br><span class="line">  */</span><br><span class="line">object JDBCDataSrc extends  App&#123;</span><br><span class="line">  val conf = new SparkConf()</span><br><span class="line">    .setMaster(&quot;local&quot;)</span><br><span class="line">    .setAppName(&quot;JDBCDataSrc&quot;)</span><br><span class="line">  val sc = new SparkContext(conf)</span><br><span class="line">  val sqlContext=new SQLContext(sc)</span><br><span class="line">  import sqlContext.implicits._</span><br><span class="line">  val url=&quot;jdbc:mysql://localhost:3306/sparkpro&quot;</span><br><span class="line">  val userName=&quot;root&quot;</span><br><span class="line">  val password=&quot;root&quot;</span><br><span class="line">  //创建Dataframe</span><br><span class="line">  val studenInfoDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map(</span><br><span class="line">    &quot;url&quot;-&gt;url,</span><br><span class="line">    &quot;dbtable&quot;-&gt;&quot;student_info&quot;,</span><br><span class="line">    &quot;user&quot;-&gt;userName,</span><br><span class="line">    &quot;password&quot;-&gt;password</span><br><span class="line">  )).load()</span><br><span class="line">  studenInfoDF.show()</span><br><span class="line"></span><br><span class="line">  //创建Dataframe</span><br><span class="line">  val studentScoreDF=sqlContext.read.format(&quot;jdbc&quot;).options(Map(</span><br><span class="line">    &quot;url&quot;-&gt;url,</span><br><span class="line">    &quot;dbtable&quot;-&gt;&quot;student_score&quot;,</span><br><span class="line">    &quot;user&quot;-&gt;userName,</span><br><span class="line">    &quot;password&quot;-&gt;password</span><br><span class="line">  )).load()</span><br><span class="line">  //studentScoreDF 转换为RDD，并过滤出分数大于80分的学生</span><br><span class="line">  val goodStudentRDD=studentScoreDF.rdd.filter(row=&gt;(row.getAs[Int](&quot;score&quot;)&gt;=80))</span><br><span class="line"> // for (elem &lt;- goodStudentRDD.collect()) &#123;</span><br><span class="line"> //   print(elem)</span><br><span class="line"> // &#125;</span><br><span class="line"></span><br><span class="line">  //a RDD for studenfInfo</span><br><span class="line">  val studenInfoRDD=studenInfoDF.rdd.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;age&quot;)))</span><br><span class="line">    .join(goodStudentRDD.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Int](&quot;score&quot;))))</span><br><span class="line">  val studenInfoRowRDD=studenInfoRDD.map(row=&gt;Row(row._1,row._2._1.toString.toLong,row._2._2.toString.toLong))</span><br><span class="line">  //studenInfoRDD.foreach(println)</span><br><span class="line">  //将RDD转化为DataFrame</span><br><span class="line">   val studentStruct=StructType(Array(</span><br><span class="line">    StructField(&quot;name&quot;,StringType,true),</span><br><span class="line">    StructField(&quot;age&quot;,LongType,true),</span><br><span class="line">    StructField(&quot;score&quot;,LongType,true)</span><br><span class="line">  ))</span><br><span class="line">  val studentStructDF=sqlContext.createDataFrame(studenInfoRowRDD,studentStruct)</span><br><span class="line">  studentStructDF.write.saveAsTable(&quot;good_student&quot;)</span><br><span class="line">  //将数据插入到数据库中</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Hive-数据源创建DataFrame"><a href="#3-Hive-数据源创建DataFrame" class="headerlink" title="3. Hive 数据源创建DataFrame"></a>3. Hive 数据源创建DataFrame</h2><p>关键点：</p>
<ul>
<li>创建HiveContext</li>
<li>hiveContext.sql() 方法创建表，删除表，加载数据，查询SQL返回DataFrame</li>
<li><p>将数据保存到Hive中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">goodStudentsDF.write.saveAsTable(&quot;GOOD_STUDENT&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>示例代码:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package com.spark.sql</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Administrator on 2017/3/12.</span><br><span class="line">  */</span><br><span class="line">object HiveDataSource extends App&#123;</span><br><span class="line">  val conf = new SparkConf()</span><br><span class="line">    .setAppName(&quot;HiveDataSource&quot;)</span><br><span class="line">  val sc = new SparkContext(conf)</span><br><span class="line">  val hiveContext = new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">  //创建student_infos表</span><br><span class="line">  hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_INFO&quot;)</span><br><span class="line">  //判断表是否存在，不存在则创建</span><br><span class="line">  hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_INFO(NAME STRING,AGE INT)&quot;)</span><br><span class="line">  //导入数据</span><br><span class="line">  hiveContext.sql(&quot;LOAD DATA &quot;</span><br><span class="line">    + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_info.txt&apos; &quot;</span><br><span class="line">    + &quot;INTO TABLE STUDENT_INFO&quot;)</span><br><span class="line"></span><br><span class="line">  //创建student_score表</span><br><span class="line">  hiveContext.sql(&quot;DROP TABLE IF EXISTS STUDENT_SCORE&quot;)</span><br><span class="line">  //判断表是否存在，不存在则创建</span><br><span class="line">  hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS STUDENT_SCORE (NAME STRING,SCORE INT)&quot;)</span><br><span class="line">  //导入数据</span><br><span class="line">  hiveContext.sql(&quot;LOAD DATA &quot;</span><br><span class="line">    + &quot;LOCAL INPATH &apos;/home/yangql/spark-study/sql/student_score.txt&apos; &quot;</span><br><span class="line">    + &quot;INTO TABLE STUDENT_SCORE&quot;)</span><br><span class="line">  //查询分数大于80的学生信息,并保存到good_student信息</span><br><span class="line">  val goodStudentsDF=hiveContext.sql(&quot;SELECT T1.NAME,T1.AGE,T2.SCORE &quot; +</span><br><span class="line">    &quot;FROM student_info T1 &quot; +</span><br><span class="line">    &quot;INNER JOIN student_SCORE T2 &quot; +</span><br><span class="line">    &quot;ON T1.NAME=T2.NAME &quot; +</span><br><span class="line">    &quot;WHERE T2.SCORE&gt;80&quot;)</span><br><span class="line">  hiveContext.sql(&quot;DROP TABLE IF EXISTS GOOD_STUDENT&quot;)</span><br><span class="line">  //goodStudentsDF.saveAsTable(&quot;GOOD_STUDENT&quot;)</span><br><span class="line">  goodStudentsDF.write.saveAsTable(&quot;GOOD_STUDENT&quot;)</span><br><span class="line"></span><br><span class="line">   //查询打印</span><br><span class="line">  val goodStudentsRows=hiveContext.table(&quot;GOOD_STUDENT&quot;).collect()</span><br><span class="line">  for(goodStudentsRow &lt;- goodStudentsRows)&#123;</span><br><span class="line">    println(goodStudentsRow)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4-JSON数据源创建DataFrame"><a href="#4-JSON数据源创建DataFrame" class="headerlink" title="4. JSON数据源创建DataFrame"></a>4. JSON数据源创建DataFrame</h2><p>关键点：</p>
<ul>
<li>sqlContext.read.json()方法读取Json文件，创建DataFrame</li>
<li>示例代码</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">package com.spark.sql</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SQLContext&#125;</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Administrator on 2017/3/12.</span><br><span class="line">  * 从JSON数据源中读取数据</span><br><span class="line">  */</span><br><span class="line">object JsonDataSource extends  App&#123;</span><br><span class="line">  val conf = new SparkConf()</span><br><span class="line">    .setMaster(&quot;local&quot;)</span><br><span class="line">    .setAppName(&quot;JsonDataSource&quot;)</span><br><span class="line">  val sc = new SparkContext(conf)</span><br><span class="line">  val sqlContext = new SQLContext(sc)</span><br><span class="line">  //导入隐式转化</span><br><span class="line">  import  sqlContext.implicits._</span><br><span class="line">  //创建学生成绩DataFrame</span><br><span class="line">  val studentScoreDF=sqlContext.read.json(&quot;E:\\spark\\src\\main\\resources\\studentscore.json&quot;)</span><br><span class="line">  //create a temporary table</span><br><span class="line">  studentScoreDF.createOrReplaceTempView(&quot;student_score&quot;)</span><br><span class="line">  val goodStudentScoreDF = sqlContext.sql(&quot;select * from student_score where score &gt;= 80&quot;)</span><br><span class="line">  val goodStudentName = goodStudentScoreDF.rdd.map(row =&gt; row(0)).collect()</span><br><span class="line">  goodStudentName.foreach(println)</span><br><span class="line">  //googStudentScoreDF.show()</span><br><span class="line">  //创建学生信息DataFrame</span><br><span class="line">  val studentInfoJson=Array(&quot;&#123;\&quot;name\&quot;:\&quot;Tom\&quot;,\&quot;age\&quot;:10&#125;&quot; +</span><br><span class="line">    &quot;&#123;\&quot;name\&quot;:\&quot;Tom1\&quot;,\&quot;age\&quot;:11&#125;&quot; +</span><br><span class="line">    &quot;&#123;\&quot;name\&quot;:\&quot;Tom2\&quot;,\&quot;age\&quot;:12&#125;&quot; +</span><br><span class="line">    &quot;&#123;\&quot;name\&quot;:\&quot;Tom3\&quot;,\&quot;age\&quot;:13&#125;&quot;)</span><br><span class="line">  val studentInfoDF=sqlContext.read.json(sc.parallelize(studentInfoJson))</span><br><span class="line">  //create a temporary table of student info</span><br><span class="line">  studentInfoDF.createOrReplaceTempView(&quot;student_info&quot;)</span><br><span class="line">  var sql = &quot;select name,age from student_info where name in (&quot;</span><br><span class="line">  for(i &lt;- 0 until goodStudentName.length)&#123;</span><br><span class="line">    sql += &quot;&apos;&quot; + goodStudentName(i) + &quot;&apos;&quot;</span><br><span class="line">    if(i &lt; goodStudentName.length-1)&#123;</span><br><span class="line">     sql += &quot;,&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  sql += &quot;)&quot;</span><br><span class="line">  println(&quot;sql========&quot; + sql)</span><br><span class="line">  val goodStudentInfoDF=sqlContext.sql(sql)</span><br><span class="line">  //将分数大于80分的学生成绩信息与基本信息进行Join</span><br><span class="line">  val goodStudentRDD = goodStudentScoreDF.rdd.map(row=&gt;(row.getAs[String](&quot;name&quot;),row.getAs[Long](&quot;score&quot;)))</span><br><span class="line">    .join(goodStudentInfoDF.rdd.map(row =&gt; (row.getAs[String](&quot;name&quot;),row.getAs[Long](&quot;age&quot;))))</span><br><span class="line">  //将RDD转换为DataFrame</span><br><span class="line">  val goodStudentRowRDD=goodStudentRDD.map(row=&gt;Row(row._1,row._2._1.toString().toLong,row._2._2.toString.toLong))</span><br><span class="line">  val studentStruct=StructType(Array(</span><br><span class="line">    StructField(&quot;name&quot;,StringType,true),</span><br><span class="line">    StructField(&quot;score&quot;,LongType,true),</span><br><span class="line">    StructField(&quot;age&quot;,LongType,true)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  val goodStudentDF=sqlContext.createDataFrame(goodStudentRowRDD,studentStruct)</span><br><span class="line">  goodStudentDF.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-Parquet数据源创建DataFrame"><a href="#5-Parquet数据源创建DataFrame" class="headerlink" title="5. Parquet数据源创建DataFrame"></a>5. Parquet数据源创建DataFrame</h2><p>关键点：</p>
<ul>
<li>sqlContext.read.parquet()方法创建DataFrame</li>
<li><p>将DataFrame数据写入到Parquet</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">studentWithNameAndAgeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append)</span><br><span class="line">    .save(&quot;E:\\spark\\src\\main\\resources\\student&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>示例代码</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">package com.spark.sql</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Administrator on 2017/3/12.</span><br><span class="line">  */</span><br><span class="line">object ParquetLoadData extends  App&#123;</span><br><span class="line">  val conf = new SparkConf()</span><br><span class="line">    .setMaster(&quot;local&quot;)</span><br><span class="line">    .setAppName(&quot;ParquetLoadData&quot;)</span><br><span class="line">  val sc = new SparkContext(conf)</span><br><span class="line">  val sqlContext=new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">  val peopleDF=sqlContext.read.parquet(&quot;E:\\spark\\src\\main\\resources\\people&quot;)</span><br><span class="line">  peopleDF.show()</span><br><span class="line">  //将dataframe转化为RDD并打印出来</span><br><span class="line">  peopleDF.rdd.map(row =&gt; &quot;name:&quot; + row(0))</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(username =&gt; println(username))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>用mergeSchema的方式读取数据，进行元数据的合并</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val studentsDF=sqlContext.read.option(&quot;mergeSchema&quot;,true).parquet(&quot;E:\\spark\\src\\main\\resources\\student&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实例代码</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.spark.sql</span><br><span class="line"></span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.apache.spark.sql.&#123;SQLContext, SaveMode&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Administrator on 2017/3/12.</span><br><span class="line">  * parquet数据元合并元数据</span><br><span class="line">  */</span><br><span class="line">object ParquetSchemaMerge extends App&#123;</span><br><span class="line">  val conf = new SparkConf()</span><br><span class="line">    .setMaster(&quot;local&quot;)</span><br><span class="line">    .setAppName(&quot;ParquetLoadData&quot;)</span><br><span class="line">  val sc = new SparkContext(conf)</span><br><span class="line">  val sqlContext=new SQLContext(sc)</span><br><span class="line">  //导入隐式转换</span><br><span class="line">  import sqlContext.implicits._</span><br><span class="line">  //创建第一个RDD</span><br><span class="line">  val studentWithNameAndAge=Array((&quot;Tom&quot;,13),(&quot;Mary&quot;,14))</span><br><span class="line">  val studentWithNameAndAgeDF=sc.parallelize(studentWithNameAndAge,1).toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">  //保存</span><br><span class="line">  studentWithNameAndAgeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append)</span><br><span class="line">    .save(&quot;E:\\spark\\src\\main\\resources\\student&quot;)</span><br><span class="line"></span><br><span class="line">  //创建第二个RDD</span><br><span class="line">  val studentWithNameAndGrade=Array((&quot;Yangql&quot;,&quot;A&quot;),(&quot;Test&quot;,&quot;B&quot;))</span><br><span class="line">  val studentWithNameAndGradeDF=sc.parallelize(studentWithNameAndGrade).toDF(&quot;name&quot;,&quot;grade&quot;)</span><br><span class="line">  studentWithNameAndGradeDF.write.format(&quot;parquet&quot;).mode(SaveMode.Append)</span><br><span class="line">    .save(&quot;E:\\spark\\src\\main\\resources\\student&quot;)</span><br><span class="line"></span><br><span class="line">  //用mergeSchema的方式读取数据，进行元数据的合并</span><br><span class="line">  val studentsDF=sqlContext.read.option(&quot;mergeSchema&quot;,true).parquet(&quot;E:\\spark\\src\\main\\resources\\student&quot;)</span><br><span class="line">  studentsDF.show()</span><br><span class="line">  studentsDF.printSchema()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2017/04/01/SparkSQL之DataSet20190524/" class="pre-post btn btn-default" title="SparkSQL之DataSet">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">SparkSQL之DataSet</span>
        </a>
    
    
        <a href="/2017/03/31/SparkSQL RDD转换DataFrame方法20190524/" class="next-post btn btn-default" title="SparkSQL RDD转换DataFrame方法">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">SparkSQL RDD转换DataFrame方法</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
    <div id="vcomments" class="valine"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: 'kuH6vv7OvJtqnW8MALiD7bCT-gzGzoHsz',
            appKey: 'vYla34T6SnCwnMeGoMzh8VDn',
            placeholder: '说点什么吧',
            notify: false,
            verify: false,
            avatar: 'null',
            meta: 'nick'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: 'zh-CN'.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-介绍"><span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-JDBC数据源创建DataFrame"><span class="toc-text">2. JDBC数据源创建DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hive-数据源创建DataFrame"><span class="toc-text">3. Hive 数据源创建DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-JSON数据源创建DataFrame"><span class="toc-text">4. JSON数据源创建DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Parquet数据源创建DataFrame"><span class="toc-text">5. Parquet数据源创建DataFrame</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>


  <script src="https://unpkg.com/mermaid@8.0.0/dist/mermaid.min.js"></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>

    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;2016 -  2019 
                <!-- <span><a href='https://coding.net/pages'>Hosted by CODING Pages</a></span> -->
                </span>
            </div>
        </div>
    </div>
</div>







<script src="/js/app.js?rev=@@hash"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>