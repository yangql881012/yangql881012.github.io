<!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="不圆的石头">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://qioinglong.top">
    <!--SEO-->





<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->


<title>Hbase-Spark读写HBase数据 | 不圆的石头</title>


    <link rel="alternate" href="/atom.xml" title="不圆的石头" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    

    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="main-header" style="background-image:url(http://snippet.shenliyang.com/img/banner.jpg)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="不圆的石头">
         <!--   <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block"> -->
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                 <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
    	</div>
    </div>
</header>

    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://qioinglong.top">不圆的石头</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/金融业务/"><i class="fa "></i>金融业务</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/数据分析/"><i class="fa "></i>数据分析</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/读书/"><i class="fa "></i>读书</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/技术/"><i class="fa "></i>技术</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Hbase-Spark读写HBase数据">
            
	            Hbase-Spark读写HBase数据
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/技术">
            技术
        </a>
    </span>
    

    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/Hbase" title="Hbase">
                        Hbase
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2017/07/16</span>
        </span>
        
    
</div>

            
            
            <p class="fa fa-exclamation-triangle warning">
                本文于<strong>694</strong>天之前发表。
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <h2 id="1-Spark写入数据到HBase"><a href="#1-Spark写入数据到HBase" class="headerlink" title="1.Spark写入数据到HBase"></a>1.Spark写入数据到HBase</h2><ul>
<li><p>HBase表结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">##创建hbase表</span><br><span class="line">create &apos;user&apos;,&apos;basic&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义HBase定义</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val hbaseConf=HBaseConfiguration.create()</span><br><span class="line">//zookeeper端口</span><br><span class="line">hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">//zookeeper地址</span><br><span class="line">hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>指定输出格式和输出表名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val jobConf=new JobConf(hbaseConf,this.getClass)</span><br><span class="line">jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line">jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;user&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义转换格式  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将RDD[(uid:Int, name:String, age:Int)] 转换成 RDD[(ImmutableBytesWritable, Put)]</span><br><span class="line">def convert(triple:(Int,String,Int))=&#123;</span><br><span class="line">   val put=new Put(Bytes.toBytes(triple._1))</span><br><span class="line">   put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(triple._2))</span><br><span class="line">   put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(triple._3))</span><br><span class="line">   (new ImmutableBytesWritable,put)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>保存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//使用saveAsHadoopDataset方法写入HBase</span><br><span class="line">localData.saveAsHadoopDataset(jobConf)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="2-Spark写HBase示例"><a href="#2-Spark写HBase示例" class="headerlink" title="2.Spark写HBase示例"></a>2.Spark写HBase示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">package com.crm.base</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"></span><br><span class="line">object TestHbaseWrite &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">      val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      .setAppName(&quot;JDBCDataSrc&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    sc.setLogLevel(&quot;ERROR&quot;)</span><br><span class="line">    val sqlContext=new SQLContext(sc)</span><br><span class="line">    val hiveContext=new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">    //导入隐式转换</span><br><span class="line">    import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">    //定义HBase定义</span><br><span class="line">    val hbaseConf=HBaseConfiguration.create()</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;)</span><br><span class="line"></span><br><span class="line">    //指定输出格式和输出表名</span><br><span class="line">    val jobConf=new JobConf(hbaseConf,this.getClass)</span><br><span class="line">    jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line">    jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;user&quot;)</span><br><span class="line"></span><br><span class="line">    //read RDD data from somewhere and convert</span><br><span class="line">    val rawData = List((1,&quot;lilei&quot;,14), (2,&quot;hanmei&quot;,18), (3,&quot;someone&quot;,38))</span><br><span class="line">    val localData= sc.parallelize(rawData).map(convert)</span><br><span class="line"></span><br><span class="line">    //使用saveAsHadoopDataset方法写入HBase</span><br><span class="line">    localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//将RDD[(uid:Int, name:String, age:Int)] 转换成 RDD[(ImmutableBytesWritable, Put)]</span><br><span class="line">  def convert(triple:(Int,String,Int))=&#123;</span><br><span class="line">    val put=new Put(Bytes.toBytes(triple._1))</span><br><span class="line">    put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(triple._2))</span><br><span class="line">    put.addColumn(Bytes.toBytes(&quot;basic&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(triple._3))</span><br><span class="line">    (new ImmutableBytesWritable,put)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Spark读HBase数据"><a href="#3-Spark读HBase数据" class="headerlink" title="3.Spark读HBase数据"></a>3.Spark读HBase数据</h2><p>Spark读取HBase，我们主要使用SparkContext 提供的newAPIHadoopRDDAPI将表的内容以 RDDs 的形式加载到 Spark 中。</p>
<ul>
<li><p>定义HBase定义</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val hbaseConf=HBaseConfiguration.create()</span><br><span class="line">hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置查询的表名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbaseConf.set(TableInputFormat.INPUT_TABLE, &quot;user&quot;)</span><br><span class="line">val userRdd=sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])</span><br></pre></td></tr></table></figure>
</li>
<li><p>遍历输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">userRdd.foreach&#123;case (_,result)=&gt;&#123;</span><br><span class="line">      val key=Bytes.toInt(result.getRow)</span><br><span class="line">      val name=Bytes.toString(result.getValue(&quot;basic&quot;.getBytes, &quot;name&quot;.getBytes))</span><br><span class="line">      val age =Bytes.toInt(result.getValue(&quot;basic&quot;.getBytes, &quot;age&quot;.getBytes))</span><br><span class="line">      println(key)</span><br><span class="line">      println(name)</span><br><span class="line">      println(age)</span><br><span class="line">    &#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="4-park读HBase示例"><a href="#4-park读HBase示例" class="headerlink" title="4.park读HBase示例"></a>4.park读HBase示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">package com.crm.base</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableInputFormatBase</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableInputFormat</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.client.Result</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Spark读取HBase，我们主要使用SparkContext 提供的newAPIHadoopRDDAPI将表的内容以 RDDs 的形式加载到 Spark 中。</span><br><span class="line"> */</span><br><span class="line">object TestHbaseRead &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      .setAppName(&quot;JDBCDataSrc&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    sc.setLogLevel(&quot;ERROR&quot;)</span><br><span class="line">    val sqlContext=new SQLContext(sc)</span><br><span class="line">    val hiveContext=new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">    //导入隐式转换</span><br><span class="line">    import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">    //定义HBase定义</span><br><span class="line">    val hbaseConf=HBaseConfiguration.create()</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop01,hadoop02,hadoop03&quot;)</span><br><span class="line"></span><br><span class="line">    //设置查询的表名</span><br><span class="line">    hbaseConf.set(TableInputFormat.INPUT_TABLE, &quot;user&quot;)</span><br><span class="line">    val userRdd=sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])</span><br><span class="line">    val count=userRdd.count()</span><br><span class="line">    println(count)</span><br><span class="line"></span><br><span class="line">    //遍历输出</span><br><span class="line">    userRdd.foreach&#123;case (_,result)=&gt;&#123;</span><br><span class="line">      val key=Bytes.toInt(result.getRow)</span><br><span class="line">      val name=Bytes.toString(result.getValue(&quot;basic&quot;.getBytes, &quot;name&quot;.getBytes))</span><br><span class="line">      val age =Bytes.toInt(result.getValue(&quot;basic&quot;.getBytes, &quot;age&quot;.getBytes))</span><br><span class="line">      println(key)</span><br><span class="line">      println(name)</span><br><span class="line">      println(age)</span><br><span class="line">    &#125;&#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2017/09/03/月结月策_20170820190524/" class="pre-post btn btn-default" title="月结月策_201708">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">月结月策_201708</span>
        </a>
    
    
        <a href="/2017/07/15/大数据场景学习之实时更新HBase数据库(三)20190524/" class="next-post btn btn-default" title="大数据场景学习之实时更新HBase数据库(三)">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">大数据场景学习之实时更新HBase数据库(三)</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
    <div id="vcomments" class="valine"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: 'kuH6vv7OvJtqnW8MALiD7bCT-gzGzoHsz',
            appKey: 'vYla34T6SnCwnMeGoMzh8VDn',
            placeholder: '说点什么吧',
            notify: false,
            verify: false,
            avatar: 'null',
            meta: 'nick'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: 'zh-CN'.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Spark写入数据到HBase"><span class="toc-text">1.Spark写入数据到HBase</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Spark写HBase示例"><span class="toc-text">2.Spark写HBase示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Spark读HBase数据"><span class="toc-text">3.Spark读HBase数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-park读HBase示例"><span class="toc-text">4.park读HBase示例</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>


  <script src="https://unpkg.com/mermaid@8.0.0/dist/mermaid.min.js"></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>

    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;2016 -  2019 
                <!-- <span><a href='https://coding.net/pages'>Hosted by CODING Pages</a></span> -->
                </span>
            </div>
        </div>
    </div>
</div>







<script src="/js/app.js?rev=@@hash"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>